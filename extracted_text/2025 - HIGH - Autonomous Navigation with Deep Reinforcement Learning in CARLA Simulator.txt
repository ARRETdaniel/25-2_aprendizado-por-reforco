Fakultät Verkehrswissenschaften „Friedrich List“, Professur für Ökonometrie und Statistik, insb. im Verkehrswesen
Master Thesis
Autonomous Navigation with Deep 
Reinforcement Learning in CARLA Simulator
Bharath Ashok Kumar
Supervised by
M.Sc. Dianzhao Li
Dresden, 07. January 2025



Abstract
Autonomous navigation is a critical component in the development of self-driving
vehicles. This thesis explores the application of deep reinforcement learning (DRL) for
autonomous navigation within the CARLA simulator, an open-source simulation plat-
form designed for autonomous driving research. The work focuses on training agents to
make optimal driving decisions in dynamic urban environments without human inter-
vention. Deep learning models were combined with reinforcement learning techniques
so the vehicle could perceive its surroundings, predict outcomes, and take appropriate
actions to navigate safely.
The study evaluates the performance of a state-of-the-art DRL algorithm, Proxi-
mal policy optimization (PPO), while actively addressing and overcoming challenges
like sparse rewards, training stability, and generalization to unseen scenarios. A cus-
tom reward function was crafted to prioritize collision avoidance, lane-keeping, smooth
acceleration, and steering, ensuring the agent adheres to realistic driving behavior.
Experimental results demonstrated that the DRL-based agent achieved promising per-
formance in various simulated driving tasks, including maintaining speed, following
traffic signals, lane-following, and intersection handling. Furthermore, the agent ex-
hibited commendable performance in novel environments, highlighting its capacity to
generalize and adapt efficiently.
This thesis contributes to the understanding of integrating DRL for autonomous
navigation in simulation-based environments and highlights the CARLA simulator’s
role as a robust testing ground. The findings lay the groundwork for further ad-
vancements in sim-to-real transfer and scalable training methods for autonomous ve-
hicles. Our code and model are available at https://github.com/bharath-ashok/
Autonomous-Navigation-DRL-CARLA.git



Contents
List of Figures v
List of Tables viii
List of Abbreviations ix
1 Introduction 1
1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Background 7
2.1 Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2.1 Feed-Forward Network . . . . . . . . . . . . . . . . . . . . . . . . 9
2.3 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3.1 Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . 10
2.3.2 Bellman Equation . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3.3 Reward Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3.4 Action Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.4 Deep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4.1 Policy-Based Approaches . . . . . . . . . . . . . . . . . . . . . . 15
2.4.2 Proximal Policy Optimization . . . . . . . . . . . . . . . . . . . . 17
3 Experiment Setup 21
3.1 CARLA Simulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.1.1 Vehicle Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.1.2 Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.1.3 Waypoints and Routes . . . . . . . . . . . . . . . . . . . . . . . . 24
3.2 Environment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.3 Deep Reinforcement Learning Setup . . . . . . . . . . . . . . . . . . . . 26
3.3.1 State Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.3.2 Action Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.3.3 Reward Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.4 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.5 Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
iii

Contents
4 Evaluation 37
4.1 Evaluating Agent on New Maps . . . . . . . . . . . . . . . . . . . . . . . 38
5 Conclusion 43
iv

List of Figures
1.1 SAE J3016 levels of driving automation [24] . . . . . . . . . . . . . . . . 2
1.2 Comparison of modular system and end-to-end driving system [14] . . . 4
2.1 Components of AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Components of ML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.3 A basic MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.4 Interaction between agent and environment in MDP [26] . . . . . . . . 10
2.5 Clip Function, positive advantages (left) and negative advantages (right).
The red circle on each plot shows the starting point for the optimization,
i.e., r = 1 [22]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.1 Vehicle control function parameters in CARLA [4] . . . . . . . . . . . . 22
3.2 Maps of towns from CARLA used in this study [10] . . . . . . . . . . . 23
3.3 Top view of Town02 used for training [4] . . . . . . . . . . . . . . . . . 24
3.4 Example route generated with waypoints marked in blue dots [4] . . . . 24
3.5 Illustration of some state values [10] . . . . . . . . . . . . . . . . . . . . 27
3.6 Overview of the proposed training setup . . . . . . . . . . . . . . . . . . 32
3.7 Average episodic reward achieved against timesteps in Town02 CARLA 36
4.1 Performance of the trained model in Town02. (top: Lateral deviation
from lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The
red splits are used to denote when the traffic light is red. . . . . . . . . . 37
4.2 Performance of the trained model in Town01. (top: Lateral deviation
from lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The
red splits are used to denote when the traffic light is red. . . . . . . . . . 39
4.3 Performance of the trained model in Town03. (top: Lateral deviation
from lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The
red splits are used to denote when the traffic light is red. . . . . . . . . . 40
4.4 Performance of the trained model in Town05. (top: Lateral deviation
from lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The
red splits are used to denote when the traffic light is red. . . . . . . . . . 41
v



List of Tables
3.1 State space of the environment . . . . . . . . . . . . . . . . . . . . . . . 26
3.2 PPO model parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.1 Performance metrics across different maps . . . . . . . . . . . . . . . . . 42
vii



List of abbreviations
AI Artificial intelligence
ML Machine learning
IL Imitation learning
RL Reinforcement learning
DL Deep learning
DRL Deep reinforcement learning
CARLA Car learning to act
DQN Deep Q-network
DDPG Deep deterministic policy gradient
ANN Artificial neural network
FNN Feedforward neural network
RNN Recurrent neural network
MLP Multi-layer perceptron
CNN Convolutional neural network
MDP Markov decision process
PPO Proximal policy optimization
TRPO Trust region policy optimization
GAE Generalized advantage estimation
KL Kullback-Leibler divergence
API Applicaiton programming interface
ix



1 Introduction
The rise of Artificial intelligence (AI) and enhanced computational capabilities have
significantly accelerated interest in autonomous driving technologies. Once a concept
confined to science fiction, autonomous vehicles are now a significant focus in the auto-
motive industry due to their potential to revolutionize transportation systems. As these
technologies evolve, their applications become clearer, positioning autonomous driving
as a central theme in the automotive industry. The widespread adoption of autonomous
driving promises substantial societal benefits, including improved road safety, reduced
traffic congestion, increased transportation efficiency, lower energy consumption, and
minimized environmental pollution.
Autonomous driving technologies are being developed to revolutionize the transporta-
tion landscape by addressing many pressing issues that have long plagued modern soci-
eties. These technologies are supposed to solve one of the biggest problems: road safety.
Countless numbers of traffic accidents due to human error have resulted in uncountable
injuries and death cases every year [1]. These vehicles, when equipped with the latest
sensors and algorithms, autonomous vehicles can significantly reduce those errors by
paying continuous attention to the road, obeying traffic rules, and being responsive to
possible dangers quickly [17]. By reducing the human element in driving, technolo-
gies can significantly decrease the number of traffic-related accidents and make the
roads much safer for everybody. One more critical problem that autonomous driving
technologies can solve is traffic congestion. Congestion increases travel time, fuel con-
sumption, and pollution in many urban areas [21]. Autonomous vehicles can improve
traffic flow through efficient route planning and coordination, reducing bottlenecks and
improving overall traffic efficiency [8]. Connected to traffic management systems, au-
tonomous vehicles will adjust speeds and routes in real time to create smooth traffic
flow and reduce congestion.
Autonomous vehicles aim to produce greater mobility and access for people who
cannot drive themselves, for example, older people or disabled [11]. Allowing reliable,
independent transportation alternatives, this technology can dramatically improve the
quality of life for those populations, affording them much more autonomy and accessi-
bility to essential activities and services. The logistics and delivery sectors are also set
to benefit highly from autonomous driving technologies. These technologies could revo-
lutionize logistics and delivery services by providing round-the-clock operations without
human intervention, resulting in a shorter, more reliable supply chain [28]. This will
deliver transformational solutions to significant societal challenges, from improved road
1

1 Introduction
safety to reduced traffic congestion and transportation efficiency. They are essential for
providing better accessibility to this technology, which may change how people travel
someday. Further development in this field holds the promise to make the world a more
sustainable and connected place.
The Society of Automotive Engineers (SAE) systematically categorized the journey
towards fully autonomous vehicles into levels of automation. These levels serve as a
framework for understanding the progression of capabilities in autonomous systems,
ranging from no automation (Level 0) to full automation (Level 5).
Figure 1.1: SAE J3016 levels of driving automation [24]
Levels of Automation
•Level 0: No Automation: The human driver performs all driving tasks without
assistance from the vehicle’s systems.
•Level 1: Driver Assistance: The vehicle can assist the driver with specific
tasks, such as adaptive cruise control or lane-keeping, but the driver remains in
complete control.
•Level 2: Partial Automation: The vehicle can control steering and accelera-
tion/deceleration under specific conditions, yet the driver must remain engaged
and monitor the environment.
•Level 3: Conditional Automation: The vehicle can perform all driving tasks
under certain conditions, with the driver expected to take over when requested.
2

•Level 4: High Automation: Without driver intervention, the vehicle can
perform all driving functions within pre-defined operational domains.
•Level 5: Full Automation: The vehicle can operate autonomously in all envi-
ronments and conditions, requiring no human input.
Traditional autonomous driving systems have been mainly based on a modular frame-
work, where individual tasks, including perception, mapping, planning, and control, are
developed separately and then integrated. This has allowed the field to make incre-
mental improvements, as researchers can focus on optimizing specific modules. For
example, perception modules are usually optimized with metrics like mean Average
Precision (mAP) to improve environmental object detection [7]. Similarly, planning
modules prioritize metrics related to driving safety and passenger comfort. Despite its
advantages, the modular approach suffers from key limitations: it often lacks alignment
between the objectives of different modules, leading to inconsistencies and suboptimal
overall system performance. The inherent complexity of integrating these modules also
makes the system more prone to error propagation and more complex to adapt to
unforeseen scenarios [5].
To overcome these limitations, end-to-end learning approaches have emerged as a
transformative paradigm for autonomous driving. Unlike modular systems, end-to-
end systems leverage Deep learning (DL) models to map raw sensory data to control
commands directly, bypassing the intermediate steps of object detection, path planning,
and decision-making. Neural networks trained using this approach aim to unify the
objectives of perception and decision-making into a single optimization framework.
Tesla’s Full Self-Driving (FSD) Beta V12 has adopted this strategy, employing a neural
network-based architecture that replaces traditional planning and control methods,
simplifying the overall system and reducing code complexity [27].
Deep reinforcement learning (DRL) has gained significant attention among end-to-
end learning techniques due to its ability to dynamically adapt to complex environ-
ments. Unlike Imitation learning (IL), which relies heavily on expert demonstration
data and struggles to generalize to new or rare driving scenarios, DRL optimizes driv-
ing policies through environmental interactions and a reward-based mechanism. This
adaptability enables DRL to address challenges in diverse real-world conditions and
outperform human drivers in specific tasks. Key studies, such as those by Mnih et
al.[18], on the Deep Q-network (DQN) algorithm, have laid the foundation for using
Reinforcement learning (RL) to solve complex decision-making problems, including au-
tonomous navigation. Similarly, Lillicrap et al.[16] introduced the Deep deterministic
policy gradient (DDPG) algorithm, enabling effective decision-making in continuous
action spaces—a crucial aspect of autonomous driving.
The flexibility of DRL has been demonstrated in simulation environments like a
widely used open-source simulator designed for developing and testing autonomous
3

1 Introduction
Figure 1.2: Comparison of modular system and end-to-end driving system [14]
driving algorithms [6]. Researchers have successfully employed DRL algorithms in Car
learning to act (CARLA) for tasks such as lane-keeping, obstacle avoidance, and ur-
ban navigation. For instance, Liang et al.[15] integrated IL with DDPG to improve
sample efficiency and achieve superior driving performance compared to traditional
approaches. Similarly, Fayjie et al.[9] implemented autonomous navigation in urban
environments using DQN with experience replay, demonstrating significant safety and
driving efficiency improvements. These studies highlight the potential of DRL to sur-
pass conventional methods, particularly in scenarios where adaptability and long-term
decision-making are critical.
Despite these advancements, the real-world application of DRL in autonomous driv-
ing remains a challenge. The gap between simulated environments and real-world con-
ditions limits the transferability of DRL models. However, ongoing research in domain
adaptation and transfer learning is making significant strides in bridging this divide.
For example, Lee et al.[12] proposed integrating human driving datasets with DRL
methods to enhance the transferability of trained models, enabling more robust and
efficient real-world performance.
The CARLA simulator provides a realistic virtual environment for testing and de-
veloping navigation models, allowing researchers to overcome real-world limitations
and iteratively refine their models in diverse and controlled scenarios. Nevertheless,
integrating DRL into such a simulation-based framework for autonomous navigation
presents its own set of challenges, including the need for efficient training algorithms,
realistic environmental dynamics, and robust evaluation metrics.
4

1.1 Problem Statement
1.1 Problem Statement
This thesis aims to address these challenges by leveraging CARLA as a simulation plat-
form to develop and test a DRL-based model for autonomous navigation. This research
focuses on the potential of DRL—an advanced technique that combines principles from
DL and RL—to tackle the complexities associated with autonomous navigation. The
work will involve training and evaluating autonomous agents capable of safe, efficient,
and adaptive navigation through complex urban environments using the CARLA sim-
ulator.
Ultimately, this research seeks to investigate DRL-based navigation strategies within
the CARLA simulator, contributing to developing a more adaptive, efficient, and uni-
fied autonomous driving system capable of handling complex real-world scenarios. Em-
phasis will be placed on how DRL can transform the field of autonomous driving,
particularly in urban navigation. This study will concentrate on the following key
areas:
•DRL Algorithms : The research will analyze and optimize algorithms that ef-
fectively facilitate autonomous driving capabilities.
•Input Data : We will examine the properties of various sensors, investigate pre-
processing techniques for input data, and identify methods to enhance model
inputs for improved accuracy and stability in decision-making.
•Reward Function : Within the context of the CARLA simulator, we will aim to
develop a more representative and effective reward function that guides the model
toward safer and more efficient driving strategies through iterative learning.
•Robustness Testing : The study will assess the model’s adaptability across
diverse environments and conditions to ensure its reliability and performance.
5

1 Introduction
1.2 Thesis Structure
Chapter 2 will present the current research and development context of autonomous
driving, end-to-end learning, and DRL. Besides, it will briefly introduce the theoreti-
cal foundations of these topics, including Machine learning (ML), DRL, and RL, and
discuss some methods that have been proposed to solve the problem.
Chapter 3 describes the research process and the steps to implement the experiment
setup. This includes building the simulation environment in CARLA with the obser-
vation space, action space, and reward function and selecting appropriate algorithms
and neural network structure. The training process will be presented. The influence of
the reward function on the policy will be discussed.
Chapter 4 , the trained model will be evaluated. The agent’s velocity, distance from
the midline, and traffic signal following capabilities will be evaluated. The trained
model will be tested in new maps.
Chapter 5 , the conclusion will be drawn regarding whether an autonomous driving
model based on a DRL approach can accomplish autonomous navigation. Addition-
ally, the stability of the model during testing will be discussed, and the feasibility of
implementing this approach from a simulated environment to the real world will be
discussed, along with the future directions for its improvement.
6

2 Background
2.1 Machine Learning
Figure 2.1: Components of AI
ML is a branch of AI that allows systems to automatically learn and improve from
experience without being explicitly programmed [20]. It focuses on developing computer
programs that can access and use data to learn for themselves. Learning begins with
observations or data, such as examples, direct experience, or instruction, to look for
patterns in data and make better decisions in the future based on the examples we
provide. The primary aim is to allow the computers to learn automatically without
human intervention or assistance and adjust actions accordingly.
ML algorithms are often categorized as supervised, unsupervised, or reinforcement
learning. Supervised algorithms require a data scientist to provide input and desired
output and feedback about predictions’ accuracy during training. In contrast, unsuper-
vised algorithms do not need to be trained with desired outcome data. Instead, they
use an iterative approach called DL to review data and arrive at conclusions.
In RL, an agent learns to make decisions by interacting with its environment. The
agent performs actions and receives feedback through rewards or penalties. Over time,
the agent learns to perform the actions that maximize its cumulative reward. This
type of learning is beneficial when the agent must make a sequence of decisions, and
the reward for an action may be delayed.
7

2 Background
Figure 2.2: Components of ML
2.2 Deep Learning
DL is a specialized area within ML that emphasizes the creation of Artificial neural
network (ANN) capable of learning from extensive datasets. These DL models draw in-
spiration from the architecture and operational mechanisms of the human brain, which
is characterized by multiple layers of interconnected nodes that facilitate information
processing. They are adept at discerning patterns within various forms of data, in-
cluding images and audio. They are applicable in diverse domains, such as image and
speech recognition, natural language processing, and robotics.
A fundamental distinction between ML and DL lies in the ability of DL models to
autonomously learn hierarchical data representations, whereas ML models typically
necessitate manual feature engineering. This autonomy enables DL models to uncover
intricate patterns and relationships within data without human input, rendering them
more potent and scalable than conventional ML models.
The development of DL models has been propelled by the availability of large labeled
datasets and powerful computational resources, particularly Graphics Processing Units
(GPUs). With their superior ability to extract high-level features from raw input
data, DL models have achieved state-of-the-art results in many domains, including
image recognition, speech recognition, natural language processing, and, most recently,
autonomous driving. Today, DL is at the forefront of AI research and application,
driving new advancements and capabilities in ML. DL uses ANN inspired by the
human brain. These networks consist of layers of interconnected nodes, or neurons,
which process and transmit information. The process begins with input data, such as
images or text, fed into the network’s input layer. Each node in this layer interacts
with the data, performing simple computations. The results are then passed to the
next layer, which continues through the network’s multiple layers. The network learns
to extract increasingly complex features as the data moves through the layers. Once
the data reaches the output layer, the network makes a prediction or decision based
on what it has learned. This entire process is known as forward propagation. If the
prediction is incorrect, the network uses a process called backpropagation to adjust
8

2.2 Deep Learning
Figure 2.3: A basic MLP
the weights and biases of the nodes based on the prediction error. Over time, the
network learns to make accurate predictions through many iterations of forward and
backpropagation.
One of DLs strength is its outstanding performance in high-dimensional analysis.
Feedforward neural network (FNN) are widely used for classification and regression
problems, while Convolutional neural network (CNN) have achieved great success in
high-dimensional data processing, such as images and videos. Meanwhile, Recurrent
neural network (RNN) and their variants have demonstrated remarkable advantages in
handling sequential data, such as text and speech.
2.2.1 Feed-Forward Network
FNN are a class of ANN in which connections between nodes propagate information
strictly in one direction, from input to output. They form the foundation of DL models,
which have revolutionized various fields by enabling the processing of complex, high-
dimensional data. Multi-layer perceptron (MLP) is a subclass of FNN that consists of
multiple layers of interconnected nodes, including an input layer, one or more hidden
layers, and an output layer. Each node (or neuron) in an MLP applies a weighted sum
of its inputs followed by a non-linear activation function. This combination allows MLP
to learn and represent complex functions, making them highly versatile for various ML
problems.
The learning process involves adjusting the weights and biases using optimization
algorithms, typically through backpropagation and gradient descent. The non-linear
activation functions enable the network to capture intricate relationships within the
data. Figure 2.3 depicts the architecture of a MLP. The diagram includes the input,
hidden, and output layers, with arrows representing the flow of information through the
network. Each connection corresponds to a weight that is optimized during training.
9

2 Background
2.3 Reinforcement Learning
RL is one type of ML in which an agent learns to make decisions by interacting with its
environment. The principal component in RL is the agent, whereas the environment
denotes the system with which the agent interacts. In RL, an agent takes actions in
an environment to achieve a goal. The agent receives feedback through rewards or
penalties and aims to maximize the total reward over time. This feedback helps the
agent learn the optimal policy, which maps states to actions that maximize the expected
cumulative reward.
The concept of RL has existed since the 1950s and 1960s, with roots in optimal
control and Markov decision process (MDP). However, it was not until the 1980s and
1990s that RL started to take shape as a distinct field, with the work of researchers
like Richard Sutton and Andrew Barto [26].
2.3.1 Markov Decision Process
A MDP is a mathematical framework that models decision-making problems where
some outcomes are random and an agent influences some. This framework formalizes
the analysis and resolution of problems where an agent continuously makes decisions
to reach a goal, often in the face of uncertainty.
Figure 2.4: Interaction between agent and environment in MDP [26]
From Barto and Sutton [26], an MDP is defined by a tuple (S, A, P, R, γ ), where
States ( S) is a finite set of states of all possible situations where agents can find them-
selves. Each state provides the necessary context for decision-making. Actions ( A)
is a finite set of actions the agent can take depending on the current state, and the
choice of action influences the subsequent state. Transition Probability ( P) defines the
probability of transitioning from one state to another, given a specific action. Formally,
10

2.3 Reinforcement Learning
P(s′|s, a)represents the probability of reaching state s′from state safter taking action
a.
P(s′|s, a) = Pr( st+1=s′|st=s, at=a) (2.3.1)
The Markov process property asserts that the future is determined solely by the
current state and action, independent of the preceding sequence of events. Reward
Function ( R) assigns a scalar value R(s, a, s′)to each transition, representing the im-
mediate reward received after transitioning from state sto state s′by taking action a.
The reward quantifies the benefit of an action taken in a given state. Discount Factor
(γ) This number between 0 and 1 represents the importance of future rewards relative
to immediate rewards. A discount factor close to 0 prioritizes immediate gains, while
a factor close to 1 emphasizes long-term rewards.
The primary objective of MDP is to derive a policy π(a|s)mapping from states to
actions. The policy should maximize the expected cumulative reward and return over
time. The return is typically expressed as:
Gt=∞/summationdisplay
k=0γkR(st+k, at+k, st+k+1), (2.3.2)
Where Gtis the total discounted reward from time tonwards. There are several
methods to derive optimal policies for MDP, including:
•Value Iteration : This iterative algorithm computes each state’s value func-
tion, representing the maximum expected cumulative reward obtainable from
that state. The optimal policy can then be derived from these values.
•Policy Iteration : This method alternates between policy evaluation (calculating
the value function for a given policy) and policy improvement (updating the policy
based on the value function) until convergence to an optimal policy is achieved.
MDP are powerful mathematical tools for modeling and solving decision-making
problems under uncertainty. By providing a systematic approach to analyzing states,
actions, transitions, rewards, and the effect of future rewards, MDP plays a vital role
in developing intelligent systems.
2.3.2 Bellman Equation
The Bellman equation is a foundational concept in RL and dynamic programming. It
formalizes the principle of optimality, which states that the value of a state under an
optimal policy is the maximum expected return achievable from that state, considering
all possible actions and subsequent states [26].
In RL, the value function V(s)represents the expected cumulative reward when
starting from a state sand following a given policy π. For a given policy π, the
11

2 Background
Bellman equation for the value function is expressed as:
Vπ(s) =Ea∼π,s′∼P/b acketleftbigR(s, a, s′) +γVπ(s′)/b acket ightbig(2.3.3)
Where srepresents the current state, adenotes the action taken in the state sas
determined by the policy π, and s′is the next state reached after action a. The reward
function R(s, a, s′)specifies the reward received for transitioning from state stos′using
action a. The discount factor, γ, ranges between 0 and 1 ( 0≤γ≤1) and determines
the relative importance of future rewards over immediate ones. Lastly, Erepresents
the expectation over all possible actions and state transitions.
When following the optimal policy π∗, the value function V∗(s)satisfies the Bellman
optimality equation:
V∗(s) = maxaEs′∼P/b acketleftbigR(s, a, s′) +γV∗(s′)/b acket ightbig(2.3.4)
Here, the agent selects the action athat maximizes the expected cumulative reward,
considering the immediate and future value of the next state.
The Bellman equation can also be applied to the action-value function Q(s, a), which
represents the expected cumulative reward of taking action ain state sand then fol-
lowing a given policy:
Qπ(s, a) =Es′∼P/b acketleftbigR(s, a, s′) +γEa′∼πQπ(s′, a′)/b acket ightbig(2.3.5)
The Bellman optimality equation for the optimal Q-function Q∗(s, a)is:
Q∗(s, a) =Es′∼P/b acketleftbigg
R(s, a, s′) +γmax
a′Q∗(s′, a′)/b acket ightbigg
(2.3.6)
The optimal policy π∗can be derived from the optimal Q-function:
π∗(s) = arg max
a∈AQ∗(s, a). (2.3.7)
The Bellman equation expresses a recursive relationship : The value of a state sis the
immediate reward plus the discounted value of the following state s′, assuming the
policy is followed. This recursive breakdown enables efficient computation of value
functions using algorithms like value iteration andpolicy iteration .
The Bellman equation is the mathematical backbone of RL, enabling the formulation
and solution of sequential decision-making problems. It connects immediate rewards,
future rewards, and optimal actions in a dynamic environment, forming the foundation
for many RL algorithms.
12

2.3 Reinforcement Learning
2.3.3 Reward Function
The reward function is one of the most critical constituents of RL. It provides feedback
to the agent about the effectiveness of its actions in achieving a specific goal or desired
outcome.
Formally, the reward function is denoted as R(s, a, s′), where s′represents the current
state, a denotes the action taken, and s is the resulting next state after executing action
a[26]. First, the reward function helps guide the learning process, informing which
actions result in desirable outcomes. Rewards with positive values reinforce behaviors
that generate desirable outcomes, while the agent learns to repeat those actions when
similar situations arise.
On the other hand, negative rewards or penalties penalize actions leading to unde-
sirable outcomes. The agent then learns through experience over time. Secondly, the
reward function captures the learning task’s objectives by quantifying the state’s de-
sirability and subsequent actions. Varying the reward amount allows the developer to
elicit different behaviors from the agent, whether points in a game or cost minimization
in resource-allocation problems. Thirdly, the reward function balances exploration and
exploitation in RL: Exploration entails trying new actions to learn their outcomes,
while in exploitation, one selects the actions already known to yield good rewards.
A well-designed reward function encourages exploration by rewarding novel actions
while exploiting successful actions determined to be beneficial. This is another way the
reward function may help solve the credit assignment problem: the agent has to figure
out which of its actions are causing its delayed outcomes. When rewards are delayed,
the agent can credit actions chosen several steps before an obtained reward. Finally,
the accumulated rewards received over time can be used as a measure of an agent’s
performance. The goal under RL is usually to maximize the expected return, defined
as the sum of discounted future rewards. A good reward function thus provides a
quantitative basis to evaluate the performances of various policies and further improve
them.
The reward function provides a wide range of functions for the agent to learn in
which direction, defines objectives, balances exploration and exploitation, and helps in
credit assignment and performance evaluation. The reward function is important since
it directly impacts the agent’s behavior in terms of its effectiveness in achieving the
goals.
2.3.4 Action Spaces
The action space is the set of all possible actions an agent can take while interacting with
its environment. This framework is essential for understanding how agents navigate
complex environments, make decisions, and ultimately learn to optimize their actions
to achieve specific goals.
13

2 Background
The action space can be characterized in several ways, primarily through its dimen-
sionality and structure. It may be classified as either finite or infinite. A finite action
space consists of a limited, countable set of actions, allowing for discrete choices. For
instance, in a chess game, the action space encompasses all the legal moves available to
a player at any moment. In contrast, an infinite action space allows for an unbounded
number of potential actions. This is often observed in robotic control tasks, where
the action space may include all possible angles or speeds of a robotic arm, creating a
continuous range of actions.
Furthermore, action spaces can be categorized as discrete or continuous. A discrete
action space consists of distinct and separate actions, such as moving left, right, up,
or down in a grid-based environment. Conversely, a continuous action space allows
actions to take any value within a defined range. For instance, in a continuous control
problem, an agent might adjust a vehicle’s throttle, enabling it to select any speed
within a specified limit.
The representation of actions within the action space is also significant. Actions can
be expressed in various forms, such as integers for discrete actions or real numbers
for continuous actions. Depending on the specific application and requirements, more
complex data structures may be employed. Moreover, the available actions in an ac-
tion space may depend on the current state of the environment. In certain scenarios,
specific actions may only be permissible in particular states. For example, in a traffic
simulation, a vehicle might only be allowed to turn left at a designated intersection,
illustrating how the action space can be context-dependent.
It serves as the foundation for decision-making processes. The action space directly
influences the choices available to the agent, which are crucial for determining the
agent’s behavior and strategy in the environment. The actions taken by the agent can
significantly impact its trajectory, leading to different states and associated rewards.
The action space is integral to the definition of an agent’s policy. A policy that maps
states to actions dictates how an agent selects actions based on their current state.
The effectiveness of a policy in maximizing expected rewards is inherently linked to
the structure and composition of the action space. An agent with a well-defined and
appropriate action space is better positioned to learn optimal behaviors and strategies.
Additionally, the reward structure of an environment is influenced by the actions
taken within the action space. Each action leads to transitions between states, which
in turn yield rewards. Thus, the action space determines the range of possible actions
and shapes the learning process by establishing the relationship between actions, states,
and rewards.
14

2.4 Deep Reinforcement Learning
2.4 Deep Reinforcement Learning
DRL is a subfield of RL that combines RL with DL. This combination allows the
agent to make decisions based on high-dimensional inputs, which would be difficult
with traditional RL methods. The advent of DRL can be traced back to developing
algorithms like DQN by researchers at DeepMind in 2013 [19]. DQN was a breakthrough
because it was the first successful attempt to combine DL with RL.
DRL has achieved several significant accomplishments. The most famous is Deep-
Mind’s use of DRL to train its AlphaGo program, which defeated the world champion
Go player in 2016, a feat previously thought to be decades away due to the complexity
of the game [25]. DRL has also been used to achieve superhuman performance in a
range of classic Atari 2600 games, and it is being actively researched for applications
in robotics, autonomous driving, and other complex control tasks.
Coupling the representation power of deep neural networks, DRL solves complex
decision-making problems where state and action spaces can be very large or continuous.
DRL often addresses pure end-to-end learning, where unprocessed inputs such as images
or sensor data can be directly fed into the network without needing hand-engineered
features. Hence, DRL has the potential to enable end-to-end autonomous driving.
Next, we will introduce policy-based approaches used in this research.
2.4.1 Policy-Based Approaches
Policy-based approaches in DRL are designed to directly learn a policy that governs
the agent’s decision-making process. Unlike value-based methods, which estimate the
expected future rewards for different actions in a given state, policy-based methods
learn the mapping from states to actions directly. This mapping is represented by the
policy, denoted as π(a|s), which gives the probability of taking action ain state s. The
objective of policy-based approaches is to optimize the policy parameters so that the
agent can select actions maximizing the cumulative reward over some temporal horizon.
This is achieved through optimization techniques, such as stochastic gradient ascent,
where the policy is updated based on the feedback received from the environment
regarding a reward. In the case of large or continuous action spaces, policy-based
solutions are both more intuitive and scalable compared to value-based methods, as
these may suffer from complexity in such cases.
The core idea behind policy-based approaches is to optimize the policy directly by
maximizing the expected return, which is the sum of all the future rewards with the
discount factor γ. The expected return from a policy πis formally expressed as:
J(π) =Eπ/b acketleftBiggT/summationdisplay
t=0γtRt/b acket ightBigg
(2.4.1)
Where Rtis the reward at timestep t,γthe discount factor, and Tthe time horizon.
15

2 Background
This objective should encourage the agent to take actions that can increase cumulative
rewards over the long run. To achieve this, the agent uses policy gradient methods,
which compute the gradient of the expected return with respect to the policy param-
eters θ. It then uses this gradient to update the parameters to make choosing actions
associated with higher rewards more probable. The approach is based on the Policy
Gradient Theorem, which offers an exact formula for updating policy parameters in
order to maximize the expected return. More specifically, the parameters of a policy
are updated as follows:
∇θJ(πθ) =Eπθ[∇θlogπθ(a|s)·Gt] (2.4.2)
Where Gtis the return (or an estimate of it), and ∇θlogπθ(a|s)is the gradient of
the log of the policy with respect to θ.
A well-known policy gradient algorithm is REINFORCE, which estimates the pol-
icy gradient based on complete episodes. This approach updates the policy after the
entire episode is finished, using the return Gtfor each time step [13]. However, while
REINFORCE is simple, it suffers from high variance in the gradient estimates, which
can slow down learning and make the training process unstable. To address this issue,
Actor-Critic methods combine policy-based and value-based approaches by maintain-
ing both an actor (the policy) and a critic (a value function). The critic estimates the
value of states or actions, and its feedback is used to reduce the variance of the policy
gradient estimates, leading to more stable updates. This combination helps balance
exploring new actions and exploiting known successful ones.
Advanced algorithms like Proximal policy optimization (PPO) and Trust region pol-
icy optimization (TRPO) further improve the stability and efficiency of policy-based
learning [23]. PPO introduces a clipped objective function that prevents large, desta-
bilizing updates, thus ensuring safer and more reliable policy updates. TRPO, on the
other hand, uses a trust region approach to constrain the size of policy updates, ensur-
ing that the policy does not change too drastically in a single step. These algorithms
have been shown to perform well in various tasks, particularly in environments with
complex and high-dimensional action spaces, such as robotic control or video games.
Despite the advantages of policy-based approaches, they also face several challenges.
One of the main difficulties is the high variance in the policy gradient estimates, which
can lead to inefficient learning. Various techniques, such as advantage estimation, aim
to mitigate this problem by providing more stable return estimates. Sample inefficiency
is another challenge; policy-based methods often require large amounts of data to learn
an effective policy, which can be computationally expensive and time-consuming. How-
ever, recent advances, such as Generalized advantage estimation (GAE), have improved
the sample efficiency by reducing the variance in the gradient estimates without intro-
ducing much bias.
In addition to these challenges, policy-based approaches must also address the explo-
16

2.4 Deep Reinforcement Learning
ration vs. exploitation trade-off. The agent must explore different actions to discover
potentially better strategies (exploration) while exploiting known actions that lead to
high rewards (exploitation). A well-designed policy should naturally promote a good
balance between these two aspects. Stochastic policies, which allow for randomization
in the action selection process, are particularly useful in environments where exploration
is crucial for finding optimal solutions.
Policy-based approaches offer a flexible and robust framework for learning optimal
decision-making policies in complex environments. They are especially suitable for tasks
with large or continuous action spaces and situations that require stochastic policies.
While they face challenges such as high variance and sample inefficiency, techniques
like Actor-Critical methods, PPO, TRPO, and advantage estimation have significantly
improved their performance and stability. As such, policy-based methods remain a
central area of research and application in DRL.
2.4.2 Proximal Policy Optimization
The TRPO algorithm has been successfully applied to many applications. However,
we also found that its computation process is too complicated, and the amount of
computation for each update step is enormous. Therefore, an improved version of the
TRPO algorithm was proposed in 2017 [22]. The PPO algorithm is based on TRPO
but is much easier to implement. Plenty of experimental results show that PPO can
learn as well as (or even faster) than TRPO, making PPO a popular DRL algorithm.
When we want to try a RL algorithm in a new environment,PPO is the first algorithm
we try.
There are two variants of PPO: PPO-Penalty and PPO-Clip. The PPO-Penalty
is put directly into the objective function using the Lagrange multiplier method with
the limit of the Kullback-Leibler divergence (KL) dispersion. This becomes an un-
constrained optimization problem. The coefficients in front of the KL divergence are
constantly updated during the iteration process. PPO-Clip is more direct. It directly
clips the updated size of the policy network to ensure that the differences between the
new and old ones are not too large.
The key idea behind PPO is to prevent excessively large policy updates, which can
destabilize training. This is done by constraining the policy’s change between successive
updates using a "clipped" objective function. The clipped objective limits the effect of
any single update on the policy, ensuring that the updates remain within a "trust
region," where the new policy does not deviate too far from the old one. This allows for
more stable training and better overall performance, especially in environments with
complex dynamics.
The objective function is derived from the standard policy gradient method; PPO
modifies the objective function by adding a clipping term that limits how much this
ratio can deviate from 1. The clipped objective function is:
17

2 Background
LCLIP(θ) =Et[min ( rt(θ)At,clip(rt(θ),1−ϵ,1 +ϵ)At)] (2.4.3)
Where:
•Atis the advantage function, which estimates how much better an action is com-
pared to the average.
•ϵis a hyperparameter that controls the clipping range.
•The clip function ensures that if the ratio rt(θ)is outside the range [1−ϵ,1 +ϵ],
the objective does not grow too large, preventing large policy updates.
This clipping ensures that updates are small and safe, leading to more stable training
and less variance. The clip (rt(θ),1−ϵ,1 +ϵ)part of the objective function essentially
limits the change in the policy, forcing the agent to make more conservative updates,
which helps avoid overfitting and instability.
Figure 2.5: Clip Function, positive advantages (left) and negative advantages (right).
The red circle on each plot shows the starting point for the optimization,
i.e., r = 1 [22].
In this study, the PPO algorithm is used to realize end-to-end autonomous driving
due to the following advantages of PPO:
•Simplicity and Efficiency : PPO is easy to implement and more computa-
tionally efficient than methods like TRPO, which requires complex second-order
optimization methods. PPO uses first-order optimization and can be applied to
a wide variety of RL problems.
•Stability : The key strength of PPO lies in its stability. The clipped objective
ensures that each update to the policy is small and does not result in dramatic
changes that could destabilize training.
18

2.4 Deep Reinforcement Learning
•Sample Efficiency : While not as sample-efficient as some other algorithms (like
off-policy methods), PPO strikes a balance between sample efficiency and ease of
implementation, making it suitable for a wide range of environments.
•Generalization : PPO works well in continuous and high-dimensional action
spaces, which makes it a good choice for tasks like robotic control, where the
action space is often continuous.
•On-policy Learning : PPO is an on-policy algorithm, meaning it requires fresh
data (trajectories) each time the policy is updated rather than using previously
collected data. This means that it has a slower learning process compared to
off-policy algorithms like DQN (Deep Q-Network), but it ensures that the policy
is constantly learning from the most current experience.
19



3 Experiment Setup
3.1 CARLA Simulator
The CARLA simulator is an open-source autonomous driving simulator developed by
the Computer Vision Center (CVC) at the Universitat Autònoma de Barcelona, de-
signed to facilitate the development and testing of self-driving vehicles in a virtual en-
vironment. It offers realistic urban environments, incorporating diverse layouts, build-
ings, and dynamic elements such as pedestrians and other vehicles while supporting
various weather conditions and times of day [6]. CARLA is built on Unreal Engine
and features high-fidelity graphics and physics and a modular architecture that allows
researchers to modify and extend its functionalities [10].
The CARLA simulator’s setup comprises essential components that create a robust
and immersive virtual environment for autonomous driving research. Built on the
powerful Unreal Engine, CARLA delivers stunning graphics and an advanced physics
engine, enabling incredibly realistic visual and physical simulations of urban driving
scenarios. The simulator offers various predefined environments featuring diverse urban
layouts, weather conditions, and lighting scenarios. Users can easily customize these
environments to meet specific testing requirements.
CARLA is a powerful simulator that helps researchers test autonomous vehicles.
It supports different sensors, such as cameras, LIDAR, and GPS. Allowing users to
recreate the data that an autonomous vehicle would collect in real life. Users can also
control the actions of other vehicles and pedestrians in the simulation, creating an
interactive environment for testing and validating driving systems.
CARLA provides a safe and customizable virtual space for researchers to explore
autonomous vehicle operations. Using a Python-based client, users can send commands
to control vehicle functions such as steering, acceleration, and braking. They can also
receive real-time data from various sensors. CARLAs flexible Applicaiton programming
interface (API) allows researchers to create and manage different vehicles and customize
multiple cameras and sensors based on their specific needs, helping them with diverse
experiments and studies.
We developed a DRL environment similar to OpenAI Gym using CARLAs Python
API. We implemented basic functionalities such as reset(), step(), and return() .
In this environment, the simulator operates in synchronous mode, meaning it waits for
the client’s task to complete before simulating the next time step. This design ensures
the integrity of the sensor data.
21

3 Experiment Setup
Conversely, running in asynchronous mode could lead to data inconsistencies, mainly
due to the potentially long training times of the PPO network. Such discrepancies may
negatively impact the simulation results.
Understanding the importance of the CARLA simulator’s asynchronous mode is es-
sential for researchers and practitioners in autonomous driving and RL. It can signif-
icantly affect the reliability of simulation results and the subsequent development of
autonomous driving systems.
3.1.1 Vehicle Control
In the CARLA simulator, the carla.VehicleControl function executes actions deter-
mined by the RL model. When an agent representing the autonomous vehicle interacts
with the environment, it receives state information, such as its position, velocity, and
obstacles. Based on this information, the agent selects actions according to its learned
policy.
The model’s output for throttle indicates the level of acceleration the vehicle should
apply, while the brake output determines whether the vehicle needs to decelerate or
stop. This dual control mechanism allows the RL agent to modulate speed effectively
based on the driving context. Additionally, the steering output from the model guides
the vehicle’s direction. As a result, the agent can adjust its trajectory to navigate turns,
avoid obstacles, or maintain proper lane discipline.
Figure 3.1: Vehicle control function parameters in CARLA [4]
The throttle parameter regulates the vehicle’s acceleration, ranging from [0, 1]. A
value of 0 indicates that the driver has not pressed down on the gas pedal, while a value
of 1 signifies that the gas pedal is fully engaged.
The steer parameter controls the vehicle’s steering, and its values range from [-1, 1].
A value of 0 means the steering wheel is aligned straight with the vehicle’s orientation,
while positive values indicate a turn to the right, and negative values indicate a turn
to the left.
22

3.1 CARLA Simulator
After the RL model produces an action, the corresponding carla.VehicleControl
object is transmitted to the CARLA simulator, which interprets these instructions and
adjusts the vehicle’s state in the simulation accordingly. This interaction establishes a
feedback loop: the vehicle acts, the environment reacts (for instance, by altering states
or introducing new obstacles), and the RL model acquires updated state data to guide
future actions.
3.1.2 Maps
As the most advanced open-source autonomous driving simulator, CARLA not only
provides users with colorful maps and interactive environments but also has an internal
map design that is full of depth and detail, making it ideal for all kinds of autonomous
driving research and validation.
A map includes both a town’s 3D model and its road definition. The latter is an
OpenDRIVE file, a standardized, annotated road definition format [2]. There are seven
maps available in CARLA. Each one has unique features and is suitable for different
objectives. CARLA delivers users impressive maps and dynamic interaction environ-
(a) Town01
 (b) Town02
 (c) Town03
 (d) Town05
Figure 3.2: Maps of towns from CARLA used in this study [10]
ments. Its highly detailed internal map design provides a solid foundation for various
autonomous driving research and validation initiatives. Each map comprises a well-
constructed 3D model of a town and precise road definitions formatted in OpenDRIVE,
a standardized and annotated road representation protocol. Currently, CARLA fea-
tures seven distinct maps, each with unique attributes tailored to address different
research objectives. Four maps from CARLA are utilized in this research to incor-
porate multiple types of scenarios for testing. Town02 has been used as the primary
map for model training. This specific map effectively simulates a typical small-town
environment characterized by numerous T-junctions and diverse structures and land-
scapes. Within Town02, multiple routes have been created to facilitate the training of
the autonomous driving model across various scenarios and driving situations, ensuring
a comprehensive and effective learning process.
23

3 Experiment Setup
Figure 3.3: Top view of Town02 used for training [4]
3.1.3 Waypoints and Routes
Waypoints in the CARLA simulation are defined as specific 3D points with positional
and orientational data. Each waypoint is associated with a carla.Transform object,
which specifies its precise location on the road in three-dimensional space and its orien-
tation. This orientation is crucial for understanding how vehicles should approach and
navigate the waypoint. Furthermore, waypoints contain detailed information about
the road environment, including descriptions of the lanes they belong to and any rele-
vant lane markings that guide vehicle movement. The API allows for retrieving various
Figure 3.4: Example route generated with waypoints marked in blue dots [4]
24

3.2 Environment Setup
variables essential for various calculations needed in driving simulations. These calcula-
tions include determining the lateral distance from the vehicle to the center of the lane,
assessing the orientation of the road, calculating lane width, identifying the direction
of the lane, and providing details about other pertinent features such as traffic signals
and road signs.
For the purposes of training and testing autonomous driving models, a route gener-
ation function is utilized to create diverse and dynamic routes for each episode. This
function generates routes of varying lengths and incorporates different navigation chal-
lenges, such as intersections, turns, and obstacles. This approach promotes more ro-
bust learning by exposing models to various scenarios and environmental conditions,
enabling the algorithms to adapt and perform effectively in real-world situations.
3.2 Environment Setup
OpenAI Gymnasium is a widely used toolkit for developing and comparing RL algo-
rithms, providing a standard interface for environments that facilitates the creation,
sharing, and evaluation of RL agents [3]. In the context of the environment class, which
simulates an autonomous vehicle within the CARLA simulator, several key methods
are implemented: __init__() ,reset() ,step() , and clean_up() . The __init__()
method initializes the environment by defining the action and observation spaces, cru-
cial for the agent’s understanding of possible actions and the state information it will
receive. The observation space consists of five dimensions, including lateral distance
from the lane center, normalized speed, current heading, future heading, and input for
traffic light detection, each normalized to specific ranges to standardize input for the
learning algorithm. This setup allows the agent to interpret its current state effectively
and make informed decisions.
At the beginning of each episode, the reset() method is called to clear previous
episode data, spawn a new vehicle, generate a new route, and return the initial ob-
servation for the agent. Including a dynamic route generation mechanism ensures
variability in training scenarios, which prevents overfitting and promotes robustness in
the learned policies. The step() method drives the interaction between the agent and
the environment by applying the agent’s actions—such as throttle, brake, and steer-
ing—to the vehicle. It subsequently updates the simulation state through the CARLA
engine’s tick function, calculates new observations and rewards based on the vehicle’s
performance, and determines whether the episode should terminate based on condi-
tions like reaching the destination, collisions, or exceeding a time limit. Finally, the
clean_up method manages resources by destroying unnecessary actors and resetting
the simulation state, ensuring that every episode starts fresh without residual effects
from previous interactions.
This structured approach, facilitated by the Gymnasium framework, enables effective
25

3 Experiment Setup
training and evaluation of RL agents, allowing them to learn complex driving behav-
iors in a dynamic environment. The modular design promotes reusability and fosters
experimentation with various algorithms and training strategies, making it a powerful
tool for advancing research in autonomous driving.
3.3 Deep Reinforcement Learning Setup
In our study, the input for the DRL model is the features collected from the CARLA
simulator, which is used as the input vector; this lower dimensional vector is used as
the input for the PPO algorithm. The Environment class simulates an autonomous
vehicle. Its observation space provides critical information about the vehicle’s state
and surroundings. The observations are encapsulated in a NumPy array of five key
dimensions, each representing different aspects of the vehicle’s current status and en-
vironment. Below is a detailed description of each observation component.
3.3.1 State Space
Table 3.1: State space of the environment
Description Symbol
Lateral Distance from Lane Center d1,norm
Normalized Speed vnorm
Normalized Future Heading θ1,norm
Normalized Heading θ2,norm
Normalized Distance to Traffic Light d2,norm
The observations made available to the autonomous vehicle are integral to its abil-
ity to make informed decisions based on its current state and the conditions of its
surroundings. By normalizing these observations, the environment ensures that the ve-
hicle can effectively learn and adapt its behavior in various driving scenarios, ultimately
contributing to more robust and reliable autonomous driving capabilities.
Lateral Distance from Lane Center ( d1,norm):This observation represents the
lateral distance of the vehicle from the center of the lane. Understanding how well the
vehicle is positioned within the lane boundaries is crucial.
d1,norm=Lateral Distance
Max Lateral Distance(3.3.1)
Where:
Lateral Distance =norm (Vehicle Position −Waypoint Position ) (3.3.2)
26

3.3 Deep Reinforcement Learning Setup
Figure 3.5: Illustration of some state values [10]
The value is normalized to lie within the range [−1,1]. A value of 0 indicates that
the vehicle is perfectly centered in the lane, while values approaching -1 or 1 indicate
that the vehicle is increasingly off-center, either to the left or right, respectively. The
extremes of the range would denote that the vehicle is outside the designated lane.
Maintaining a proper lateral position is vital for safe driving, particularly in avoiding
collisions with lane boundaries or other vehicles. It informs the vehicle’s control strategy
regarding steering adjustments needed to stay centered.
Normalized Speed ( vnorm):This observation reflects the vehicle’s speed, nor-
malized to a range of [−1,1]. We calculate the speed based on the vehicle’s current
velocity and divide it by the max speed (e.g., 60 km/h). The normalization process
defines speed relative to the max speed. The normalized speed is calculated from the
vehicle’s velocity vector:
vnorm=Speed
Max Speed(3.3.3)
Where:
Speed = 3.6×/ adicalBig
v2x+v2y+v2z (3.3.4)
Here, vx,vy, and vzare the components of the vehicle’s velocity vector, and the
factor of 3.6 converts the speed from meters per second (m/s) to kilometers per hour
27

3 Experiment Setup
(km/h).
A value of 0 means a complete stop (0 km/h), and a value of 1 means the max
speed, 60 km/h. Speed is a critical factor in driving behavior, influencing acceleration,
braking, and general vehicle dynamics decisions. This observation helps the vehicle
assess whether it is moving within a safe and acceptable speed range.
Normalized Future Heading ( θ1,norm)This is the expected future heading of the
vehicle relative to the generated route. The value indicates the direction the vehicle
should optimally follow the planned route. The future heading is calculated based on
the yaw difference between the current waypoint and the next few waypoints ahead:
θ1,norm=Future Yaw Difference
180(3.3.5)
Where:
Future Yaw Difference =n/summationdisplay
i=1|Yaw current −Yaw future ,i| (3.3.6)
This is taken over a number nof future waypoints. The heading is normalized to
the range [−1,1], with 0 representing straight alignment with the waypoint and values
approaching -1 or 1 indicating sharp left or right turns, respectively. Understanding
future heading is essential for navigation and trajectory planning. This lets the vehicle
expect needed steering adjustments to stay on course.
Normalized Heading ( θ2,norm):This observation captures the vehicle’s current
heading relative to the waypoint’s orientation. It explains how well the vehicle aligns
with the intended route’s direction. The normalized heading is calculated similarly
to the future heading but focuses on the current heading relative to the waypoint’s
orientation:
θ2,norm=Relative Yaw
180(3.3.7)
Where:
Relative Yaw =Yaw waypoint −Yaw vehicle (3.3.8)
This value is normalized to ensure it fits within the range of [−180,180].
Similar to the future heading, this value is normalized to [−1,1], where 0 indicates
perfect alignment, while -1 and 1 signify substantial deviations to the left or right,
respectively. The normalized heading is crucial for assessing the vehicle’s orientation
and determining adjustments needed in steering to stay on course. It plays a significant
role in the vehicle’s ability to execute turns and maintain lane discipline.
Normalized Distance to Traffic Light( d2,norm):This observation provides in-
formation about the distance from the vehicle to the nearest traffic light, normalized
to the range [0,1]. It indicates how close the vehicle is to the traffic light relative to a
predefined maximum distance (e.g., 5 meters).
28

3.3 Deep Reinforcement Learning Setup
d2,norm=Distance to Traffic Light
Max Distance(3.3.9)
Where:
Distance to Traffic Light =Distance (Vehicle Position ,Traffic Light Position )
This distance is then normalized based on a predefined maximum distance (e.g., 5
meters), which is the stopline from the traffic light:
Max Distance = 5
A value of 0 indicates that the traffic light is at the closest point (e.g., less than
5 meters away), while a value of 1 indicates that the vehicle is far from the traffic
light (beyond 5 meters). The traffic light distance value changes only when the traffic
light state is red or yellow. The distance to the traffic light is critical for assessing
whether the vehicle needs to stop or slow down. This observation helps the vehicle
make decisions responding to traffic light signals (e.g., green, yellow, red) and manage
its speed accordingly to ensure safe stops at intersections. The observation vector is
typically represented as:
Observation =
d1,norm
vnorm
θ1,norm
θ2,norm
d2,norm
(3.3.10)
Provides a comprehensive view of the vehicle’s positional and operational context.
Each dimension of the observation is designed to inform the decision-making process
of the autonomous vehicle, enabling it to navigate safely and efficiently through its
environment. The observations made available to the autonomous vehicle are integral
to its ability to make informed decisions based on its current state and the conditions
of its surroundings. By normalizing these observations, the environment ensures that
the vehicle can effectively learn and adapt its behavior in various driving scenarios,
ultimately contributing to more robust and reliable autonomous driving capabilities.
3.3.2 Action Space
In this context, the action space is continuous, allowing for a wide range of values for
each action variable. The action space consists of two primary actions:
1.Throttle : This action controls the vehicle’s acceleration. In typical RL scenarios,
throttle values are normalized to a range, allowing for smooth control over acceleration.
29

3 Experiment Setup
In this environment, the throttle action can take values in the continuous range [−1,1],
where: - A value of 0 indicates no acceleration (the vehicle maintains its speed).
- Positive values (0 to 1) are used to accelerate the vehicle.
- Negative values (-1 to 0) can be interpreted as a command to reverse the vehicle or
apply braking.
2.Steer : This action controls the direction in which the vehicle is pointed. Like
throttle, the steering action is also represented in a continuous range, typically [−1,1].
Here:
- A value of 0 indicates no change in the vehicle heading.
- Positive values (0 to 1) represent turning the vehicle wheel right, with increasing
values leading to sharper turns.
- Negative values (-1 to 0) represent turning the vehicle wheel left, again with sharper
turns corresponding to more negative values.
The continuous nature of the action space allows for finer control over the vehicle’s
movement, enabling the agent to perform nuanced maneuvers such as gentle acceler-
ation, braking, and smooth turning. This granularity is particularly important in a
driving environment where abrupt changes in speed or direction could lead to unsafe
situations or collisions.
Additionally, the choice of action space structure is essential for the learning process.
It allows the agent to explore various driving strategies, optimizing its policy based
on the feedback it receives in the form of rewards from the environment. Properly
defining and normalizing the action space enhances the agent’s ability to learn efficiently
and effectively, contributing to improved performance in navigating complex driving
scenarios.
3.3.3 Reward Function
Designing an optimal reward function is crucial to RL as it directly influences the
agent’s learning trajectory and overall performance. The reward function must effec-
tively encapsulate the objectives of the task while minimizing the risk of unintended
behaviors that can arise from poorly structured incentives. In our scenario, the reward
function comprises multiple components: rewards for maintaining speed, staying within
designated lanes, following routes, avoiding collisions, and adhering to signals.
To accomplish the autonomous navigation task, the vehicle should travel along the
centerline of the lane. Under ideal conditions, its lateral deviation and heading devia-
tion are zero, and its speed should closely approach the desired speed and follow the
traffic signals. In this study, the agent is penalized for bad behavior, so the agent’s
goal is to minimize the penalty. Based on these metrics, we have designed a reward
function as follows:
1.Speed Reward rspeed:The agent is incentivized to maintain a speed within
a defined range. No reward or penalty is administered if the agent’s speed remains
30

3.3 Deep Reinforcement Learning Setup
within this optimal range. However, should the speed fall below 0.25 or exceed 0.50,
the agent incurs a penalty. This penalty is computed using a logistic function, ensuring
a gradual transition in penalty severity as the speed deviates from the optimal range.
This framework encourages the agent to sustain an appropriate velocity. If speed <0.25
(underspeed):
rspeed =/pa enleftbigg
−5
1 +e−k·(0.25−v)/pa en ightbigg
(3.3.11)
If speed >0.50(overspeed):
rspeed =/pa enleftbigg
−5
1 +e−k·(v−0.50)/pa en ightbigg
(3.3.12)
Where vthe speed kis the steepness parameter for the penalty curve.
2.Lateral Distance Reward rld: The agent receives a penalty for deviating
laterally from the center of its lane. The penalty correlates to the square of the lateral
distance, indicating that more significant deviations from the center result in more
extensive penalties. This structuring incentivizes the agent to remain positioned in the
center of the lane.
rld=−5×d1,norm (3.3.13)
3.Relative Heading Reward ryaw: The agent faces penalties for deviations in its
heading relative to the desired trajectory. This penalty is determined using a logistic
function, which intensifies as the deviation from the desired heading increases. This
design promotes the maintenance of the correct heading direction.
ryaw=−/pa enleftbigg10
1 +e−10·θ2,norm−5/pa en ightbigg
(3.3.14)
Where θ2,norm is absolute heading deviation.
4.Steering Change Reward rsteer:The agent is penalized for making frequent
or abrupt adjustments to the steering angle. The penalty is proportional to the square
of the change in the steering angle, thereby encouraging the agent to execute smooth
and gradual steering adjustments. This leads to improved driving behavior.
rsteer=−0.5×(∆steer)2(3.3.15)
Where ∆steeris the change in the steering angle.
5.Traffic Light Compliance Reward rlights:The agent incurs penalties for
failing to stop at red or yellow traffic lights. The severity of the penalty escalates as the
agent approaches the traffic signal without halting. This mechanism fosters adherence
to traffic regulations.
Ifv >0, R lights =−Fpenalty ·v·4 (3.3.16)
Where vis the speed, and Fpenalty is the penalty factor calculated from the distance
31

3 Experiment Setup
from the traffic light. The overall reward Rtcombines the Speed Reward rspeed, Lateral
Distance Reward rld, Relative Heading Reward ryaw, Steering Change Reward rsteer,
and Traffic Light Compliance Reward rlights to guide the vehicle’s behavior. The total
reward is given by a weighted sum of the individual components:
Rt=wspeed(rspeed) +wld(rld) +wyaw(ryaw) +wsteer(rsteer) +wlights(rlights)(3.3.17)
Where wspeed, wld, wyaw, wsteer, wlights are weights that represent the relative impor-
tance of different components of the reward function. A larger weight value means
that the corresponding reward component is more important. Tuning the weights of
these components is vital for balancing competing priorities and guiding the agent
efficiently. For instance, assigning a high weight to collision penalties promotes safe
navigation while placing greater emphasis on lane-keeping rewards enhances precision
in driving. On the other hand, excessively penalizing certain actions may deter explo-
ration, restricting the agent’s capacity to learn optimal behaviors. To determine an
ideal configuration, we engage in iterative experimentation, adjusting the weights and
assessing the agent’s performance through episodic returns and behavioral consistency
metrics.
The tuning process involves systematically varying the weights and analyzing their
effects on the reward distribution and the agent’s learned policy. Visualizing reward
curves over training episodes can help identify potential issues, such as reward stag-
nation, high variance, or unintended reward maximization. By balancing conflicting
objectives through an optimal weight configuration, the agent can prioritize safety and
efficiency while successfully navigating tasks. This careful design of the reward func-
tion ensures that the agent’s behavior aligns with real-world expectations and task
goals within the CARLA environment.
Figure 3.6: Overview of the proposed training setup
The termination conditions are predicated upon the predefined episode time limit.
In the event that the agent collides with another object, it incurs a penalty, and its
32

3.4 Network Architecture
position is subsequently reset. Should the agent reach the designated time limit for an
episode, the episode is deemed terminated, and the agent’s state is reset accordingly.
Upon the agent’s successful navigation to the end of the route, a new route is generated
based on the agent’s current position. These termination conditions serve to encourage
the agent to avoid collisions and to complete the designated task within the stipulated
time limit.
3.4 Network Architecture
In our research, we utilize the PPO algorithm to train an agent. PPO is a policy-
gradient method that balances exploration and exploitation by iteratively optimizing
the policy to maximize cumulative rewards. The PPO algorithm trains policy and value
networks simultaneously. The role of the policy network is to determine the appropriate
action for the agent to take in the current state based on the input observation space,
while the value network is responsible for estimating the value of the current state,
which refers to the expected cumulative rewards the agent can obtain from that state.
The value network’s output is used to calculate the advantage function, which eval-
uates how favorable a particular action is relative to the average performance. This
advantage function guides the optimization of the policy network’s parameters, en-
hancing the agent’s decision-making capability. Policy optimization and value function
estimation.
The input observation space is represented as a five-dimensional vector. The input
layer accepts the state observations from the environment. Each observation is a vector
of features that describe the current state. The size of this layer corresponds to the
dimensionality of the state space defined by the environment.
The hidden layers are fully connected (dense) with non-linear activation functions.
These layers allow the network to learn complex patterns and relationships between
states and actions. By default, MlpPolicy includes two hidden layers. Each hidden
layer contains 64 neurons (by default), although this can be customized. Activation
Function: Rectified Linear Unit (ReLU) is used as the activation function, defined as:
f(x) = max(0 , x) (3.4.1)
ReLU introduces non-linearity, enabling the network to approximate complex func-
tions. The output layer of the network architecture consists of two distinct heads - the
Policy Head and the Value Head. The Policy Head outputs a probability distribution
over possible actions within the action space. For discrete action spaces, this head
provides the probabilities for each action, and for continuous action spaces, it outputs
the mean and standard deviation of a Gaussian distribution. The activation function
used is a softmax function for discrete actions to ensure that the probabilities sum
to one, while no activation is applied for continuous actions as the outputs represent
33

3 Experiment Setup
parameters of a distribution.
On the other hand, the Value Head outputs a single scalar value, which represents
the estimated cumulative future reward, also known as the value function, for a given
state. As the value can range over real numbers, a linear activation function is used for
the value head. This structure is highly efficient for RL tasks, where the policy is used
to decide actions, and the value function assists in policy evaluation and advantage
estimation.
3.5 Model Training
During training, the agent interacts with its environment, updating its policy and value
function to learn how to choose optimal actions that maximize returns. The process
begins by defining the state space, action space, reward function, and parameters.
Initially, the agent samples actions randomly to begin its training. The next step is
initializing the agent’s state, randomly occurring within the environment. As training
progresses, the agent selects actions based on the current state and its existing pol-
icy, receiving feedback from the next state and a corresponding reward. This process
continues until the maximum number of training steps is reached. The state, action,
reward, and next state are stored in an experienced pool, and a random batch is se-
lected to update the policy and value function. This process repeats until training is
complete.
The training duration was set at 2 million timesteps. Preliminary runs revealed
that the total episode reward increased initially before plateauing, suggesting that the
agent successfully learned an optimal or near-optimal policy for the environment. Loss
curves associated with policy optimization—including policy loss, value loss, and en-
tropy loss—appeared to stabilize after a certain number of timesteps. These patterns
indicate that additional training would not lead to significant performance improve-
ments.
Capping the training at 2 million timesteps allowed for more efficient use of compu-
tational resources, ensuring robust model performance without the risk of overfitting.
This approach aligns with established practices in RL, where training is typically halted
once key metrics indicate convergence and stable learning.
In addition to fine-tuning the model’s hyperparameters, optimizing the CARLA sim-
ulation environment is essential for enhancing the performance of the RL agent. A key
parameter to consider is the fixed time step of the CARLA simulator, which dictates
the interval between simulation updates. By adjusting the fixed time step, one can
achieve greater control over the simulation’s granularity; smaller time steps provide
smoother transitions and more precise agent interactions, while larger steps can reduce
computational overhead. Thoughtful selection of the time step ensures a balance be-
tween computational efficiency and the fidelity of the agent’s learning experience. The
34

3.5 Model Training
CARLA simulator has been configured to operate in asynchronous mode with a fixed
time step of 0.04 seconds (25 Hz). The model was trained for up to 2 million steps
within the Town02 map of the CARLA simulator, using randomly generated routes to
create a diverse training environment.
Table 3.2: PPO model parameters
Parameter Value
learning rate 0.0003
batch size 128
clip range 0.1
reward discount factor 0.99
time steps per update 1024
total time steps 2,000,000
Hyperparameter tuning is essential for optimizing RL models. In this study, we sys-
tematically adjusted key hyperparameters like learning rate, n_steps, batch size, and
clip range to find the best configuration. The learning rate influences the magnitude
of updates during training; lower rates stabilize training but slow convergence, while
higher rates accelerate learning but can overshoot solutions. N_steps control how many
environment interactions are collected before updating the policy, with larger values
enhancing sample efficiency but increasing memory usage. The batch size affects the
stability and speed of updates, as smaller batches produce noisier updates. The clip
range in Proximal Policy Optimization (PPO) limits policy updates to prevent drastic
changes that might destabilize learning. Through experiments with various hyperpa-
rameter configurations and evaluation metrics like average episodic reward and loss
convergence, we identified an optimal combination that maximized performance and
stability, highlighting the importance of systematic hyperparameter tuning in develop-
ing robust RL models.
To effectively track and visualize training progress, monitoring how the reward value
curve evolves is important. As the number of training iterations increases, we ideally
want the reward value to trend towards a maximum level close to zero. This trend
indicates that the agent gradually learns an optimal strategy through accumulated ex-
periences, aiming to maximize each state’s reward value. By observing these changes
closely, we can gain valuable insights into the agent’s learning process and make in-
formed adjustments as needed. Figure (3.7) illustrates that the model’s reward values
converge to zero. Regarding training time, the model took 4.2 hours to complete 2
million time steps.
The graph 3.7 illustrates the training progression of the RL agent by plotting the
average episodic reward against timesteps, aggregated over multiple random seeds.
35

3 Experiment Setup
Figure 3.7: Average episodic reward achieved against timesteps in Town02 CARLA
Initially, the agent’s performance is subpar, as evidenced by low and highly variable
rewards during the exploratory phase, wherein the agent has yet to learn an optimal
policy. As training advances, the average episodic reward, represented by the dark blue
line, increases sharply, highlighting the agent’s growing ability to optimize actions and
enhance performance. The variability around the line across different seeds diminishes
over time, signifying that the agent’s behavior is stabilizing and becoming more consis-
tent. Despite this improvement, the agent’s performance falls short of the theoretical
maximum reward, indicated by the red dashed line. This lack of full attainment sug-
gests potential limitations in the policy, inherent stochasticity in the environment, or
challenges associated with the design of the reward function. Ultimately, the rewards
plateau, signaling convergence or near-convergence in the agent’s learning process.
In practice, several training attempts were required to achieve a well-trained model.
The initial training efforts failed due to various factors, such as unreasonable reward
function values and hyperparameters. The training environment was also adjusted to
address the instability of reward function curves. Accurate and simple input informa-
tion is crucial for the DRL agent: the more precise and straightforward the input, the
better the agent can learn the desired driving strategies.
36

4 Evaluation
Figure 4.1: Performance of the trained model in Town02. (top: Lateral deviation from
lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The red
splits are used to denote when the traffic light is red.
The evaluation of the RL agent in Town02 figure 4.1 was conducted over 10 episodes,
focusing on its ability to maintain lateral stability and adhere to speed regulations
while following the route in a dynamic environment. The lateral stability component
is also critical while making turns and at intersections, as it also demonstrates the
agent’s ability to steer and maintain its lane. The lateral deviation plot demonstrates
37

4 Evaluation
that the agent performs well in lane-keeping tasks, with deviations primarily staying
within a controlled range of 40cm on either side of the lane center. Which indicates
well-controlled driving. The observed deviations in lateral deviation align with the
red light states, which correspond to junctions and turns being executed by the agent.
Then, it slowly moves back to the lane center. This indicates the agent’s adaptabil-
ity in navigating intersections and adhering to traffic signals. The speed plot further
showcases the agent’s effective response to red traffic light states, marked by the red
regions, with smooth deceleration and appropriate stopping behavior. The agent accel-
erates and maintains a constant speed of 18-24 kmph. These results highlight the RL
agent’s ability to handle junctions and traffic light compliance scenarios, demonstrating
promising progress toward robust driving performance.
4.1 Evaluating Agent on New Maps
This section examines the performance of the agent trained in Town02 and tested
across three distinct maps: Town01, Town03, and Town05. Key performance met-
rics—speed regulation, lane-keeping ability, and heading alignment—were evaluated
over ten episodes across various routes within each map. Observations were visualized
in plots to analyze the model’s adaptability and effectiveness in unfamiliar environ-
ments. The CARLA simulator replayer feature was used to record video and analyze
the agent’s capacity to handle different scenarios.
To comprehensively evaluate the robustness of our trained model, we incorporated
unfamiliar environments to identify any signs of overfitting. Testing was conducted
under challenging driving conditions in Town01, Town03, and Town05. Figure 3.2 il-
lustrates the top view of the map. Unlike Town02, Town03 presents a more intricate
road network, featuring a labyrinth of streets and diverse traffic scenarios that reflect
real-world unpredictability. This environment includes multiple lanes, extended turns,
and new challenges such as cross intersections and roundabouts. Town01 bears a resem-
blance to Town02 but has a more expansive layout. In contrast, Town05 represents the
most complex environment, with 4-lane roads, numerous cross junctions, and highway-
like roads. This rich diversity in conditions establishes these towns as ideal settings for
evaluating the model’s adaptability and resilience.
38

4.1 Evaluating Agent on New Maps
Figure 4.2: Performance of the trained model in Town01. (top: Lateral deviation from
lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The red
splits are used to denote when the traffic light is red.
In Town01, as illustrated in Figure 4.2, the RL agent exhibited commendable per-
formance, signifying a successful transfer of learned behaviors from the training envi-
ronment. The agent’s lateral deviation from the lane center typically remained within
40 cm, with most episodes showing deviations less than 20 cm. Moreover, it displayed
effective speed regulation, experiencing only occasional fluctuations. The agent main-
tained its lane, adhering closely to the designated route with minimal deviations. The
episodic rewards were on par with those in Town02, underscoring the agent’s adapt-
ability. When errors occurred, they were minor and swiftly corrected. The agent en-
countered no collisions over ten episodes. Overall, the agent’s performance in Town01
was impressive, reflecting an effective transfer of learned behaviors from the training
environment.
39

4 Evaluation
Figure 4.3: Performance of the trained model in Town03. (top: Lateral deviation from
lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The red
splits are used to denote when the traffic light is red.
In contrast, as shown in Figure 4.3, the agent’s performance in Town03 was inconsis-
tent. It demonstrated a strong ability to adapt to the more complex traffic environment;
however, it faced challenges with lane-keeping due to the heightened complexity of the
scenario. This was evident from the sharp increases in lateral deviation, which reached
up to 2 meters in some instances. The difficulties arose from the multiple lanes and in-
tricate intersections in that area. Although the agent was observed straying outside its
designated lane several times, no collisions occurred. Overall, the increased complexity
in Town03, characterized by numerous lanes, detailed intersections, and roundabouts,
negatively impacted the agent’s performance.
40

4.1 Evaluating Agent on New Maps
Figure 4.4: Performance of the trained model in Town05. (top: Lateral deviation from
lanecenter over 10 episodes; bottom: Speed over 10 episodes.) The red
splits are used to denote when the traffic light is red.
The performance observed in Town05, as illustrated in Figure 4.4, was notably er-
ratic. While the agent appropriately responded to traffic signals, it struggled to main-
tain lane centering consistently during transitions. The lane-keeping performance was
subpar, marked by frequent deviations and delayed corrections. This issue was partic-
ularly evident on the longer, curved roads with multiple lanes, as analyzed in the video
replay. Additionally, the agent consistently adhered to traffic signals across all maps.
However, it frequently failed to maintain proper heading alignment, often deviating
from the intended route and requiring extended periods to realign itself.
41

4 Evaluation
Table 4.1: Performance metrics across different maps
Metric Town02 Town01 Town03 Town05
Avg. lateral deviation 0.141 0.161 0.411 0.731
Avg. speed 18.265 18.636 19.527 18.860
Collision 0 0 0 0
Avg. reward per episode -367.272 -347.067 -1207.412 -1783.514
The performance metrics summarized in Table 4.1 reveal the variability in the RL
agent’s behavior across different simulated environments. Notably, the agent exhibited
optimal performance in Town01, characterized by superior directional control, mini-
mal lateral deviations, and the highest reward outcomes, suggesting its effectiveness
in this setting. In contrast, Town05 proved to be the most challenging environment,
exhibiting significant lateral deviations, pronounced heading errors, and the lowest re-
wards—indicative of the difficulties posed by its complex layout. Despite this, the agent
maintained a consistent operational speed across the various towns, reflecting effective
velocity management, though slight variations were noted in specific instances. These
insights are critical for enhancing the agent’s algorithms, particularly for improving
adaptability in more complex scenarios like Town05 to achieve robust performance
across diverse environments. There were no collisions in all the tests; in Town01 and
Town02, it was due to optimal lane keeping and route following ability shown, and in
Town03 and Town05, it was also able to follow the route and try to maintain lane center
and the multi-lanes being present keeping the vehicle in the road even when outside
the assigned lane.
The observed discrepancies in performance across different maps highlight the gen-
eralization challenges RL agents trained in controlled environments face. The agent’s
relatively strong performance in Town01 suggests that certain learned behaviors can
be generalized in settings that share characteristics with the training map. However,
the performance decline in Town03 and Town05 underscores the model’s limitations in
adapting to a broader range of scenarios and environments. Key factors influencing
this variability include map complexity, potential training biases, and the robustness
of the agent’s policy.
The agent experiences substantial hurdles in unfamiliar contexts characterized by
increased complexity, such as multi-lane intersections, highway configurations, and
roundabouts. These challenges are particularly evident in Town03 and Town05. Such
environments introduce dynamic challenges, intricate merging or lane-switching tasks,
and ambiguous navigation cues. These complexities strain the agent’s decision-making
framework, leading to increased lateral deviations, elevated heading errors, and a no-
table reward decrease.
42

5 Conclusion
In conclusion, this thesis has demonstrated the effectiveness of a DRL agent trained
within the CARLA simulator to navigate urban driving environments. Through rigor-
ous training and evaluation across diverse town maps, the study has highlighted the
agent’s ability to adapt to varying driving conditions while maintaining a high level
of performance. The evaluation metrics, including lateral distance, speed, heading de-
viation, and reward values, illustrate the agent’s competency in essential aspects of
autonomous driving. The agent’s ability to minimize lateral deviations, particularly in
Town02 (0.141m average lateral deviation), indicates a firm grasp of lane navigation
and trajectory adherence—critical elements for ensuring safe and efficient driving. Sim-
ilarly, its consistent speed regulation across towns demonstrates well-calibrated control
over velocity, further reflecting its ability to maintain stable and safe driving behavior.
The research also emphasized the critical role of a well-defined reward function and
carefully tuned training parameters. Adjustments to the reward structure and training
environment played a significant role in stabilizing the learning process, leading to the
development of a robust model capable of handling various real-world complexities.
For instance, the agent’s relatively strong performance in Town01 and Town03 sug-
gests that the iterative refinements made during training successfully mitigated issues
related to overfitting, enabling the model to perform effectively in previously unseen
environments.
Moreover, the agent’s ability to adhere to traffic regulations and maintain direc-
tional stability, as demonstrated by lower heading deviations and stable speed ranges,
reinforces its competency in handling urban driving conditions. These findings high-
light the potential of DRL in autonomous driving, providing a foundation for further
advancements and practical applications in this field.
As urban spaces evolve and present new challenges, such as more dynamic traffic
patterns and increasingly complex road networks, the methodologies and insights de-
veloped in this study can be refined and extended. Future progress and betterment
could enhance the agent’s ability to generalize and adapt to novel scenarios. This
research not only underscores the viability of DRL for autonomous driving but also
provides actionable pathways for advancing the safety, efficiency, and scalability of
autonomous vehicles.
43



Bibliography
[1] National Highway Traffic Safety Administration. „Critical Reasons for Crashes
Investigated in the National Motor Vehicle Crash Causation Survey“. In: (2015).
[2]ASAM OpenDRIVE ®1.8.1, Static Road Network Description .https://www.
asam.net/standards/detail/opendrive/ .
[3] Greg Brockman et al. „OpenAI Gym“. In: CoRR abs/1606.01540 (2016).
[4]CARLA Simulator — carla.readthedocs.io .https://carla.readthedocs.io/
en/latest/ .
[5] L. Chen et al. „DeepDriving: Learning Affordance for Direct Perception in Au-
tonomous Driving“. In: Proceedings of the IEEE International Conference on
Computer Vision (ICCV) . 2015.
[6] A. Dosovitskiy et al. „CARLA: An Open Urban Driving Simulator“. In: Proceed-
ings of the Conference on Robot Learning (CoRL) . 2017.
[7] M. Everingham et al. „The Pascal Visual Object Classes (VOC) Challenge“. In:
International Journal of Computer Vision (2010).
[8] Daniel J Fagnant and Kara Kockelman. „Preparing a Nation for Autonomous Ve-
hicles: Opportunities, Barriers and Policy Recommendations“. In: Transportation
Research Part A: Policy and Practice 77 (2015), pp. 167–181.
[9] A.R. Fayjie, M.K. Hasan, T. Neubert, et al. „Deep Reinforcement Learning Based
Autonomous Driving in Urban Environments Using CARLA“. In: Sensors (2020).
[10] GitHub - carla-simulator/carla: Open-source simulator for autonomous driving
research. — github.com .https://github.com/carla-simulator/carla.git .
[11] Corey D Harper, Chris T Hendrickson, and Constantine Samaras. „Estimating
the Mobility Benefits of Autonomous Vehicles for the Elderly and People with Dis-
abilities“. In: Transportation Research Part C: Emerging Technologies 53 (2016),
pp. 1–13.
[12] J. Lee, J. Lee, J. Park, et al. „Bridging the Gap: Combining Real-World Human
Driving Data with Deep Reinforcement Learning for Autonomous Driving“. In:
IEEE Transactions on Intelligent Transportation Systems (2022).
[13] Matthias Lehmann. The Definitive Guide to Policy Gradients in Deep Reinforce-
ment Learning: Theory, Algorithms and Implementations . 2024.
45

Bibliography
[14] Dianzhao Li, Paul Auerbach, and Ostap Okhrin. Towards Autonomous Driving
with Small-Scale Cars: A Survey of Recent Development . Apr. 2024.
[15] X. Liang, T. Xu, X. Chen, et al. „End-to-End Learning for Autonomous Driving
Using Guided Policy Search“. In: Advances in Neural Information Processing
Systems (NeurIPS) . 2018.
[16] T.P. Lillicrap, J.J. Hunt, A. Pritzel, et al. „Continuous control with deep rein-
forcement learning“. In: Proceedings of the International Conference on Learning
Representations (ICLR) . 2016.
[17] Todd Litman. „Autonomous Vehicle Implementation Predictions: Implications
for Transport Planning“. In: Victoria Transport Policy Institute (2020).
[18] V. Mnih, K. Kavukcuoglu, D. Silver, et al. „Human-level control through deep
reinforcement learning“. In: Nature (2015).
[19] Volodymyr Mnih et al. „Playing Atari with Deep Reinforcement Learning“. In:
(Dec. 2013).
[20] Cynthia Rudin. Stop Explaining Black Box Machine Learning Models for High
Stakes Decisions and Use Interpretable Models Instead . May 2019.
[21] David Schrank, Bill Eisele, and Tim Lomax. „Urban Mobility Report“. In: (2019).
[22] John Schulman et al. Proximal Policy Optimization Algorithms . July 2017. doi:
10.48550/arXiv.1707.06347 .
[23] John Schulman et al. „Trust Region Policy Optimization“. In: Proceedings of Ma-
chine Learning Research 37 (2015). Ed. by Francis Bach and David Blei, pp. 1889–
1897.
[24] Alex Serban, Erik Poll, and Joost Visser. „A Standard Driven Software Architec-
ture for Fully Autonomous Vehicles“. In: Journal of Automotive Software Engi-
neering 1 (Jan. 2020).
[25] David Silver et al. „Mastering Chess and Shogi by Self-Play with a General Re-
inforcement Learning Algorithm“. In: (Dec. 2017).
[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction .
MIT press, 2018.
[27] Tesla. „Full Self-Driving Beta V12 Overview“. In: Tesla AI Day Presentation .
2023.
[28] Zia Wadud, Don MacKenzie, and Paul Leiby. „Help or Hindrance? The Travel,
Energy and Carbon Impacts of Highly Automated Vehicles“. In: Transportation
Research Part A: Policy and Practice 86 (2016), pp. 1–18.
46

Acknowledgement
The author gratefully acknowledges Mr. Dianzhao Li for his invaluable assistance and
expert guidance throughout the completion of this thesis. His practical advice at every
critical stage of my thesis was greatly appreciated.
47

Statement of Authorship
Ich versichere, dass ich die Arbeit selbständig verfasst und keine anderen als
die angegebenen Quellen und Hilfsmittel benutzt habe. Ich reiche sie erstmals
als Prüfungsleistung ein. Mir ist bekannt, dass ein Betrugsversuch mit der
Note "nicht ausreichend" (5,0) geahndet wird und im Wiederholungsfall zum
Ausschluss von der Erbringung weiterer Prüfungsleistungen führen kann.
Dresden, 07. January 2025
Bharath Ashok Kumar

