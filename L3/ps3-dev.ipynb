{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656f8afe-d315-4948-9c9e-6b7da5773761",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9175826fd75c114e00bf025ad5dfc3da",
     "grade": false,
     "grade_id": "cell-4c19c1d2c77c4758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lista de Exercícios 3: Aproximação de Funções\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c2dd1-29c7-4dd2-9b4b-3780cc7a1136",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cc7d6658f61e5d6da8dbcc48df24048",
     "grade": false,
     "grade_id": "cell-f9d9bd24b843d103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8cafad-b798-43c4-acf0-319b049f9518",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "577070e8ade874cd808f2d0374ff6b5c",
     "grade": false,
     "grade_id": "cell-f164f0410942aeb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÕES QUE NÃO SEGUIREM AS INSTRUÇÕES A SEGUIR NÃO SERÃO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido antes de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps3.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 30/10/2025. Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa68b5-b720-41fc-b6e2-aa8a3f899aaa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "070293a3324a7cc123e3fb82eda16153",
     "grade": false,
     "grade_id": "cell-f9a3ddde9dedcf66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984dec9-c68b-4c81-9b84-4a04342618b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "074ed009b6ecb9654239d5c90e221810",
     "grade": false,
     "grade_id": "cell-64a3af160a59fce5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mountain Car é um ambiente no qual um agente precisa conduzir um carro até o topo de uma montanha. No entanto, o motor do carro não é potente o suficiente para subir diretamente até o topo. Por isso, o agente precisa aprender a aproveitar o impulso - movendo-se para frente e para trás - para ganhar velocidade suficiente e alcançar o topo.\n",
    "\n",
    "![](https://gymnasium.farama.org/_images/mountain_car.gif)\n",
    "\n",
    "O espaço de observação é contínuo e composto por dois valores: a posição e a velocidade do carro. O agente pode escolher entre três ações discretas: acelerar o carro para a esquerda, acelerar para a direita, ou não acelerar. A posição inicial do agente é definida de forma uniformemente aleatória no intervalo $[-0.6, -0.4]$. O episódio se encerra quando o carro atinge o topo da montanha à direita (posição $0.5$) ou quando o limite de 200 passos é atingido. A cada passo, o agente recebe uma penalidade de $-1$, incentivando-o a alcançar o objetivo no menor número possível de passos. Para mais detalhes sobre o ambiente leia a [documentação do gymnasium](https://gymnasium.farama.org/environments/classic_control/mountain_car/).\n",
    "\n",
    "Devido à natureza contínua do espaço de estados, métodos tabulares não são eficazes no Mountain Car. Assim, é comum o uso de técnicas de aproximação de função para solucionar o problema de forma eficiente. Nesta lista de exercícios, você irá trabalhar com o ambiente descrito acima. Seu objetivo será implementar o algoritmo *Semi-Gradient Episodic Sarsa* com aproximação linear e explorar como diferentes técnicas de construção de features influenciam o desempenho do agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530b8ce-4fd5-4394-bd10-e609ba76f987",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2622ec1bcee08e1245fc15de2ae6d243",
     "grade": false,
     "grade_id": "cell-123718121e8a49d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Semi-Gradient Episodic Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c78de-e691-4da5-a116-0b880e4e4ba7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f109362cd1f2d0a8aee8bc61ed2ee7ed",
     "grade": false,
     "grade_id": "cell-e469841ad084257f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nesta atividade, você implementará um agente baseado no algoritmo Semi-Gradient Episodic SARSA, utilizando uma **função de valor linear**.\n",
    "\n",
    "Antes de iniciar sua implementação, analise a interface `FeatureExtractor` fornecida abaixo. Ela será a base para os construtores de features que você implementará na próxima seção. O agente utilizará um objeto com essa interface para extrair features a partir das observações dos estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031cdc8-e000-4924-afd4-3c629d9ad514",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00cf4f6ad83cd5e1d7f381d01308234b",
     "grade": false,
     "grade_id": "cell-3acae5684f619fa4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49285f-de9f-4875-bc7c-320c608dd5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d17f85efd8fa3151d16dcba1aa33344",
     "grade": false,
     "grade_id": "cell-73b0cc412a521e00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(ABC):\n",
    "    \"\"\"\n",
    "    Interface for feature extractors that convert environment states into feature vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Property that returns the size of the feature vector produced by this extractor.\n",
    "\n",
    "        Returns:\n",
    "            An integer representing the length of the feature vector.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self, state):\n",
    "        \"\"\"\n",
    "        Converts a raw state into a 1D feature vector.\n",
    "\n",
    "        Args:\n",
    "            state: The observation state from the environment.\n",
    "\n",
    "        Returns:\n",
    "            The extracted feature vector representation.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd9099-4e4b-4a64-9523-06000e00dcbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b6ba7d77909531d7cc02f88ee551c2b",
     "grade": false,
     "grade_id": "cell-9bcee510ee1c562f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Siga as instruções abaixo para implementar seu agente:\n",
    "\n",
    "1. Implemente o método `__init__` que inicializa um novo agente. Ele deve receber como parâmetros o espaço de observações, o espaço de ações, um construtor de features (`feature_extractor`), a taxa de aprendizado $\\alpha$, o fator de desconto $\\gamma$, e o parâmetro de exploração $\\varepsilon$. Inicialize os pesos do modelo na variável `self.weights`.\n",
    "2. Implemente o método `compute_q_values`, que recebe um vetor de features e calcula os *Q-values* de acordo com a entrada.\n",
    "3. Implemente o método `choose_action`, responsável por escolher uma ação a partir de um estado observado, seguindo a política $\\varepsilon$-greedy.\n",
    "4. Implemente o método `learn`, que atualiza os pesos do agente com base na experiência obtida durante a interação com o ambiente.\n",
    "5. Implemente o método `train`, que executa o loop de treinamento do algoritmo Sarsa. O ambiente de treinamento e o número de episódios devem ser fornecidos como parâmetros de entrada. O método deve retornar uma lista com a soma das recompensas obtidas ao longo de cada episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879de152-526a-4a56-b28c-a725d3ae5524",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f706253d52ac575b9e8aa0b3181f3fc",
     "grade": false,
     "grade_id": "cell-5a69b1dafa706f11",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SGESAgent:\n",
    "    def __init__(self, observation_space, action_space, feature_extractor, alpha, gamma, epsilon):\n",
    "        \"\"\"\n",
    "        Initializes the Semi-Gradient Episodic Sarsa (SGES) Agent.\n",
    "\n",
    "        Args:\n",
    "            observation_space: The environment's observation space.\n",
    "            action_space: The environment's action space.\n",
    "            feature_extractor (FeatureExtractor): An object to convert states to feature vectors.\n",
    "            [cite_start]alpha (float): The step-size parameter (learning rate). [cite: 31, 795]\n",
    "            gamma (float): The discount-rate parameter.\n",
    "            [cite_start]epsilon (float): The probability for exploration (epsilon-greedy). [cite: 31, 795]\n",
    "        \"\"\"\n",
    "        # Store essential parameters\n",
    "        self.action_space = action_space\n",
    "        self.n_actions = action_space.n\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_size = feature_extractor.feature_size\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # [cite_start]Initialize value-function weights (theta) [cite: 31, 796]\n",
    "        # We use a linear function approximator: q(s, a, w) = x(s)^T * w_a\n",
    "        # Therefore, our weights 'w' will be a matrix of shape (feature_size, n_actions),\n",
    "        # where each column represents the weights for a specific action.\n",
    "        # [cite_start]We initialize them to zero as suggested by the pseudocode[cite: 31, 796].\n",
    "        self.weights = np.zeros((self.feature_size, self.n_actions))\n",
    "\n",
    "        # Initialize a random number generator for reproducibility\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "    def compute_q_values(self, features):\n",
    "        \"\"\"\n",
    "        Computes the Q-values for all actions given a state's feature vector.\n",
    "        [cite_start]This implements the linear approximation q(s, a, w) = x(s)^T * w_a for all 'a'. [cite: 739, 91]\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): The feature vector x(s) for the current state.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 1D array of Q-values for all possible actions.\n",
    "        \"\"\"\n",
    "        # Perform a matrix multiplication: x(s)^T * W\n",
    "        # features shape: (feature_size,)\n",
    "        # self.weights shape: (feature_size, n_actions)\n",
    "        # Resulting shape: (n_actions,)\n",
    "        return features @ self.weights\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        [cite_start]Chooses an action from a given state using an epsilon-greedy policy. [cite: 38, 803]\n",
    "\n",
    "        Args:\n",
    "            state: The current environment state.\n",
    "\n",
    "        Returns:\n",
    "            int: The chosen action.\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy exploration\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            # Choose a random action\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            # Choose the greedy action\n",
    "            # 1. Extract features from the state\n",
    "            features = self.feature_extractor.extract(state)\n",
    "            # 2. Compute Q-values for all actions\n",
    "            q_values = self.compute_q_values(features)\n",
    "            # [cite_start]3. Select the action with the highest Q-value [cite: 27]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"\n",
    "        [cite_start]Updates the agent's weights using the semi-gradient Sarsa update rule. [cite: 38, 803]\n",
    "\n",
    "        Args:\n",
    "            state: The state S.\n",
    "            action: The action A.\n",
    "            reward: The reward R.\n",
    "            next_state: The next state S'.\n",
    "            next_action: The next action A'.\n",
    "            done (bool): True if S' is a terminal state, False otherwise.\n",
    "        \"\"\"\n",
    "        # 1. Get the feature vector for the current state, x(S)\n",
    "        features = self.feature_extractor.extract(state)\n",
    "\n",
    "        # 2. Get the predicted Q-value for the current state-action pair: q(S, A, w)\n",
    "        q_value = self.compute_q_values(features)[action]\n",
    "\n",
    "        # 3. Determine the update target (U_t)\n",
    "        if done:\n",
    "            # [cite_start]If S' is terminal, the target is just the reward R. [cite: 35, 36, 800, 801]\n",
    "            q_target = reward\n",
    "        else:\n",
    "            # [cite_start]If S' is not terminal, the target is R + gamma * q(S', A', w) [cite: 38, 803]\n",
    "\n",
    "            # 3a. Get the feature vector for the next state, x(S')\n",
    "            next_features = self.feature_extractor.extract(next_state)\n",
    "\n",
    "            # 3b. Get the Q-value for the *next* state-action pair: q(S', A', w)\n",
    "            next_q_value = self.compute_q_values(next_features)[next_action]\n",
    "\n",
    "            # 3c. Calculate the full Sarsa target\n",
    "            q_target = reward + self.gamma * next_q_value\n",
    "\n",
    "        # 4. Calculate the TD error (delta)\n",
    "        # delta = [Target - Prediction]\n",
    "        td_error = q_target - q_value\n",
    "\n",
    "        # 5. Get the gradient of the value function approximator, grad(q(S, A, w))\n",
    "        # For a linear function q(S, A, w) = x(S)^T * w_A,\n",
    "        # [cite_start]the gradient with respect to w_A is just the feature vector x(S). [cite: 91, 739]\n",
    "        gradient = features\n",
    "\n",
    "        # 6. Perform the semi-gradient update\n",
    "        # [cite_start]w_A <- w_A + alpha * delta * grad(q(S, A, w)) [cite: 38, 803]\n",
    "        # We only update the weights for the action that was actually taken.\n",
    "        self.weights[:, action] += self.alpha * td_error * gradient\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        \"\"\"\n",
    "        Runs the main training loop for the SGESAgent.\n",
    "\n",
    "        Args:\n",
    "            env: The Gymnasium environment to train in.\n",
    "            episodes (int): The total number of episodes to run.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing the total return (sum of rewards) for each episode.\n",
    "        \"\"\"\n",
    "        # List to store the total reward for each episode\n",
    "        episode_returns = []\n",
    "\n",
    "        # [cite_start]Loop for each episode [cite: 32, 797]\n",
    "        for episode in range(episodes):\n",
    "            total_reward = 0\n",
    "\n",
    "            # [cite_start]Initialize S (state) [cite: 33, 798]\n",
    "            state, info = env.reset()\n",
    "\n",
    "            # [cite_start]Choose A (action) from S using the policy [cite: 33, 798]\n",
    "            action = self.choose_action(state)\n",
    "\n",
    "            done = False\n",
    "\n",
    "            # [cite_start]Loop for each step of the episode [cite: 33, 798]\n",
    "            while not done:\n",
    "                # [cite_start]Take action A, observe R, S' [cite: 34, 799]\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "\n",
    "                # [cite_start]Choose A' (next action) from S' using the policy [cite: 38, 803]\n",
    "                next_action = self.choose_action(next_state)\n",
    "\n",
    "                # Perform the Sarsa update (S, A, R, S', A')\n",
    "                # [cite_start]The 'done' flag is passed to handle the terminal update case [cite: 35, 36, 800, 801]\n",
    "                self.learn(state, action, reward, next_state, next_action, done)\n",
    "\n",
    "                # S <- S'; [cite_start]A <- A' [cite: 39, 40, 804]\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "            # Store the total return for this episode\n",
    "            episode_returns.append(total_reward)\n",
    "\n",
    "        return episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d770e7c-d0f0-40ac-8542-be19bb633788",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dce024544d538817499b0a09ca891f4c",
     "grade": true,
     "grade_id": "cell-1581dad1471d2254",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a880af-b6d9-49c7-b2ca-e608aa59227d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f13b804c440019ba980f44df55f8f08a",
     "grade": true,
     "grade_id": "cell-d9ad11a09933e17c",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e0efc-d69d-453b-aade-c1ae507104e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31d43906c48c7039240e1fea9af4f6c1",
     "grade": true,
     "grade_id": "cell-6bbe66dac39d7e17",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe66ff-2d3e-471e-bc81-7970a5b9bb0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f2ec3f3833b047e8971d71adf1c2e57",
     "grade": true,
     "grade_id": "cell-fac233e6d8645e9e",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0495d16-1298-4cd3-9193-dccc149c4678",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe717c67b91fd9975f181dcd48a43153",
     "grade": false,
     "grade_id": "cell-e42599779f53b3e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Construção de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61beab6-4a29-4505-b8a5-cdaec5484cf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2646c9d4329584c597ff4d3371f399d8",
     "grade": false,
     "grade_id": "cell-d264a9f794c5e744",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Com o algoritmo do agente pronto, você deverá implementar 3 modelos de construtores de features diferentes:\n",
    "\n",
    "1. **Identidade**\n",
    "2. **Tile Coding**\n",
    "3. **Funções de Base Radial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb71073-f8ef-462b-a25f-5ec7b1a2f419",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2a6122e5b5a7d4ab935c66cf4b5d5f9",
     "grade": false,
     "grade_id": "cell-08d1055c0c0a1318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Identidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278df1d7-d78d-46d1-aaab-a35613e94e6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d3865f8f2d1e6fb59386471a9b740e5",
     "grade": false,
     "grade_id": "cell-54e7e236f108e69a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Implemente o `IdentityFeatureExtractor`, cujo método `extract` apenas retorna a observação de entrada, sem realizar nenhuma operação nela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a85dc-5a11-42f8-bdab-5c261cd71d18",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a61b75778c03707f4724dca465b40c7",
     "grade": false,
     "grade_id": "cell-8c81c63bcd766788",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, observation_space):\n",
    "        \"\"\"\n",
    "        Initializes the IdentityFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            observation_space: The environment's observation space.\n",
    "        \"\"\"\n",
    "        # Store the observation space to determine the feature size.\n",
    "        self._observation_space = observation_space\n",
    "\n",
    "        # The feature size is the dimensionality of the observation space.\n",
    "        self._feature_size = observation_space.shape[0]\n",
    "\n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Property that returns the size of the feature vector.\n",
    "        For identity, this is the same as the observation space dimension.\n",
    "\n",
    "        Returns:\n",
    "            An integer representing the length of the feature vector.\n",
    "        \"\"\"\n",
    "        return self._feature_size\n",
    "\n",
    "    def extract(self, state):\n",
    "        \"\"\"\n",
    "        Converts a raw state into a 1D feature vector.\n",
    "        As per the instructions, this extractor performs no transformation.\n",
    "\n",
    "        Args:\n",
    "            state: The observation state from the environment.\n",
    "\n",
    "        Returns:\n",
    "            The untransformed state as the feature vector.\n",
    "        \"\"\"\n",
    "        # Returns the state as-is, performing no operations.\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a520b-9eb6-4d8e-b2e2-cfec25b396bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6296588cdfbd3989801ea511d8a7d58e",
     "grade": true,
     "grade_id": "cell-7b19ca6e8a004b3e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de609b7-083d-4af7-a727-d57e438b3722",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "725009ceae186f4b5b769c8e129e7e7f",
     "grade": false,
     "grade_id": "cell-fdf844a8a866e21c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. Treine um novo agente no ambiente Mountain Car por 200 episódios utilizando o `IdentityFeatureExtractor`. O agente deve ser treinado com os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.01$, fator de desconto $\\gamma = 0.99$ e parâmetro de exploração $\\varepsilon = 0.1$. Armazene os retornos episódicos, obtidos no método `train`, na variável `identity_agent`.\n",
    "\n",
    "**Nota:** Não se preocupe se o desempenho do agente com o `IdentityFeatureExtractor` for insatisfatório. Ele será utilizado apenas como baseline, permitindo observar o comportamento do agente quando nenhuma transformação é aplicada às observações do ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff2b1f-bbbc-4ce3-9f35-14d2ebbc9dd3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bac7fe0ba1b8b2af3f80a38646d2543",
     "grade": false,
     "grade_id": "cell-085f6d93aa847917",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Define the hyperparameters specified in Task 7\n",
    "ALPHA = 0.01\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1\n",
    "EPISODES = 200\n",
    "\n",
    "# 1. Instantiate the IdentityFeatureExtractor\n",
    "# This extractor will just pass the (position, velocity) state as features.\n",
    "identity_feat_extractor = IdentityFeatureExtractor(env.observation_space)\n",
    "\n",
    "# 2. Instantiate the SGESAgent\n",
    "# We pass the environment, the feature extractor, and the hyperparameters.\n",
    "agent = SGESAgent(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    feature_extractor=identity_feat_extractor,\n",
    "    alpha=ALPHA,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON\n",
    ")\n",
    "\n",
    "# 3. Train the agent for 200 episodes\n",
    "# [cite_start]The `train` method executes the episodic loop from the pseudocode [cite: 78, 797]\n",
    "# and returns the list of total rewards for each episode.\n",
    "identity_agent = agent.train(env, EPISODES)\n",
    "\n",
    "env.close()\n",
    "\n",
    "mean_return = np.mean(identity_agent)\n",
    "print(f\"Mean Return: {mean_return:.2f}\")\n",
    "\n",
    "last_mean = np.mean(identity_agent[-20:])\n",
    "print(f\"Mean Return of the Last 20 Episodes: {last_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ede4a-ddcb-408b-b405-2d19378947cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aea3f40a80395ea1a24d2f4c8714a3c",
     "grade": true,
     "grade_id": "cell-d1981b644a5b113b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f23c80-882a-4102-9602-362d9d9bfae4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76621f30b1d49456eb3d201455c196b6",
     "grade": false,
     "grade_id": "cell-c0bae56555c46531",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tile Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a676c2-7c62-459f-9312-fbbfdae074eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e48c2735b2cca94d407e83d349a9b21",
     "grade": false,
     "grade_id": "cell-250d747b0305c2b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. Implemente o `TileCodingFeatureExtractor`, cujo método `extract` gera um vetor de features utilizando a técnica de *tile coding*. A quantidade de tilings a ser gerada é definida pelo parâmetro `num_tilings`, enquanto a quantidade de divisões por dimensão em cada tiling é definida pelo parâmetro `tiles_per_dim`. Ambos valores são fornecidos ao construtor da classe. Por exemplo: um `TileCodingFeatureExtractor` com `num_tilings = 3` e `tiles_per_dim = 2` deve gerar 3 tilings, cada um dividindo o espaço em uma grade de tamanho 2$\\times$2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa3e95-c413-4145-8c95-706c8ffd62d9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34a8b68d2ef9f660410a88f95fae02bd",
     "grade": false,
     "grade_id": "cell-c5337854b951c2f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TileCodingFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, observation_space, num_tilings, tiles_per_dim):\n",
    "        \"\"\"\n",
    "        Initializes the TileCodingFeatureExtractor.\n",
    "\n",
    "        This implementation creates `num_tilings` grids (tilings),\n",
    "        each of size `tiles_per_dim` x `tiles_per_dim`.\n",
    "        The grids are offset from each other to provide a finer-grained\n",
    "        and generalized feature representation.\n",
    "\n",
    "        Args:\n",
    "            observation_space: The environment's observation space.\n",
    "            num_tilings (int): The number of tilings to create.\n",
    "            tiles_per_dim (int): The number of tiles for each dimension\n",
    "                                (e.g., 8 for an 8x8 grid).\n",
    "        \"\"\"\n",
    "        # Store state space boundaries\n",
    "        self.low = observation_space.low\n",
    "        self.high = observation_space.high\n",
    "\n",
    "        # Store tiling parameters\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tiles_per_dim = tiles_per_dim\n",
    "        self.num_dims = len(self.low)\n",
    "\n",
    "        # Calculate the width of a single tile in each dimension\n",
    "        # This matches the method from the provided Lisp code,\n",
    "        # [cite_start]e.g., pos-width = (range / tiles_per_dim) [cite: 40, 41]\n",
    "        self.tile_widths = (self.high - self.low) / self.tiles_per_dim\n",
    "\n",
    "        # Calculate the total size of one tiling's grid (e.g., 8x8 = 64)\n",
    "        self.tiling_size = self.tiles_per_dim ** self.num_dims\n",
    "\n",
    "        # Calculate the total number of features\n",
    "        # This is (number of tilings) * (features per tiling)\n",
    "        self._feature_size = self.num_tilings * self.tiling_size\n",
    "\n",
    "        # Create offsets for each tiling\n",
    "        # We will offset each tiling by a fraction of a tile width\n",
    "        # This creates the staggered, overlapping grids.\n",
    "        self.tiling_offsets = np.zeros((self.num_tilings, self.num_dims))\n",
    "        for i in range(self.num_tilings):\n",
    "            # Asymmetrical offset: offset dimension 'j' by 'i * (2j + 1)'\n",
    "            # This is a common and effective method.\n",
    "            # We divide by num_tilings to scale the offset.\n",
    "            for j in range(self.num_dims):\n",
    "                self.tiling_offsets[i, j] = (i * (2 * j + 1) / self.num_tilings) * self.tile_widths[j]\n",
    "\n",
    "        # Pre-calculate multipliers for flattening the N-D tile coordinates\n",
    "        # For 2D: [tiles_per_dim^0, tiles_per_dim^1] = [1, 8] (if 8x8)\n",
    "        # Index = (coord[0] * 1) + (coord[1] * 8)\n",
    "        self.index_multipliers = self.tiles_per_dim ** np.arange(self.num_dims)\n",
    "\n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Property that returns the total size of the feature vector.\n",
    "\n",
    "        Returns:\n",
    "            An integer representing the total number of features.\n",
    "        \"\"\"\n",
    "        return self._feature_size\n",
    "\n",
    "    def extract(self, state):\n",
    "        \"\"\"\n",
    "        Converts a raw state into a sparse 1D binary feature vector.\n",
    "        The vector will have exactly `num_tilings` active features (1.0),\n",
    "        one for each tiling.\n",
    "\n",
    "        Args:\n",
    "            state: The observation state (position, velocity).\n",
    "\n",
    "        Returns:\n",
    "            The extracted binary feature vector.\n",
    "        \"\"\"\n",
    "        # Create a sparse vector (all zeros)\n",
    "        features = np.zeros(self._feature_size)\n",
    "\n",
    "        # Loop over each of the 'num_tilings'\n",
    "        for i in range(self.num_tilings):\n",
    "            # Apply the offset for the current tiling\n",
    "            offset_state = state + self.tiling_offsets[i]\n",
    "\n",
    "            # Find the discrete (x, y) coordinates in this tiling's grid\n",
    "            coordinates = np.floor((offset_state - self.low) / self.tile_widths).astype(int)\n",
    "\n",
    "            # Clip coordinates to be within the grid boundaries [0, tiles_per_dim - 1]\n",
    "            coordinates = np.clip(coordinates, 0, self.tiles_per_dim - 1)\n",
    "\n",
    "            # Convert the N-D coordinates to a single 1D index for this tiling\n",
    "            # E.g., for 8x8: (pos_idx * 1) + (vel_idx * 8)\n",
    "            local_index = np.dot(coordinates, self.index_multipliers)\n",
    "\n",
    "            # Calculate the global index in the final feature vector\n",
    "            # This places the local index in the correct 'block' for this tiling\n",
    "            global_index = (i * self.tiling_size) + local_index\n",
    "\n",
    "            # Activate the corresponding feature\n",
    "            features[global_index] = 1.0\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5718c6-1511-49e8-aa54-4e318a568752",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ed44b74cd58a3a950ece2ab94c78311",
     "grade": true,
     "grade_id": "cell-570f8900bc692806",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293b3c1-1f97-4736-b1c8-aa6db757df7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d5c4de8444919582b56764ec0d7219a",
     "grade": false,
     "grade_id": "cell-81ac171b1b0f806b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Treine um novo agente no ambiente Mountain Car por 200 episódios utilizando o `TileCodingFeatureExtractor`. O agente deve ser treinado com os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.01$, fator de desconto $\\gamma = 0.99$ e parâmetro de exploração $\\varepsilon = 0.1$. Para os parâmetros `num_tilings` e `tiles_per_dim`, utilize os valores que proporcionarem os melhores resultados. Armazene os retornos episódicos, obtidos no método `train`, na variável `tile_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd8ee1-eef8-4621-8664-c12a6cd8af12",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82a3486011a111511e8d473a390e8ef7",
     "grade": false,
     "grade_id": "cell-1c17a6bf39513cf8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# --- Hyperparameters ---\n",
    "# Agent parameters are fixed by the notebook instructions (Task 9)\n",
    "ALPHA = 0.01\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1\n",
    "EPISODES = 200\n",
    "\n",
    "# Feature Extractor parameters are flexible.\n",
    "# We will use the values from the reference materials which are\n",
    "# shown to work well for this specific problem.\n",
    "#\n",
    "# The textbook (Chapter 10, Example 10.1) states:\n",
    "# \"We used 8 tilings, with each tile covering 1/8th of the bounded\n",
    "# [cite_start]distance in each dimension\"[cite: 679].\n",
    "# [cite_start]The Lisp code used to generate the figures also uses 8 tilings[cite: 1017].\n",
    "NUM_TILINGS = 8\n",
    "TILES_PER_DIM = 8\n",
    "\n",
    "# 1. Instantiate the TileCodingFeatureExtractor\n",
    "tile_feat_extractor = TileCodingFeatureExtractor(\n",
    "    observation_space=env.observation_space,\n",
    "    num_tilings=NUM_TILINGS,\n",
    "    tiles_per_dim=TILES_PER_DIM\n",
    ")\n",
    "\n",
    "# 2. Instantiate the SGESAgent\n",
    "agent = SGESAgent(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    feature_extractor=tile_feat_extractor,\n",
    "    alpha=ALPHA,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON\n",
    ")\n",
    "\n",
    "# 3. Train the agent for 200 episodes\n",
    "# The `train` method will execute the Sarsa loop and return the\n",
    "# list of total rewards per episode.\n",
    "tile_agent = agent.train(env, EPISODES)\n",
    "\n",
    "env.close()\n",
    "\n",
    "mean_return = np.mean(tile_agent)\n",
    "print(f\"Mean Return: {mean_return:.2f}\")\n",
    "\n",
    "last_mean = np.mean(tile_agent[-20:])\n",
    "print(f\"Mean Return of the Last 20 Episodes: {last_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27900f37-5fb9-48b6-9f76-f9eee036ea07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e51a7cd6b30504fb0d52be1ef2de347f",
     "grade": true,
     "grade_id": "cell-4645209cd7a25a0e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8c52a-592e-4733-95c8-2ac0ccc37b9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d96586af9aee5787fc0dfc03d080b0e",
     "grade": false,
     "grade_id": "cell-698007219f41675d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Funções de Base Radial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d240d9-9de7-4a17-8555-5ba26b136ed1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e024714bdfe6542af3ce239c5a3642e",
     "grade": false,
     "grade_id": "cell-c24fd7907217afeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Implemente o `RBFFeatureExtractor`, cujo método `extract` gera um conjunto de features baseadas em *Radial Basis Functions*. A quantidade total de componentes a ser gerada é definida pelo parâmetro `n_components`, fornecido ao construtor da classe. Utilize a biblioteca *scikit-learn* (sklearn) para auxiliar sua implementação.\n",
    "\n",
    "**Importante:** Você pode combinar RBFs com diferentes parâmetros para capturar melhor dinâmicas complexas do ambiente e potencialmente melhorar o desempenho do agente. Experimente diferentes configurações para identificar as combinações que produzem os melhores resultados de aprendizado. Normalizar o vetor de entrada também pode facilitar o aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3308f7-d12d-457c-91fc-ba0608f14027",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80c20704e1481d4aa0f94726e1f8e20",
     "grade": false,
     "grade_id": "cell-e0d3eb3f4a41bb02",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We need these sklearn components as suggested by the notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "class RBFFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, observation_space, n_components):\n",
    "        \"\"\"\n",
    "        Initializes the RBFFeatureExtractor.\n",
    "\n",
    "        This implementation uses sklearn's RBFSampler, also known as\n",
    "        \"Random Kitchen Sinks\", to approximate an RBF kernel feature map.\n",
    "\n",
    "        Following the notebook's hint, we:\n",
    "        1. Normalize the input state using StandardScaler.\n",
    "        2. Combine multiple RBF samplers with different 'gamma' values\n",
    "           to capture features at different scales.\n",
    "\n",
    "        Args:\n",
    "            observation_space: The environment's observation space.\n",
    "            n_components (int): The target number of features.\n",
    "        \"\"\"\n",
    "        # --- 1. Fit a StandardScaler to normalize inputs ---\n",
    "        # We sample from the observation space to get a representative\n",
    "        # distribution of states for fitting the scaler.\n",
    "        samples = np.array(\n",
    "            [observation_space.sample() for _ in range(10000)],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(samples)\n",
    "\n",
    "        # --- 2. Create multiple RBF Samplers ---\n",
    "        # We use a few different gamma values to capture features at\n",
    "        # different scales, as hinted in the notebook.\n",
    "        self.gammas = [0.1, 0.5, 1.0, 5.0]\n",
    "        n_gammas = len(self.gammas)\n",
    "\n",
    "        # We divide the total components among the samplers.\n",
    "        # Using ceil ensures we get at least n_components.\n",
    "        self.n_components_per_sampler = int(\n",
    "            np.ceil(n_components / n_gammas)\n",
    "        )\n",
    "\n",
    "        # The true feature size is the sum of components from all samplers\n",
    "        self._feature_size = self.n_components_per_sampler * n_gammas\n",
    "\n",
    "        self.samplers = []\n",
    "        for gamma in self.gammas:\n",
    "            # We use a fixed random_state for reproducibility\n",
    "            sampler = RBFSampler(\n",
    "                gamma=gamma,\n",
    "                n_components=self.n_components_per_sampler,\n",
    "                random_state=42\n",
    "            )\n",
    "            # Fit the sampler to the scaled data\n",
    "            sampler.fit(self.scaler.transform(samples))\n",
    "            self.samplers.append(sampler)\n",
    "\n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Property that returns the total size of the feature vector.\n",
    "\n",
    "        Returns:\n",
    "            An integer representing the total number of features.\n",
    "        \"\"\"\n",
    "        return self._feature_size\n",
    "\n",
    "    def extract(self, state):\n",
    "        \"\"\"\n",
    "        Converts a raw state into a 1D feature vector using RBFs.\n",
    "\n",
    "        Args:\n",
    "            state: The observation state (position, velocity).\n",
    "\n",
    "        Returns:\n",
    "            The extracted feature vector.\n",
    "        \"\"\"\n",
    "        # Reshape state to 2D array [1, n_dims] for the scaler\n",
    "        state_2d = state.reshape(1, -1)\n",
    "\n",
    "        # Normalize the state\n",
    "        scaled_state = self.scaler.transform(state_2d)\n",
    "\n",
    "        # Get features from each RBF sampler\n",
    "        all_features = [\n",
    "            sampler.transform(scaled_state)[0] for sampler in self.samplers\n",
    "        ]\n",
    "\n",
    "        # Concatenate the feature vectors from all samplers into one\n",
    "        return np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a9d3f-5f7b-4afd-a95f-6f034683a870",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d4a84cf5ae7c7871fd0bb3d8dd1f96",
     "grade": true,
     "grade_id": "cell-ab38da8e2ed8b89b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b756f-5bd9-4fd8-8f17-b85397e776c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e107c124cd1328ce19740ba33d6551",
     "grade": false,
     "grade_id": "cell-1a3aadd23d015a3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Treine um novo agente no ambiente Mountain Car por 200 episódios utilizando o `RBFFeatureExtractor` com 100 componentes. O agente deve ser treinado com os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.01$, fator de desconto $\\gamma = 0.99$ e parâmetro de exploração $\\varepsilon = 0.1$. Armazene os retornos episódicos, obtidos no método `train`, na variável `rbf_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566aed60-719d-49fa-bfb0-1d74c6148935",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b08f9ee642479890aabca12c0df6a86",
     "grade": false,
     "grade_id": "cell-bdafa3d25a88a2f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# --- Hyperparameters ---\n",
    "# Define the hyperparameters specified in Task 11\n",
    "ALPHA = 0.01\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1\n",
    "EPISODES = 200\n",
    "N_COMPONENTS = 100\n",
    "\n",
    "# 1. Instantiate the RBFFeatureExtractor\n",
    "# This uses the class implemented in cell 34, configured with\n",
    "# 100 components as required by the task.\n",
    "rbf_feat_extractor = RBFFeatureExtractor(\n",
    "    observation_space=env.observation_space,\n",
    "    n_components=N_COMPONENTS\n",
    ")\n",
    "\n",
    "# 2. Instantiate the SGESAgent\n",
    "# This uses the agent class from cell 10, passing in the\n",
    "# RBF extractor and the specified hyperparameters.\n",
    "agent = SGESAgent(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    feature_extractor=rbf_feat_extractor,\n",
    "    alpha=ALPHA,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON\n",
    ")\n",
    "\n",
    "# 3. Train the agent for 200 episodes\n",
    "# The train() method executes the Sarsa loop\n",
    "# and returns the list of total rewards for each episode.\n",
    "rbf_agent = agent.train(env, EPISODES)\n",
    "\n",
    "env.close()\n",
    "\n",
    "mean_return = np.mean(rbf_agent)\n",
    "print(f\"Mean Return: {mean_return:.2f}\")\n",
    "\n",
    "last_mean = np.mean(rbf_agent[-20:])\n",
    "print(f\"Mean Return of the Last 20 Episodes: {last_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42821725-3cf0-47f4-8fe4-aaea1cf59716",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e0e4a468e0cbd925631b2bc04ead2a5",
     "grade": true,
     "grade_id": "cell-aef7056df4c92e10",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'rbf_agent' in vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da40c2-2d34-48ea-bc7c-3462b1378677",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4ef875068b35e0ada7fc1cda52576c3",
     "grade": false,
     "grade_id": "cell-bcb7edcab7279bb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a471cf-0a42-4957-9da1-52561bb58199",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dfe64f9a9b817a3b821ccf830039af5",
     "grade": false,
     "grade_id": "cell-68c98e2523e84e43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Utilize a biblioteca matplotlib para construir um gráfico comparativo dos retornos episódicos obtidos pelos agentes utilizando cada um dos construtores de features implementados. Utilize as variáveis `identiy_agent`, `tile_agent` e `rbf_agent` obtidas nos exercícios anteriores. No eixo X, represente os episódios; no eixo Y, o retorno acumulado por episódio. Caso seja necessário, aplique uma média movel para suavizar as curvas e deixar as tendências mais evidentes. Inclua título, legendas e rótulos de eixos apropriados para facilitar a interpretação. Se utilizar algum tipo de suavização, indique claramente no gráfico qual o método aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90135676-0a9d-4b74-a424-0b627606f2de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66e658f6be2caabbc9ec8997ef82445f",
     "grade": true,
     "grade_id": "cell-6335198c450f6612",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# This cell uses 'matplotlib.pyplot' (as plt) and 'numpy' (as np),\n",
    "# which were imported in cell 8 of the notebook.\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"\n",
    "    Calculates the moving average of a 1D array using convolution.\n",
    "    Args:\n",
    "        data (np.ndarray or list): The 1D input data.\n",
    "        window_size (int): The width of the moving average window.\n",
    "    Returns:\n",
    "        np.ndarray: The smoothed data.\n",
    "    \"\"\"\n",
    "    # Use 'valid' mode to ensure the average is only calculated\n",
    "    # for full windows.\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Define the window size for smoothing\n",
    "WINDOW_SIZE = 20\n",
    "\n",
    "# Apply the moving average to each agent's episodic returns\n",
    "smooth_identity = moving_average(identity_agent, WINDOW_SIZE)\n",
    "smooth_tile = moving_average(tile_agent, WINDOW_SIZE)\n",
    "smooth_rbf = moving_average(rbf_agent, WINDOW_SIZE)\n",
    "\n",
    "# Create the x-axis (episodes).\n",
    "# This axis is shifted to align with the 'valid' convolution,\n",
    "# representing the *end* of each window.\n",
    "x_axis = np.arange(WINDOW_SIZE - 1, len(identity_agent))\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the smoothed learning curves for each agent\n",
    "plt.plot(x_axis, smooth_identity, label=f'Identidade (M.M. {WINDOW_SIZE} episódios)')\n",
    "plt.plot(x_axis, smooth_tile, label=f'Tile Coding (M.M. {WINDOW_SIZE} episódios)')\n",
    "plt.plot(x_axis, smooth_rbf, label=f'RBF (M.M. {WINDOW_SIZE} episódios)')\n",
    "\n",
    "# Add plot labels and a title as requested\n",
    "plt.title('Desempenho dos Extratores de Features no Mountain Car')\n",
    "plt.xlabel('Episódio')\n",
    "plt.ylabel('Retorno Acumulado (Média Móvel)')\n",
    "\n",
    "# Set the y-axis limit to focus on the relevant reward range.\n",
    "# The worst possible return is -200 (truncation).\n",
    "plt.ylim([-205, -90])\n",
    "\n",
    "# Add a legend to identify each line\n",
    "plt.legend()\n",
    "\n",
    "# Add a grid for easier reading of the values\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the final plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9e281-2f0d-4204-9b1b-e5684feb0a5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06086dc71d78359622963096092f3663",
     "grade": false,
     "grade_id": "cell-2e00546e8b4c6fbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Explique por que a modelagem do construtor de features pode ser crucial para o desempenho de um agente que utiliza aproximação de função."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce766f7-d059-4c83-ae1d-e73585ee9df5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1c67a250750ccadbb7f49bf90d214f0",
     "grade": true,
     "grade_id": "cell-a5307d7b384c7373",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "A modelagem do construtor de features é crucial porque, em problemas com aproximação de função, o agente não aprende sobre o estado \"bruto\" (como `(posição, velocidade)`), mas sim sobre o **vetor de features** que representa esse estado[cite: 24, 570].\n",
    "\n",
    "Com a aproximação linear que usamos, o Q-value é calculado como um produto escalar: $\\hat{q}(s,a,\\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s,a)$[cite: 24, 120, 570]. Isso significa duas coisas:\n",
    "\n",
    "1.  **Teto de Desempenho:** A qualidade máxima do agente é limitada pela qualidade das features. Se o vetor $\\mathbf{x}(s,a)$ não contiver as informações necessárias para distinguir um estado bom de um estado ruim, o agente **jamais** conseguirá aprender a política ótima, independentemente de quanto ele treine. O construtor `IdentityFeatureExtractor` falha por isso: a função de valor real do Mountain Car é complexa e não-linear (como um \"vale\" [cite: 553]), e os estados brutos `(posição, velocidade)` não são linearmente separáveis para representá-la.\n",
    "\n",
    "2.  **Base para Generalização:** Em espaços contínuos, o agente nunca visitará o mesmo estado exato duas vezes. O único jeito de ele aprender é **generalizando** o conhecimento de estados visitados para estados novos, mas similares. As features são o mecanismo para isso[cite: 565].\n",
    "    * Um bom construtor (como Tile Coding ou RBF) garante que estados próximos no espaço original (ex: `(-0.5, 0.0)` e `(-0.51, 0.0)`) produzam vetores de features parecidos (ex: que compartilhem \"tiles\" ativos)[cite: 112].\n",
    "    * Isso faz com que a atualização de valor para um estado \"vaze\" para seus vizinhos. O agente aprende uma função de valor \"suave\" e generalizada, em vez de tentar memorizar pontos discretos, o que seria impossível.\n",
    "\n",
    "Em resumo, o construtor de features é o componente que \"traduz\" o problema do mundo real para uma forma que o agente linear consegue entender e resolver. Se a tradução for ruim, o agente falha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87757007-4320-4f83-9c36-50362b426f12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24e15393eb11f5e1d0d8bae7ef11bb17",
     "grade": false,
     "grade_id": "cell-f4ddf7228d082311",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Quais critérios devem guiar a escolha dos modelos e da quantidade de features a serem utilizadas na construção do espaço de features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd436b1-067a-49c6-99e9-3e358e61239e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2afa89a168378d25b29fb861dabb9ae",
     "grade": true,
     "grade_id": "cell-5c2a50c39d4b19c5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "A escolha do modelo e da quantidade de features deve ser guiada por um balanço entre **poder de discriminação** e **capacidade de generalização**.\n",
    "\n",
    "1.  **Escolha do Modelo (Ex: Tile Coding vs. RBF):**\n",
    "    * **Natureza do Problema:** O Tile Coding é excelente para partições rígidas do espaço e generalização local (estados vizinhos ativam features parecidos). RBFs oferecem uma generalização mais suave e global. A escolha depende se o valor da função muda abruptamente ou suavemente no espaço de estados.\n",
    "    * **Dimensionalidade:** O Tile Coding sofre com a \"maldição da dimensionalidade\" (features crescem exponencialmente com as dimensões do estado). Métodos como RBFs (especialmente suas aproximações) podem escalar melhor para problemas com mais dimensões.\n",
    "\n",
    "2.  **Quantidade de Features (Ex: `num_tilings` ou `n_components`):**\n",
    "    * **Poucas Features (Generalização Excessiva):** Se houver poucas features (ex: poucos *tiles*), o agente não consegue **discriminar** estados diferentes que exigem ações diferentes. Isso é *underfitting*. O agente pode, por exemplo, agrupar o fundo do vale e a subida da montanha no mesmo \"bloco\", tornando impossível aprender a política correta.\n",
    "    * **Muitas Features (Memorização):** Se houver features demais (ex: milhares de *tiles*), o agente perde a capacidade de **generalizar**. A experiência de um estado não é \"transferida\" para seus vizinhos. Isso é *overfitting* (ou memorização). O aprendizado se torna muito lento, pois o agente precisa visitar uma quantidade massiva de estados para aprender os valores de todos os pesos.\n",
    "    * **O Ponto Ideal:** O objetivo é ter features suficientes para representar as nuances da função de valor (discriminação), mas poucas o suficiente para que o aprendizado de um estado generalize para estados próximos (generalização). Esse balanço é quase sempre encontrado empiricamente, através de ajuste de hiperparâmetros, como visto nos gráficos do livro-texto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
