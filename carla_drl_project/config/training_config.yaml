# Training Configuration for DRL Algorithms
training:
  # Algorithm selection: "ddpg", "td3", "sac"
  algorithm: "td3"
  
  # Training parameters
  total_timesteps: 100000
  eval_freq: 2000
  eval_episodes: 10
  save_freq: 5000
  
  # Environment parameters
  environment:
    max_episode_steps: 1000
    reward_type: "sparse"  # "sparse" or "dense"
    action_repeat: 1
    frame_stack: 4
    
  # Observation space
  observation:
    image_size: [84, 84]  # Resized for CNN processing
    normalize: true
    channels: 3  # RGB
    
  # Action space (continuous control)
  action:
    steering_range: [-1.0, 1.0]
    throttle_range: [0.0, 1.0]
    brake_range: [0.0, 1.0]
    
  # Reward function parameters
  reward:
    speed_reward_weight: 1.0
    collision_penalty: -100.0
    lane_keeping_reward: 0.1
    distance_reward_weight: 0.5
    
# Algorithm-specific hyperparameters
algorithms:
  ddpg:
    learning_rate: 1e-4
    buffer_size: 100000
    learning_starts: 1000
    batch_size: 64
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    noise_type: "normal"
    noise_std: 0.1
    
  td3:
    learning_rate: 1e-3
    buffer_size: 100000
    learning_starts: 1000
    batch_size: 256
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    policy_delay: 2
    target_policy_noise: 0.2
    target_noise_clip: 0.5
    
  sac:
    learning_rate: 3e-4
    buffer_size: 100000
    learning_starts: 1000
    batch_size: 256
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    ent_coef: "auto"
    target_entropy: "auto"
    
# Network architectures
networks:
  # CNN feature extractor for image observations
  cnn:
    features_dim: 256
    filters: [32, 64, 64]
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
    activation: "relu"
    
  # MLP layers for policy and value functions
  policy:
    hidden_dims: [256, 256]
    activation: "relu"
    
  value:
    hidden_dims: [256, 256]
    activation: "relu"
