# CARLA DRL Training Configuration
# PPO Algorithm Configuration

algorithm:
  name: "PPO"
  version: "2.0"
  
  # Core PPO Parameters
  ppo:
    learning_rate: 3.0e-4
    n_steps: 2048          # Number of steps to run for each environment per update
    batch_size: 64         # Minibatch size
    n_epochs: 10           # Number of epoch when optimizing the surrogate loss
    gamma: 0.99            # Discount factor
    gae_lambda: 0.95       # Factor for trade-off of bias vs variance for GAE
    clip_range: 0.2        # Clipping parameter for PPO
    clip_range_vf: null    # Clipping parameter for value function (None = same as clip_range)
    normalize_advantage: true
    ent_coef: 0.0          # Entropy coefficient for the loss calculation
    vf_coef: 0.5           # Value function coefficient for the loss calculation
    max_grad_norm: 0.5     # Maximum norm for the gradient clipping
    target_kl: null        # Target KL divergence between old and new policy
    
  # Network Architecture
  networks:
    # Policy Network (Actor)
    policy:
      type: "MultimodalPolicy"
      cnn_features_dim: 512
      mlp_hidden_dims: [256, 256]
      activation: "relu"
      dropout_rate: 0.1
      share_features_extractor: true
      
    # Value Network (Critic)  
    value:
      type: "MultimodalValue"
      cnn_features_dim: 512
      mlp_hidden_dims: [256, 256]
      activation: "relu"
      dropout_rate: 0.1
      
    # Feature Extractor
    feature_extractor:
      type: "CNNExtractor"
      cnn_layers:
        - filters: 32
          kernel_size: 8
          stride: 4
          activation: "relu"
        - filters: 64
          kernel_size: 4
          stride: 2
          activation: "relu"
        - filters: 64
          kernel_size: 3
          stride: 1
          activation: "relu"
      flatten_dim: 64

# Training Configuration
training:
  # General Training Settings
  total_timesteps: 1000000
  eval_freq: 10000
  eval_episodes: 5
  save_freq: 25000
  
  # Environment Settings
  n_envs: 1              # Number of parallel environments
  env_kwargs: {}
  
  # Logging and Monitoring
  verbose: 1
  tensorboard_log: "./logs/tensorboard"
  log_interval: 1
  save_replay_buffer: false
  
  # Checkpointing
  checkpoint_dir: "./checkpoints"
  model_save_path: "./models"
  load_checkpoint: null  # Path to checkpoint to load
  
  # Early Stopping
  early_stopping:
    enabled: false
    patience: 100000  # Steps without improvement
    min_delta: 0.01   # Minimum change to qualify as improvement
    
# Observation and Action Space
spaces:
  # Observation Space
  observation:
    type: "Dict"
    spaces:
      image:
        type: "Box"
        shape: [3, 84, 84]  # RGB image resized to 84x84
        dtype: "uint8"
        low: 0
        high: 255
      vector:
        type: "Box"
        shape: [10]         # Vehicle state vector
        dtype: "float32"
        low: -10.0
        high: 10.0
        
  # Action Space
  action:
    type: "Box"
    shape: [3]             # [throttle, brake, steering]
    dtype: "float32"
    low: [-1.0, 0.0, -1.0]
    high: [1.0, 1.0, 1.0]

# Reward Function Configuration
reward:
  # Reward Components
  components:
    speed_reward:
      enabled: true
      weight: 1.0
      target_speed: 30.0  # km/h
      speed_range: [0.0, 50.0]
      
    lane_keeping:
      enabled: true
      weight: 2.0
      lane_center_threshold: 1.0  # meters
      
    collision_penalty:
      enabled: true
      weight: -100.0
      
    off_road_penalty:
      enabled: true
      weight: -50.0
      
    goal_reward:
      enabled: true
      weight: 100.0
      
    comfort:
      enabled: true
      weight: 0.5
      max_lateral_acceleration: 2.0  # m/s²
      max_longitudinal_acceleration: 3.0  # m/s²
      
  # Reward Scaling
  reward_scale: 1.0
  normalize_rewards: false

# Curriculum Learning (Optional)
curriculum:
  enabled: false
  stages:
    - name: "basic_driving"
      episodes: 1000
      weather: 1  # Clear weather
      traffic_density: 0.2
      
    - name: "traffic_interaction"
      episodes: 2000
      weather: 1
      traffic_density: 0.5
      
    - name: "adverse_weather"
      episodes: 1000
      weather: 7  # Rain
      traffic_density: 0.3

# Evaluation Configuration
evaluation:
  # Evaluation Settings
  eval_env_kwargs: {}
  deterministic: true
  render: false
  
  # Metrics to Track
  metrics:
    - "episode_reward"
    - "episode_length"
    - "success_rate"
    - "collision_rate"
    - "lane_violation_rate"
    - "average_speed"
    - "comfort_score"
    
  # Success Criteria
  success_criteria:
    min_episode_reward: 500
    max_collision_rate: 0.1
    min_success_rate: 0.8

# Hardware Configuration
hardware:
  # Device Settings
  device: "auto"  # auto, cuda, cpu
  cuda_deterministic: false
  
  # Memory Management
  pin_memory: true
  prefetch_factor: 2
  
  # Performance
  compile_model: false  # PyTorch 2.0 compilation
  mixed_precision: false

# Reproducibility
reproducibility:
  seed: 42
  deterministic_torch: false
  benchmark_torch: true
