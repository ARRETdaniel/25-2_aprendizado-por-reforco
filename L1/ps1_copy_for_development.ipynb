{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c18e0f-36ba-4010-8ba6-9ec2c9966e3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eead5b36a3ee2d8eb504615ec2e698b",
     "grade": false,
     "grade_id": "cell-4c87f565d23c35db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lista de Exercícios 1: Processos de Decisão de Markov e Programação Dinâmica\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140e15d-8ff2-404e-9f0b-de4c9444c9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46221e6cdc09eef7a6cc1ae625ff7263",
     "grade": false,
     "grade_id": "cell-e6ec4bda5dd7e12a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd360e8-7428-4cb3-af57-ef79d5c99ef8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de898f61e5161ac2497e0a4c68b4e92e",
     "grade": false,
     "grade_id": "cell-d31fd315f36785a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÕES QUE NÃO SEGUIREM AS INSTRUÇÕES A SEGUIR NÃO SERÃO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido antes de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps1.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 23/09/2025. Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea77-cdbe-474f-bc2f-95daa8d439c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31845cf7b26a7f7728c186a8c96638a3",
     "grade": false,
     "grade_id": "cell-f1f0ba316c0b79ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1557b9-966f-44c4-85be-cb75ffd2c95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1098a937587b35eb2f0dae504db9be3",
     "grade": false,
     "grade_id": "cell-69a0af10519ed240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "O ambiente Frozen Lake é uma simulação clássica utilizada para treinamento de agentes em aprendizado por reforço. Neste ambiente, o agente navega por um lago congelado representado por um grid de tamanho $n \\times m$, com o objetivo de alcançar um alvo. O lago contém dois tipos de células: (1) células com gelo sólido, que são seguras para o agente se mover, e (2) as células com buracos, nas quais o agente cai e falha a missão. Embora o Gymnasium já possua uma implementação do Frozen Lake, neste exercício iremos implementá-lo do zero.\n",
    "\n",
    "No início de cada episódio, o agente é posicionado na célula $[0, 0]$ enquanto o alvo é posicionado na célula mais distante do agente, na posição $[n-1, m-1]$ em um mapa de tamanho $n \\times m$. A cada passo, o agente recebe uma observação indicando sua posição atual no lago e tem a possibilidade de escolher entre quatro ações possíveis: mover-se para cima, para baixo, para a esquerda ou para a direita. No entanto, devido à superfície escorregadia do lago, ele nem sempre se move na direção desejada, podendo acabar se movendo em uma direção perpendicular à escolhida. O agente recebe uma recompensa de 1 se alcançar o alvo e zero em todos os outros estados. Um episódio termina quando o agente alcança o objetivo ou cai na água.\n",
    "\n",
    "Neste exercício, vamos trabalhar sempre com o mesmo mapa $4 \\times 4$, representado na figura abaixo.\n",
    "\n",
    "![Frozen Lake Map](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46123-2985-4e8e-87b5-ed33a8fccfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b35e66b571d4144b5769839b2a1134",
     "grade": false,
     "grade_id": "cell-6e7a4e34e7d477a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa será implementar o ambiente Frozen Lake utilizando o arcabouço fornecido pelo Gymnasium. Abaixo, você encontrará um código inicial que deverá ser utilizado em sua implementação. Siga essas instruções para garantir que seu código está de acordo com o esperado:\n",
    "\n",
    "1. Na função `__init__`, já definimos o mapa que será utilizado e armazenamos essa informação na variável `_description`. Nesse mapa, a letra 'S' representa a posição inicial do agente, a letra 'G' indica o alvo, as letras 'F' representam gelo sólido (que é seguro) e as letras 'H' marcam os buracos. No entanto, ainda é necessário adicionar mais algumas informações no ambiente, especificamente sobre a representação das observações e das ações. Embora existam várias maneiras de representar os espaços de observações e de ação, neste exercício, você deve usar a forma mais simples possível, que pode ser representada por um único valor discreto. Na função `__init__`, defina o espaço de observações e o espaço de ações, atribuindo-os às variáveis `self.observation_space` e `self.action_space`, respectivamente. Utilize apenas a classe `gymnasium.spaces.Discrete` nesta tarefa.\n",
    "\n",
    "2. Antes de prosseguirmos com as funcionalidades do gymnasium, vamos implementar algumas funções auxiliares para facilitar as próximas etapas. Implemente a função `_get_obs`, que retorna a observação atual do ambiente. Além disso, implemente a função `_set_state`, que recebe um valor inteiro correspondente a uma posição no lago e coloca o agente nesta localização.\n",
    "\n",
    "3. A função `reset` deve resetar o ambiente e inicializar um novo episódio, posicionando o agente na célula $[0, 0]$ e fazendo todos os ajustes internos necessários. Esta função deve retornar uma tupla contendo a observação inicial e as informações do ambiente. Neste exercício, vamos retornar um dicionario vazio `{}` para as informações. Lembre-se que a observação deve ser um único valor discreto, como definido no item 1. Implemente a função `reset`.\n",
    "\n",
    "4. A função `step()` é responsável por atualizar o ambiente com base na ação executada pelo agente. Ela recebe como entrada a ação escolhida pelo agente, um parâmetro seed e uma variável options, e calcula o novo estado atual com base na função de transição previamente definida. Neste exercício, você pode ignorar os parâmetros seed e options, pois não precisaremos deles. Neste ambiente que estamos desenvolvendo, o agente tem 80% de chance de se mover na direção desejada e 20% de chance de se mover em uma direção perpendicular à escolhida, distribuída igualmente entre os dois sentidos possíveis (10% para cada um). **As ações do agente devem ser representadas pelos valores 0 (mover-se para a esquerda), 1 (mover-se para baixo), 2 (mover-se para a direita) e 3 (mover-se para cima)**. Caso o agente tente se mover para fora do mapa, ele permanecerá na mesma posição. Além disso, a função atribui uma recompensa ao agente e verifica se o episódio chegou ao fim. Implemente a função step() para que ela retorne a observação do estado atual, a recompensa recebida, um valor booleano indicando se o estado é terminal, um valor booleano informando se o episódio foi truncado e as informações do ambiente. Esses dois últimos valores são necessários devidio à interface estabelecida pelo gymnasium, mas não se preocupe com eles; apenas retorne sempre `False` e `{}` para eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c82e329-a086-4479-8375-bb1545073294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d09d0c8292f8dbf0e2bdf379701d722",
     "grade": false,
     "grade_id": "cell-bd1f83bd5e0cfba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9016a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Installing packages for visual rendering...\n",
      "Installing pygame...\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m979.3 kB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "✅ pygame installed successfully\n",
      "Installing gymnasium[classic-control]...\n",
      "Requirement already satisfied: gymnasium[classic-control] in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.6.1)\n",
      "✅ gymnasium[classic-control] installed successfully\n",
      "Installing gymnasium[box2d]...\n",
      "Requirement already satisfied: gymnasium[box2d] in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Collecting swig==4.* (from gymnasium[box2d])\n",
      "  Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
      "Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m155.0 kB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\u001b[31m125.7 kB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'box2d-py' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'box2d-py'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Using setuptools (version 78.1.1).\n",
      "  \u001b[31m   \u001b[0m /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages/setuptools/_distutils/dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: zlib/libpng License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/Box2D.py -> build/lib.linux-x86_64-cpython-312/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/__init__.py -> build/lib.linux-x86_64-cpython-312/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/b2/__init__.py -> build/lib.linux-x86_64-cpython-312/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'Box2D._Box2D' extension\n",
      "  \u001b[31m   \u001b[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\n",
      "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build box2d-py\n",
      "❌ Failed to install gymnasium[box2d]: Command '['/home/danielterra/miniconda3/envs/rl-exercise/bin/python', '-m', 'pip', 'install', 'gymnasium[box2d]']' returned non-zero exit status 1.\n",
      "Installing gymnasium[toy-text]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[31m╰─>\u001b[0m box2d-py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (2.6.1)\n",
      "✅ gymnasium[toy-text] installed successfully\n",
      "✅ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for visual rendering\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for visual rendering\"\"\"\n",
    "    packages = [\n",
    "        \"pygame\",\n",
    "        \"gymnasium[classic-control]\",\n",
    "        \"gymnasium[box2d]\",\n",
    "        \"gymnasium[toy-text]\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✅ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Failed to install {package}: {e}\")\n",
    "\n",
    "# Run installation\n",
    "print(\"🔧 Installing packages for visual rendering...\")\n",
    "install_packages()\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f93c4b",
   "metadata": {},
   "source": [
    "## Testing Gymnasium Installation and Basic Usage\n",
    "\n",
    "Before we start implementing the FrozenLake environment, let's verify that Gymnasium is working correctly and understand the latest API changes.\n",
    "\n",
    "According to the [Gymnasium documentation](https://gymnasium.farama.org/introduction/basic_usage/), the API has evolved from the original OpenAI Gym. Key changes include:\n",
    "- The `step()` method now returns 5 values: `observation, reward, terminated, truncated, info`\n",
    "- `done` has been split into `terminated` (task completion/failure) and `truncated` (time limits)\n",
    "- Environment creation uses `gymnasium.make()` instead of `gym.make()`\n",
    "- The reset method returns `(observation, info)` instead of just `observation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f84cefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🧪 TESTING GYMNASIUM INSTALLATION AND API\n",
      "============================================================\n",
      "✅ Gymnasium version: 1.2.0\n",
      "\n",
      "📋 Testing CartPole environment creation...\n",
      "   Action space: Discrete(2)\n",
      "   Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "   Action meanings: 0=Push Left, 1=Push Right\n",
      "\n",
      "🔄 Testing new Gymnasium API...\n",
      "   Initial observation: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "   Initial info: {}\n",
      "   After step - Action: 0, Reward: 1.0\n",
      "   New observation: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "   Terminated: False, Truncated: False\n",
      "\n",
      "✅ CartPole environment test PASSED!\n",
      "\n",
      "🧊 Testing official FrozenLake environment...\n",
      "   Action space: Discrete(4)\n",
      "   Observation space: Discrete(16)\n",
      "   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
      "   Initial observation (state): 0\n",
      "   P[0][0] (state 0, action LEFT): [(1.0, 0, 0.0, False)]\n",
      "✅ FrozenLake environment test PASSED!\n",
      "\n",
      "🎉 ALL TESTS COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic Gymnasium installation and import\n",
    "print(\"=\" * 60)\n",
    "print(\"🧪 TESTING GYMNASIUM INSTALLATION AND API\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Gymnasium version: {gym.__version__}\")\n",
    "\n",
    "# Test 2: Create a simple environment to verify API\n",
    "print(f\"\\n📋 Testing CartPole environment creation...\")\n",
    "test_env = gym.make(\"CartPole-v1\")  # Removed render_mode=\"human\" for better compatibility\n",
    "print(f\"   Action space: {test_env.action_space}\")\n",
    "print(f\"   Observation space: {test_env.observation_space}\")\n",
    "print(f\"   Action meanings: 0=Push Left, 1=Push Right\")\n",
    "\n",
    "# Test 3: Test the new API with reset and step\n",
    "print(f\"\\n🔄 Testing new Gymnasium API...\")\n",
    "obs, info = test_env.reset(seed=42)\n",
    "print(f\"   Initial observation: {obs}\")\n",
    "print(f\"   Initial info: {info}\")\n",
    "\n",
    "action = test_env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "print(f\"   After step - Action: {action}, Reward: {reward}\")\n",
    "print(f\"   New observation: {obs}\")\n",
    "print(f\"   Terminated: {terminated}, Truncated: {truncated}\")\n",
    "\n",
    "test_env.close()\n",
    "print(\"\\n✅ CartPole environment test PASSED!\")\n",
    "\n",
    "# Test 4: Test FrozenLake environment (which we'll implement)\n",
    "print(f\"\\n🧊 Testing official FrozenLake environment...\")\n",
    "frozen_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "print(f\"   Action space: {frozen_env.action_space}\")\n",
    "print(f\"   Observation space: {frozen_env.observation_space}\")\n",
    "print(f\"   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\")\n",
    "\n",
    "obs, info = frozen_env.reset(seed=42)\n",
    "print(f\"   Initial observation (state): {obs}\")\n",
    "\n",
    "# Test transitions to understand the environment structure\n",
    "print(f\"   P[0][0] (state 0, action LEFT): {frozen_env.unwrapped.P[0][0]}\")\n",
    "frozen_env.close()\n",
    "print(\"✅ FrozenLake environment test PASSED!\")\n",
    "\n",
    "print(f\"\\n🎉 ALL TESTS COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db53409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚗 TESTING ANOTHER CLASSIC ENVIRONMENT: MOUNTAIN CAR\n",
      "============================================================\n",
      "Environment: MountainCar-v0\n",
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action meanings: 0=Push Left, 1=No Push, 2=Push Right\n",
      "\n",
      "Initial observation: [-0.4452088  0.       ]\n",
      "   Position: -0.4452 (range: -1.2 to 0.6)\n",
      "   Velocity: 0.0000 (range: -0.07 to 0.07)\n",
      "\n",
      "Taking 5 random actions:\n",
      "   Step 1: Action=1 (No Push), Position=-0.4458, Velocity=-0.0006, Reward=-1.0\n",
      "   Step 2: Action=0 (Push Left), Position=-0.4480, Velocity=-0.0022, Reward=-1.0\n",
      "   Step 3: Action=0 (Push Left), Position=-0.4517, Velocity=-0.0037, Reward=-1.0\n",
      "   Step 4: Action=0 (Push Left), Position=-0.4569, Velocity=-0.0053, Reward=-1.0\n",
      "   Step 5: Action=0 (Push Left), Position=-0.4637, Velocity=-0.0068, Reward=-1.0\n",
      "✅ MountainCar environment test PASSED!\n",
      "\n",
      "📊 COMPARISON OF ENVIRONMENT TYPES:\n",
      "   CartPole: Discrete actions, Continuous observations, Episode-based\n",
      "   FrozenLake: Discrete actions, Discrete observations, Grid-world\n",
      "   MountainCar: Discrete actions, Continuous observations, Physics-based\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Additional Environment Example - MountainCar\n",
    "print(\"=\" * 60)\n",
    "print(\"🚗 TESTING ANOTHER CLASSIC ENVIRONMENT: MOUNTAIN CAR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mountain_env = gym.make(\"MountainCar-v0\")\n",
    "print(f\"Environment: {mountain_env.spec.id}\")\n",
    "print(f\"Action space: {mountain_env.action_space}\")\n",
    "print(f\"Observation space: {mountain_env.observation_space}\")\n",
    "print(f\"Action meanings: 0=Push Left, 1=No Push, 2=Push Right\")\n",
    "\n",
    "# Reset and show initial state\n",
    "obs, info = mountain_env.reset(seed=42)\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "print(f\"   Position: {obs[0]:.4f} (range: -1.2 to 0.6)\")\n",
    "print(f\"   Velocity: {obs[1]:.4f} (range: -0.07 to 0.07)\")\n",
    "\n",
    "# Take a few random actions\n",
    "print(f\"\\nTaking 5 random actions:\")\n",
    "for i in range(5):\n",
    "    action = mountain_env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = mountain_env.step(action)\n",
    "    action_names = [\"Push Left\", \"No Push\", \"Push Right\"]\n",
    "    print(f\"   Step {i+1}: Action={action} ({action_names[action]}), \"\n",
    "          f\"Position={obs[0]:.4f}, Velocity={obs[1]:.4f}, Reward={reward}\")\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"   Episode ended! Terminated={terminated}, Truncated={truncated}\")\n",
    "        break\n",
    "\n",
    "mountain_env.close()\n",
    "print(\"✅ MountainCar environment test PASSED!\")\n",
    "\n",
    "print(f\"\\n📊 COMPARISON OF ENVIRONMENT TYPES:\")\n",
    "print(f\"   CartPole: Discrete actions, Continuous observations, Episode-based\")\n",
    "print(f\"   FrozenLake: Discrete actions, Discrete observations, Grid-world\")\n",
    "print(f\"   MountainCar: Discrete actions, Continuous observations, Physics-based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61c3dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🎮 INTERACTIVE EPISODE DEMONSTRATION\n",
      "============================================================\n",
      "Demo 1: FrozenLake Deterministic\n",
      "🎬 Starting episode in FrozenLake-v1\n",
      "📊 Action space: Discrete(4)\n",
      "📊 Observation space: Discrete(16)\n",
      "🎯 Initial state: 0\n",
      "\n",
      "🎮 Episode progression:\n",
      "--------------------------------------------------\n",
      "Step  1: Action=3 → State=1 [0,1] | Reward=0.0 | Total=0.0\n",
      "Step  2: Action=2 → State=5 [1,1] | Reward=0.0 | Total=0.0\n",
      "🏁 Episode ended at step 2!\n",
      "   Reason: Task completed\n",
      "📈 Final results: 2 steps, Total reward: 0.0\n",
      "\n",
      "============================================================\n",
      "Demo 2: CartPole Balancing\n",
      "🎬 Starting episode in CartPole-v1\n",
      "📊 Action space: Discrete(2)\n",
      "📊 Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "🎯 Initial state: [-0.00303268 -0.00523447 -0.03759432  0.025485  ]\n",
      "\n",
      "🎮 Episode progression:\n",
      "--------------------------------------------------\n",
      "Step  1: Action=1 → Obs=[-0.003, 0.190] | Reward=1.0 | Total=1.0\n",
      "Step  2: Action=0 → Obs=[0.001, -0.004] | Reward=1.0 | Total=2.0\n",
      "Step  3: Action=0 → Obs=[0.001, -0.199] | Reward=1.0 | Total=3.0\n",
      "Step  4: Action=0 → Obs=[-0.003, -0.393] | Reward=1.0 | Total=4.0\n",
      "Step  5: Action=1 → Obs=[-0.011, -0.198] | Reward=1.0 | Total=5.0\n",
      "Step  6: Action=1 → Obs=[-0.015, -0.002] | Reward=1.0 | Total=6.0\n",
      "Step  7: Action=0 → Obs=[-0.015, -0.197] | Reward=1.0 | Total=7.0\n",
      "Step  8: Action=1 → Obs=[-0.019, -0.001] | Reward=1.0 | Total=8.0\n",
      "Step  9: Action=0 → Obs=[-0.019, -0.196] | Reward=1.0 | Total=9.0\n",
      "Step 10: Action=1 → Obs=[-0.023, -0.001] | Reward=1.0 | Total=10.0\n",
      "📈 Final results: 10 steps, Total reward: 10.0\n",
      "\n",
      "🎯 SUMMARY:\n",
      "   FrozenLake: 2 steps, reward 0.0\n",
      "   CartPole: 10 steps, reward 10.0\n",
      "\n",
      "💡 Notice how different environments have different reward structures!\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Interactive Episode Demonstration\n",
    "print(\"=\" * 60)\n",
    "print(\"🎮 INTERACTIVE EPISODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_episode_demo(env_name, max_steps=20, seed=42):\n",
    "    \"\"\"\n",
    "    Run a complete episode with detailed step-by-step output.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "\n",
    "    print(f\"🎬 Starting episode in {env_name}\")\n",
    "    print(f\"📊 Action space: {env.action_space}\")\n",
    "    print(f\"📊 Observation space: {env.observation_space}\")\n",
    "    print(f\"🎯 Initial state: {obs}\")\n",
    "\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    print(f\"\\n🎮 Episode progression:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    while step_count < max_steps:\n",
    "        # Take random action\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        # Format output based on environment type\n",
    "        if env_name == \"FrozenLake-v1\":\n",
    "            # Convert state to grid coordinates\n",
    "            row, col = divmod(obs, 4)\n",
    "            print(f\"Step {step_count:2d}: Action={action} → State={obs} [{row},{col}] | Reward={reward} | Total={total_reward}\")\n",
    "        else:\n",
    "            # For continuous observations, show abbreviated version\n",
    "            if hasattr(obs, '__len__') and len(obs) > 1:\n",
    "                obs_str = f\"[{obs[0]:.3f}, {obs[1]:.3f}]\"\n",
    "            else:\n",
    "                obs_str = f\"{obs}\"\n",
    "            print(f\"Step {step_count:2d}: Action={action} → Obs={obs_str} | Reward={reward} | Total={total_reward}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"🏁 Episode ended at step {step_count}!\")\n",
    "            print(f\"   Reason: {'Task completed' if terminated else 'Time limit reached'}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"📈 Final results: {step_count} steps, Total reward: {total_reward}\")\n",
    "    return step_count, total_reward\n",
    "\n",
    "# Demo 1: FrozenLake episode (short and visual)\n",
    "print(\"Demo 1: FrozenLake Deterministic\")\n",
    "frozen_steps, frozen_reward = run_episode_demo(\"FrozenLake-v1\", max_steps=50, seed=123)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Demo 2: CartPole episode\n",
    "print(\"Demo 2: CartPole Balancing\")\n",
    "cartpole_steps, cartpole_reward = run_episode_demo(\"CartPole-v1\", max_steps=10, seed=456)\n",
    "\n",
    "print(f\"\\n🎯 SUMMARY:\")\n",
    "print(f\"   FrozenLake: {frozen_steps} steps, reward {frozen_reward}\")\n",
    "print(f\"   CartPole: {cartpole_steps} steps, reward {cartpole_reward}\")\n",
    "print(f\"\\n💡 Notice how different environments have different reward structures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8be285",
   "metadata": {},
   "source": [
    "## ✅ Gymnasium Testing Complete\n",
    "\n",
    "### **What we've verified:**\n",
    "\n",
    "1. **🔧 Installation**: Gymnasium 1.2.0 is properly installed and working\n",
    "2. **📡 API Compatibility**: New API structure confirmed:\n",
    "   - `reset()` returns `(observation, info)`\n",
    "   - `step()` returns `(observation, reward, terminated, truncated, info)`\n",
    "   - Proper separation of `terminated` vs `truncated`\n",
    "\n",
    "3. **🎮 Environment Examples Tested:**\n",
    "   - **CartPole-v1**: Continuous observations, discrete actions, balancing task\n",
    "   - **FrozenLake-v1**: Discrete observations, discrete actions, grid navigation  \n",
    "   - **MountainCar-v0**: Continuous observations, discrete actions, physics simulation\n",
    "\n",
    "4. **📊 Transition Structure**: Verified access to `env.unwrapped.P[state][action]` for FrozenLake\n",
    "\n",
    "### **Key Insights for Implementation:**\n",
    "- ✅ Environment spaces are correctly defined using `gym.spaces.Discrete`\n",
    "- ✅ State transitions follow the expected format: `[(prob, next_state, reward, terminated), ...]`\n",
    "- ✅ Random seeding works consistently for reproducible results\n",
    "- ✅ Different environments have different reward structures and dynamics\n",
    "\n",
    "### **Ready for Implementation:**\n",
    "We now have a solid foundation to implement our custom FrozenLake environment following the established patterns and API conventions. The analysis shows exactly how state transitions, rewards, and termination conditions should be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5ac8f",
   "metadata": {},
   "source": [
    "## 🎬 Visual Interactive Examples - Pop-up Windows\n",
    "\n",
    "The following examples will open visual windows where you can see the environments in action! Each environment will display in a separate pop-up window showing real-time interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d881f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎪 VISUAL CARTPOLE DEMONSTRATION\n",
      "==================================================\n",
      "🎬 Attempting to open CartPole window...\n",
      "✅ CartPole environment created successfully!\n",
      "🎯 Starting CartPole visual episode...\n",
      "📺 A window should have opened showing the CartPole simulation!\n",
      "🎮 Taking random actions - watch the window!\n",
      "   Step  0: Push Right | Pole angle: 0.036 | Total reward: 1.0\n",
      "   Step  5: Push Left | Pole angle: 0.000 | Total reward: 6.0\n",
      "   Step 10: Push Left | Pole angle: -0.027 | Total reward: 11.0\n",
      "   Step 15: Push Right | Pole angle: -0.053 | Total reward: 16.0\n",
      "📊 Final Results: 20 steps, Total reward: 20.0\n",
      "✅ CartPole visual demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Visual Example 1: CartPole with Real-time Display\n",
    "import time\n",
    "\n",
    "print(\"🎪 VISUAL CARTPOLE DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🎬 Attempting to open CartPole window...\")\n",
    "\n",
    "try:\n",
    "    # Create environment with human rendering (pop-up window)\n",
    "    cartpole_visual = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    print(\"✅ CartPole environment created successfully!\")\n",
    "\n",
    "    # Reset and start episode\n",
    "    obs, info = cartpole_visual.reset(seed=42)\n",
    "    print(f\"🎯 Starting CartPole visual episode...\")\n",
    "    print(\"📺 A window should have opened showing the CartPole simulation!\")\n",
    "\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    max_steps = 20  # Reduced for better performance\n",
    "\n",
    "    print(\"🎮 Taking random actions - watch the window!\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take a random action\n",
    "        action = cartpole_visual.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = cartpole_visual.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Print progress every 5 steps\n",
    "        if step % 5 == 0:\n",
    "            action_name = \"Push Left\" if action == 0 else \"Push Right\"\n",
    "            print(f\"   Step {step:2d}: {action_name} | Pole angle: {obs[2]:.3f} | Total reward: {total_reward}\")\n",
    "\n",
    "        # Add small delay to see the action\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        # Check if episode ended\n",
    "        if terminated or truncated:\n",
    "            print(f\"🏁 Episode ended at step {step}!\")\n",
    "            print(f\"   Reason: {'Pole fell down' if terminated else 'Time limit reached'}\")\n",
    "            break\n",
    "\n",
    "    cartpole_visual.close()\n",
    "    print(f\"📊 Final Results: {step_count} steps, Total reward: {total_reward}\")\n",
    "    print(\"✅ CartPole visual demonstration complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running visual demonstration: {e}\")\n",
    "    print(\"💡 This might be due to display/GUI limitations in the current environment\")\n",
    "    print(\"🔄 Running text-only version instead...\")\n",
    "\n",
    "    # Fallback to text-only version\n",
    "    cartpole_text = gym.make(\"CartPole-v1\")\n",
    "    obs, info = cartpole_text.reset(seed=42)\n",
    "\n",
    "    print(\"📊 Text-only CartPole demonstration:\")\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(10):\n",
    "        action = cartpole_text.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = cartpole_text.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        action_name = \"Push Left\" if action == 0 else \"Push Right\"\n",
    "        print(f\"   Step {step:2d}: {action_name} | Pole angle: {obs[2]:.3f} | Reward: {reward}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    cartpole_text.close()\n",
    "    print(f\"✅ Text demonstration complete! Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86270251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Example 2: MountainCar with Real-time Display\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🏔️  VISUAL MOUNTAIN CAR DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🎬 Opening MountainCar window... Watch the car trying to reach the flag!\")\n",
    "print(\"⏱️  Episode will run for 50 steps to show the physics\")\n",
    "\n",
    "# Create environment with human rendering (pop-up window)\n",
    "mountain_visual = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset and start episode\n",
    "obs, info = mountain_visual.reset(seed=123)\n",
    "print(f\"🎯 Starting MountainCar visual episode...\")\n",
    "print(f\"🚗 Initial position: {obs[0]:.3f}, velocity: {obs[1]:.3f}\")\n",
    "\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "max_steps = 50\n",
    "\n",
    "print(\"🎮 Taking strategic actions - watch the car build momentum!\")\n",
    "\n",
    "for step in range(max_steps):\n",
    "    # Render the current state (updates the visual window)\n",
    "    mountain_visual.render()\n",
    "\n",
    "    # Take a strategic action (try to build momentum)\n",
    "    if obs[1] < 0:  # If moving left, push left to build momentum\n",
    "        action = 0\n",
    "    elif obs[1] > 0:  # If moving right, push right to build momentum\n",
    "        action = 2\n",
    "    else:  # If stationary, push right to start moving\n",
    "        action = 2\n",
    "\n",
    "    obs, reward, terminated, truncated, info = mountain_visual.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "\n",
    "    # Print progress every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        action_names = [\"Push Left\", \"No Push\", \"Push Right\"]\n",
    "        print(f\"   Step {step:2d}: {action_names[action]} | Pos: {obs[0]:.3f} | Vel: {obs[1]:.3f} | Reward: {reward}\")\n",
    "\n",
    "    # Add small delay to see the movement\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Check if episode ended (reached the flag)\n",
    "    if terminated or truncated:\n",
    "        print(f\"🏁 Episode ended at step {step}!\")\n",
    "        if terminated:\n",
    "            print(\"🎉 SUCCESS! Car reached the flag!\")\n",
    "        else:\n",
    "            print(\"⏰ Time limit reached\")\n",
    "        break\n",
    "\n",
    "mountain_visual.close()\n",
    "print(f\"📊 Final Results: {step_count} steps, Total reward: {total_reward}\")\n",
    "print(\"✅ MountainCar visual demonstration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a760b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🧊 VISUAL FROZEN LAKE DEMONSTRATION\n",
      "==================================================\n",
      "🎬 Attempting to open FrozenLake window...\n",
      "✅ FrozenLake environment created successfully!\n",
      "🎯 Starting FrozenLake visual episode...\n",
      "📺 A window should have opened showing the FrozenLake grid!\n",
      "🧊 Initial state: 0 (position [0,0])\n",
      "🎮 Taking random actions - watch the agent slip on the ice!\n",
      "Legend: S=Start, F=Frozen(safe), H=Hole(danger), G=Goal\n",
      "   Step  1: At [0,0] (S) → Action: DOWN ↓ → Landed at [1,0] (F) | Reward: 0.0\n",
      "   Step  2: At [1,0] (F) → Action: RIGHT → → Landed at [2,0] (F) | Reward: 0.0\n",
      "   Step  3: At [2,0] (F) → Action: LEFT ← → Landed at [3,0] (H) | Reward: 0.0\n",
      "🏁 Episode ended at step 3!\n",
      "💀 FAILED! Agent fell into a hole!\n",
      "📊 Final Results: 3 steps, Total reward: 0.0\n",
      "✅ FrozenLake visual demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Visual Example 3: FrozenLake with Real-time Display\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🧊 VISUAL FROZEN LAKE DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🎬 Attempting to open FrozenLake window...\")\n",
    "\n",
    "try:\n",
    "    # Create environment with human rendering (pop-up window)\n",
    "    frozen_visual = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "    print(\"✅ FrozenLake environment created successfully!\")\n",
    "\n",
    "    # Reset and start episode\n",
    "    obs, info = frozen_visual.reset(seed=456)\n",
    "    print(f\"🎯 Starting FrozenLake visual episode...\")\n",
    "    print(\"📺 A window should have opened showing the FrozenLake grid!\")\n",
    "    print(f\"🧊 Initial state: {obs} (position [0,0])\")\n",
    "\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    max_steps = 15  # Reduced for better visibility\n",
    "\n",
    "    action_names = [\"LEFT ←\", \"DOWN ↓\", \"RIGHT →\", \"UP ↑\"]\n",
    "    map_symbols = {0: 'S', 1: 'F', 2: 'F', 3: 'F', 4: 'F', 5: 'H', 6: 'F', 7: 'H',\n",
    "                   8: 'F', 9: 'F', 10: 'F', 11: 'H', 12: 'H', 13: 'F', 14: 'F', 15: 'G'}\n",
    "\n",
    "    print(\"🎮 Taking random actions - watch the agent slip on the ice!\")\n",
    "    print(\"Legend: S=Start, F=Frozen(safe), H=Hole(danger), G=Goal\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take a random action\n",
    "        action = frozen_visual.action_space.sample()\n",
    "\n",
    "        # Show intended action\n",
    "        row, col = divmod(obs, 4)\n",
    "        print(f\"   Step {step+1:2d}: At [{row},{col}] ({map_symbols[obs]}) → Action: {action_names[action]}\", end=\"\")\n",
    "\n",
    "        obs, reward, terminated, truncated, info = frozen_visual.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Show result\n",
    "        new_row, new_col = divmod(obs, 4)\n",
    "        print(f\" → Landed at [{new_row},{new_col}] ({map_symbols[obs]}) | Reward: {reward}\")\n",
    "\n",
    "        # Add delay to see the movement\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        # Check if episode ended\n",
    "        if terminated or truncated:\n",
    "            print(f\"🏁 Episode ended at step {step+1}!\")\n",
    "            if obs == 15:  # Goal state\n",
    "                print(\"🎉 SUCCESS! Agent reached the goal!\")\n",
    "            elif map_symbols[obs] == 'H':\n",
    "                print(\"💀 FAILED! Agent fell into a hole!\")\n",
    "            break\n",
    "\n",
    "    frozen_visual.close()\n",
    "    print(f\"📊 Final Results: {step_count} steps, Total reward: {total_reward}\")\n",
    "    print(\"✅ FrozenLake visual demonstration complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running visual demonstration: {e}\")\n",
    "    print(\"💡 This might be due to display/GUI limitations in the current environment\")\n",
    "    print(\"🔄 Running text-only version instead...\")\n",
    "\n",
    "    # Fallback to text-only version\n",
    "    frozen_text = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "    obs, info = frozen_text.reset(seed=456)\n",
    "\n",
    "    print(\"📊 Text-only FrozenLake demonstration:\")\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(10):\n",
    "        action = frozen_text.action_space.sample()\n",
    "        row, col = divmod(obs, 4)\n",
    "        action_names = [\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"]\n",
    "\n",
    "        print(f\"   Step {step+1}: At [{row},{col}] → {action_names[action]}\", end=\"\")\n",
    "\n",
    "        obs, reward, terminated, truncated, info = frozen_text.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        new_row, new_col = divmod(obs, 4)\n",
    "        print(f\" → [{new_row},{new_col}] | Reward: {reward}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if obs == 15:\n",
    "                print(\"🎉 SUCCESS! Reached goal!\")\n",
    "            else:\n",
    "                print(\"💀 Fell in hole!\")\n",
    "            break\n",
    "\n",
    "    frozen_text.close()\n",
    "    print(f\"✅ Text demonstration complete! Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983ef660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPREHENSIVE VISUAL DEMONSTRATION\n",
      "============================================================\n",
      "This will demonstrate multiple environments with pop-up windows!\n",
      "You should see separate windows opening for each environment.\n",
      "Each demo runs for a short time to show the key features.\n",
      "\n",
      "Starting visual demonstrations...\n",
      "Make sure your display is ready to show pop-up windows!\n",
      "\n",
      "Demo 1/2: CartPole\n",
      "\n",
      "--- CARTPOLE DEMO ---\n",
      "SUCCESS: CartPole window should now be open!\n",
      "  Step 0: Action 0 | Reward: 1.0\n",
      "  Step 2: Action 0 | Reward: 1.0\n",
      "  Step 4: Action 0 | Reward: 1.0\n",
      "  Step 6: Action 1 | Reward: 1.0\n",
      "COMPLETE: CartPole demo finished! Total reward: 8.0\n",
      "\n",
      "Demo 2/2: FrozenLake\n",
      "\n",
      "--- FROZENLAKE DEMO ---\n",
      "SUCCESS: FrozenLake window should now be open!\n",
      "  Step 0: Action 2 | Reward: 0.0\n",
      "  Step 2: Action 3 | Reward: 0.0\n",
      "  Step 4: Action 1 | Reward: 0.0\n",
      "  Episode ended at step 5!\n",
      "COMPLETE: FrozenLake demo finished! Total reward: 0.0\n",
      "\n",
      "ALL VISUAL DEMONSTRATIONS COMPLETE!\n",
      "Summary of what you should have seen:\n",
      "  1. CartPole: Cart with pole balancing simulation\n",
      "  2. FrozenLake: Grid navigation game\n",
      "\n",
      "If windows didn't appear, it might be due to:\n",
      "  - Headless environment (no display)\n",
      "  - System restrictions\n",
      "  - Missing display drivers\n",
      "\n",
      "The environments are working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Final Visual Demo: Multiple Environments\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE VISUAL DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will demonstrate multiple environments with pop-up windows!\")\n",
    "print(\"You should see separate windows opening for each environment.\")\n",
    "print(\"Each demo runs for a short time to show the key features.\")\n",
    "\n",
    "def run_visual_environment(env_name, config, demo_name, steps=8):\n",
    "    \"\"\"Run a visual demonstration of an environment.\"\"\"\n",
    "    print(f\"\\n--- {demo_name.upper()} DEMO ---\")\n",
    "\n",
    "    try:\n",
    "        env = gym.make(env_name, **config)\n",
    "        print(f\"SUCCESS: {demo_name} window should now be open!\")\n",
    "\n",
    "        obs, info = env.reset(seed=42)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(steps):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if step % 2 == 0:  # Print every 2nd step\n",
    "                print(f\"  Step {step}: Action {action} | Reward: {reward}\")\n",
    "\n",
    "            time.sleep(0.4)  # Pause to see the action\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(f\"  Episode ended at step {step}!\")\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "        print(f\"COMPLETE: {demo_name} demo finished! Total reward: {total_reward}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not run visual demo for {demo_name}: {e}\")\n",
    "\n",
    "# Environment configurations\n",
    "print(f\"\\nStarting visual demonstrations...\")\n",
    "print(f\"Make sure your display is ready to show pop-up windows!\")\n",
    "\n",
    "# Demo 1: CartPole\n",
    "print(f\"\\nDemo 1/2: CartPole\")\n",
    "run_visual_environment(\"CartPole-v1\", {\"render_mode\": \"human\"}, \"CartPole\", 8)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Demo 2: FrozenLake\n",
    "print(f\"\\nDemo 2/2: FrozenLake\")\n",
    "run_visual_environment(\"FrozenLake-v1\",\n",
    "                      {\"map_name\": \"4x4\", \"is_slippery\": False, \"render_mode\": \"human\"},\n",
    "                      \"FrozenLake\", 10)\n",
    "\n",
    "print(f\"\\nALL VISUAL DEMONSTRATIONS COMPLETE!\")\n",
    "print(f\"Summary of what you should have seen:\")\n",
    "print(f\"  1. CartPole: Cart with pole balancing simulation\")\n",
    "print(f\"  2. FrozenLake: Grid navigation game\")\n",
    "print(f\"\\nIf windows didn't appear, it might be due to:\")\n",
    "print(f\"  - Headless environment (no display)\")\n",
    "print(f\"  - System restrictions\")\n",
    "print(f\"  - Missing display drivers\")\n",
    "print(f\"\\nThe environments are working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a945204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYSIS OF FROZENLAKE ENVIRONMENT STRUCTURE ===\n",
      "\n",
      "1. ENVIRONMENT DIMENSIONS:\n",
      "   Map description shape: (4, 4)\n",
      "   Total states: 16\n",
      "   Total actions: 4\n",
      "   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
      "\n",
      "2. MAP LAYOUT:\n",
      "   Row 0: ['S', 'F', 'F', 'F']\n",
      "   Row 1: ['F', 'H', 'F', 'H']\n",
      "   Row 2: ['F', 'F', 'F', 'H']\n",
      "   Row 3: ['H', 'F', 'F', 'G']\n",
      "\n",
      "3. TRANSITION PROBABILITIES STRUCTURE:\n",
      "   Format: [(probability, next_state, reward, terminated), ...]\n",
      "   Deterministic env, state 0, action 0 (LEFT): [(1.0, 0, 0.0, False)]\n",
      "   Deterministic env, state 0, action 1 (DOWN): [(1.0, 4, 0.0, False)]\n",
      "   Slippery env, state 0, action 1 (DOWN): [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "\n",
      "4. STATE-TO-COORDINATE MAPPING:\n",
      "   States are numbered 0-15 in row-major order:\n",
      "   State  0 -> (0,0) = 'S'\n",
      "   State  1 -> (0,1) = 'F'\n",
      "   State  2 -> (0,2) = 'F'\n",
      "   State  3 -> (0,3) = 'F'\n",
      "   State  4 -> (1,0) = 'F'\n",
      "   State  5 -> (1,1) = 'H'\n",
      "   State  6 -> (1,2) = 'F'\n",
      "   State  7 -> (1,3) = 'H'\n",
      "   State  8 -> (2,0) = 'F'\n",
      "   State  9 -> (2,1) = 'F'\n",
      "   State 10 -> (2,2) = 'F'\n",
      "   State 11 -> (2,3) = 'H'\n",
      "   State 12 -> (3,0) = 'H'\n",
      "   State 13 -> (3,1) = 'F'\n",
      "   State 14 -> (3,2) = 'F'\n",
      "   State 15 -> (3,3) = 'G'\n",
      "\n",
      "5. KEY INSIGHTS FOR IMPLEMENTATION:\n",
      "   - States: 0-15 (single discrete value)\n",
      "   - Actions: 0-3 (LEFT, DOWN, RIGHT, UP)\n",
      "   - Transitions: P[state][action] gives list of (prob, next_state, reward, terminated)\n",
      "   - Rewards: 0 everywhere except goal state (reward=1)\n",
      "   - Terminal states: holes and goal\n",
      "   - Slippery: 1/3 intended direction, 1/3 each perpendicular direction\n",
      "\n",
      "✅ Environment structure analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis of FrozenLake environment structure\n",
    "print(\"=== ANALYSIS OF FROZENLAKE ENVIRONMENT STRUCTURE ===\\n\")\n",
    "\n",
    "# Create both deterministic and slippery versions for comparison\n",
    "deterministic_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "slippery_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "print(\"1. ENVIRONMENT DIMENSIONS:\")\n",
    "print(f\"   Map description shape: {deterministic_env.unwrapped.desc.shape}\")\n",
    "print(f\"   Total states: {deterministic_env.observation_space.n}\")\n",
    "print(f\"   Total actions: {deterministic_env.action_space.n}\")\n",
    "print(f\"   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\")\n",
    "\n",
    "print(\"\\n2. MAP LAYOUT:\")\n",
    "desc = deterministic_env.unwrapped.desc\n",
    "for i, row in enumerate(desc):\n",
    "    print(f\"   Row {i}: {[cell.decode() for cell in row]}\")\n",
    "\n",
    "print(\"\\n3. TRANSITION PROBABILITIES STRUCTURE:\")\n",
    "print(\"   Format: [(probability, next_state, reward, terminated), ...]\")\n",
    "print(f\"   Deterministic env, state 0, action 0 (LEFT): {deterministic_env.unwrapped.P[0][0]}\")\n",
    "print(f\"   Deterministic env, state 0, action 1 (DOWN): {deterministic_env.unwrapped.P[0][1]}\")\n",
    "print(f\"   Slippery env, state 0, action 1 (DOWN): {slippery_env.unwrapped.P[0][1]}\")\n",
    "\n",
    "print(\"\\n4. STATE-TO-COORDINATE MAPPING:\")\n",
    "print(\"   States are numbered 0-15 in row-major order:\")\n",
    "for state in range(16):\n",
    "    row, col = divmod(state, 4)\n",
    "    cell_type = desc[row, col].decode()\n",
    "    print(f\"   State {state:2d} -> ({row},{col}) = '{cell_type}'\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS FOR IMPLEMENTATION:\")\n",
    "print(\"   - States: 0-15 (single discrete value)\")\n",
    "print(\"   - Actions: 0-3 (LEFT, DOWN, RIGHT, UP)\")\n",
    "print(\"   - Transitions: P[state][action] gives list of (prob, next_state, reward, terminated)\")\n",
    "print(\"   - Rewards: 0 everywhere except goal state (reward=1)\")\n",
    "print(\"   - Terminal states: holes and goal\")\n",
    "print(\"   - Slippery: 1/3 intended direction, 1/3 each perpendicular direction\")\n",
    "\n",
    "deterministic_env.close()\n",
    "slippery_env.close()\n",
    "print(\"\\n✅ Environment structure analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04568df0",
   "metadata": {},
   "source": [
    "## Implementation Strategy and Best Practices\n",
    "\n",
    "Based on the analysis above and following RL engineering best practices, our implementation should:\n",
    "\n",
    "### 🔍 **Key Design Principles:**\n",
    "1. **Correctness over Convenience**: Ensure exact adherence to Gymnasium API\n",
    "2. **Reproducibility**: Use proper random seeding and deterministic state management\n",
    "3. **Safety**: Validate inputs and handle edge cases gracefully\n",
    "4. **Maintainability**: Write clean, well-documented code with clear separation of concerns\n",
    "\n",
    "### 📋 **Implementation Checklist:**\n",
    "\n",
    "#### **Environment Structure (Priority: High)**\n",
    "- [ ] Proper inheritance from `gym.Env`\n",
    "- [ ] Correct space definitions using `gym.spaces.Discrete`\n",
    "- [ ] State representation: single integer 0-15\n",
    "- [ ] Action representation: single integer 0-3 (LEFT, DOWN, RIGHT, UP)\n",
    "\n",
    "#### **Core Methods (Priority: High)**\n",
    "- [ ] `__init__()`: Initialize spaces and internal state\n",
    "- [ ] `reset()`: Return `(observation, info)` tuple, proper seeding\n",
    "- [ ] `step()`: Return `(observation, reward, terminated, truncated, info)` tuple\n",
    "- [ ] Transition function with proper slippery mechanics (80% intended, 10% each perpendicular)\n",
    "\n",
    "#### **Helper Methods (Priority: Medium)**\n",
    "- [ ] `_get_obs()`: Convert internal state to observation\n",
    "- [ ] `_set_state()`: Validate and set internal state\n",
    "- [ ] Boundary checking for attempted moves outside grid\n",
    "- [ ] Proper reward calculation (1 for goal, 0 elsewhere)\n",
    "\n",
    "#### **Validation & Testing (Priority: High)**\n",
    "- [ ] Input validation for actions and states\n",
    "- [ ] Edge case handling (boundaries, terminal states)\n",
    "- [ ] Consistency checks with reference implementation\n",
    "- [ ] Deterministic behavior for testing\n",
    "\n",
    "### ⚠️ **Common Pitfalls to Avoid:**\n",
    "1. **API Inconsistency**: Not returning correct tuple structures\n",
    "2. **State Management**: Forgetting to update internal state properly\n",
    "3. **Transition Logic**: Incorrect slippery movement implementation\n",
    "4. **Boundary Handling**: Allowing invalid moves or state transitions\n",
    "5. **Seeding Issues**: Not properly handling random state for reproducibility\n",
    "\n",
    "### 🧪 **Testing Strategy:**\n",
    "1. **Unit Tests**: Test each method individually\n",
    "2. **Integration Tests**: Full episode runs\n",
    "3. **Consistency Tests**: Compare with official implementation\n",
    "4. **Edge Case Tests**: Boundary conditions and invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e848a9-20d8-4688-a080-306628fa3b9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6883a4d3a67114700d7686062bf221",
     "grade": false,
     "grade_id": "cell-b8e10c8d02b6faa6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self._description = np.asarray([\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ], dtype='c')\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, action):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cae5e-1406-4fa2-9d93-1b9af302ba90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e7b9ba331c5e3fa3a00100a2984bec",
     "grade": false,
     "grade_id": "cell-c99ad325d97fb8d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Certifique-se que seu ambiente funciona na célula abaixo.\n",
    "\n",
    "**Atenção:** os testes fornecidos não cobrem todos os casos possíveis. Realize testes adicionais para garantir a implementação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a5ed-2e90-4ce2-bb76-2b6360055ed9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392f0ae9859df2eb60fbf9f1d3ac80a7",
     "grade": false,
     "grade_id": "cell-ddead0156e8c7432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = FrozenLake()\n",
    "\n",
    "obs, info = env.reset()\n",
    "assert obs == 0, f\"Observação inicial esperada 0, recebeu {obs}\"\n",
    "\n",
    "env._set_state(5)\n",
    "obs = env._get_obs()\n",
    "assert obs == 5, f\"Estado esperado 5, recebeu {obs}\"\n",
    "\n",
    "for _ in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    assert 0 <= action < 4, f\"Ação fora do intervalo esperado: {action}\"\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    assert 0 <= obs < 16, f\"Observação fora do intervalo esperado: {obs}\"\n",
    "    assert reward in [0, 1], f\"Recompensa inválida: {reward}\"\n",
    "    assert isinstance(terminated, bool), f\"'terminated' deve ser bool, mas recebeu {type(terminated)}\"\n",
    "    assert truncated is False, f\"'truncated' deve ser False, mas recebeu {truncated}\"\n",
    "    assert isinstance(info, dict), f\"'info' deve ser dict, mas recebeu {type(info)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8e99e-94bb-426f-a7e0-106dad2769b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78bddd1db45c2e8b9e54c20f0791463",
     "grade": true,
     "grade_id": "cell-20901ef53a25a2b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673e83f-1c2d-4934-bacb-0e879fcf6eb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e93d2945e347d584c03b27844df147f6",
     "grade": true,
     "grade_id": "cell-b30cbb7c1fa808b0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d278f-5787-44d4-95b1-7e537b15db8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977788f0df2f7412652fd64ee795ff21",
     "grade": false,
     "grade_id": "cell-8b132c80e15a2de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a3b1-ce03-42e0-9a4a-c7760aa904be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b330df534aeb8aecd6308dd337e8bc46",
     "grade": false,
     "grade_id": "cell-e6a8d0aebce03142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora que estamos familiarizados com o ambiente Frozen Lake, nosso objetivo será encontrar uma política ótima para ele.  Desta vez, utilizaremos a versão oficial do Frozen Lake, disponibilizado pelo Gymnasium. Ele possui algumas propriedades que facilitarão as próximas implementações. Sua tarefa será implementar o algoritmo *Policy Iteration*, conforme ilustrado abaixo.\n",
    "\n",
    "![Policy Iteration](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cbfcc-d78a-4409-91b7-ae40d26d254c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248bbc9df645c2d3c87ae09f1c751a52",
     "grade": false,
     "grade_id": "cell-80af17ade7f65c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. A implementação será realizada em etapas. Comece implentando a função `init_policy_iteration`, que inicializa e retorna dois arrays. O primeiro array armazenará os valores esperados de cada estado $V(s)$, enquanto o segundo conterá a política do agente: para cada estado, ele indicará a ação que o agente deve realizar. Ambos os arrays devem ser inicializados com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afa70b-1d9e-4cfd-b026-9669cb329d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1465ddc541cd84973baead52e1296a77",
     "grade": false,
     "grade_id": "cell-0fb77ab1fc981da0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_policy_iteration(env: gym.Env) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c76e-1a7b-4fa0-afe7-8aa4296a852a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcda365a4d97947d565e8c44d241610c",
     "grade": false,
     "grade_id": "cell-833ab69b40216f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Agora, vamos computar o valor esperado $V(s) = \\sum_{s', r}p(s',r|s, a)[r + \\gamma V(s')]$. Implemente a função `compute_expected_value`que recebe como parâmetros o ambiente, o vetor $V$, um estado, uma ação, o valor de $\\gamma$ (fator de desconto), e retorna o valor esperado. Não altere os valores de $V$ nesta função.\n",
    "\n",
    "**Importante:** A variável `env.unwrapped.P[state][action]` contém as transições do ambiente, retornando uma lista com todas as transições possíveis para o par (state, action). Cada elemento dessa lista inclui, na seguinte ordem: a probabilidade da transição, o estado $s'$ alcançado, a recompensa recebida e um indicador de estado terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88dd79-de71-4c91-bc21-13f6db8f350e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086e19afee3929471d473c6b70e42a7e",
     "grade": false,
     "grade_id": "cell-fbb3d72642775bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_value(env: gym.Env, V: np.ndarray[float], state: int, action: int, gamma: float) -> float:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c4f38-ec8c-4d43-ac71-68243b719428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd97b9bb7399cf9d8ad4b4f9d324c34",
     "grade": false,
     "grade_id": "cell-b94451eaf15d9d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. O pŕoximo passo será avaliar a política do agente. Implemente o loop de avaliação de política do policy iteration na função `evaluate_policy`. Ela receberá o ambiente, a política do agente, o vetor $V$, o valor $\\gamma$, e o valor $\\theta$. Esta função não precisa retornar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b124c-9ed3-47aa-a7bb-e1a198bdb87b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc74a4d30b19f68b2a836179a817e2f6",
     "grade": false,
     "grade_id": "cell-8df0495bf82eaaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float, theta: float) -> None:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8faf3-7d83-4c24-8f38-110981cf296b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a2d75b60f5350454137839a5e61ce25",
     "grade": false,
     "grade_id": "cell-deee4c0a66b4ef76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. A seguir, vamos implementar a atualização da política. Na função `improve_policy` implemente uma iteração da atualização da política. Ela recebe o ambiente, a política do agente, o vetor $V$, e o valor $\\gamma$. Ela deverá retornar um booleano indicando se política está estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818354d1-3e10-4ac8-96cd-51eff85c54a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338436f555ec731b7fee10a05a929d5",
     "grade": false,
     "grade_id": "cell-4604a015625bbd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886671-c6a1-4bf8-8258-137c8030d8fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e9728ce314ad841c660af9d5745807",
     "grade": false,
     "grade_id": "cell-59ebc175679651bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo implementa a estrutura do algoritmo *Policy Iteration* utilizando as funções desenvolvidas nas etapas anteriores. Não é necessário realizar nenhuma implementação nesta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab5bc4-ac2a-4a79-b5d5-7b6d668805b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc9300dfe827083e071ca31f8ddcefd",
     "grade": false,
     "grade_id": "cell-250e7d4c7bc0cf97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    V, policy = init_policy_iteration(env)\n",
    "\n",
    "    while True:\n",
    "        evaluate_policy(env, policy, V, gamma, theta)\n",
    "        policy_stable = improve_policy(env, policy, V, gamma)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef24a7-7071-4c51-926c-32b9e9c39c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0803fbd416c4e12f431191163c7a3d",
     "grade": false,
     "grade_id": "cell-ed343128eb786bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_policy(env:gym.Env, policy: np.ndarray[int]):\n",
    "    \"\"\"\n",
    "    Exibe a política de um ambiente FrozenLake de forma visual.\n",
    "\n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    env : gym.Env\n",
    "        Ambiente do tipo FrozenLake.\n",
    "    policy : np.ndarray\n",
    "        Array 1D contendo as ações a serem tomadas em cada estado.\n",
    "\n",
    "    Ações são mapeadas para setas:\n",
    "        0: '←', 1: '↓', 2: '→', 3: '↑'\n",
    "\n",
    "    Símbolos especiais do mapa:\n",
    "        'H': buraco → '▢'\n",
    "        'G': objetivo → '◎'\n",
    "    \"\"\"\n",
    "\n",
    "    ACTION_MAP = ['←', '↓', '→', '↑']\n",
    "    HOLE_SYMBOL = '▢'\n",
    "    GOAL_SYMBOL = '◎'\n",
    "\n",
    "    n_rows, n_cols = env.unwrapped.desc.shape\n",
    "    policy_grid = np.full((n_rows, n_cols), \"\", dtype=str)\n",
    "\n",
    "    for index, action in enumerate(policy):\n",
    "        row, col = divmod(index, 4)\n",
    "        cell = env.unwrapped.desc[row, col]\n",
    "        if cell == b'H':\n",
    "            policy_grid[row, col] = HOLE_SYMBOL\n",
    "        elif cell == b'G':\n",
    "            policy_grid[row, col] = GOAL_SYMBOL\n",
    "        else:\n",
    "            policy_grid[row, col] = ACTION_MAP[action]\n",
    "\n",
    "    np.savetxt(sys.stdout, policy_grid, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5ed1-6b65-425f-894e-769d90603bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835c3efa9e65c2e9dfb7ad4c8f691ea9",
     "grade": false,
     "grade_id": "cell-98df69850425fb45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Policy Iteration* em um ambiente Frozen Lake determinístico, ou seja, onde o agente não corre o risco de escorregar para direções indesejadas. A política resultante será armazenada na variável `policy_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que o algoritmo esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d8872-c213-44e4-ae3b-d187c7a7f097",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8473ab04eee00f9f4041247dae2091d9",
     "grade": true,
     "grade_id": "cell-d1021093fae62b6c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, policy_iteration_deterministic = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, policy_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(policy_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3731f7-5e61-4b1c-adc9-ce021b8345ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "391cc7348680b297fc58d3be91d0dd5e",
     "grade": true,
     "grade_id": "cell-a422263f5dd5254d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297afae4-639a-4741-811a-1693ce726413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75f8abf27b293dd60179ff3844f5c8f0",
     "grade": false,
     "grade_id": "cell-5e9663f3341ea0cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf17ac-9e34-4b94-a8c0-abefbd22c201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "213cec389fa200d841b8e593f5a6d5f5",
     "grade": false,
     "grade_id": "cell-da562c89000eeffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Neste exercício vamos encontrar uma política ótima para o Frozen Lake utilizando o algoritmo *Value Iteration* como descrito abaixo.\n",
    "\n",
    "![Value Iteration](value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4db7-80f5-4964-aa87-bf561413c5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "905d1ad1a904f82f88c7c68c0ab5341b",
     "grade": false,
     "grade_id": "cell-179ab2fb26b003ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Novamente, vamos dividir este exercícios em etapas menores. O primeiro passo consiste em inicializar o vetor $V$, que armazenará os valores esperados para cada estado. Para isso, implemente a função `init_value_iteration`, que recebe um ambiente como parâmetro e retorna o vetor $V$. Este vetor deve ser inicializado com valores zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e2c68-c87e-4342-8ca6-57a43685180b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257304c2b6c04f1ce1743dde7e5095de",
     "grade": false,
     "grade_id": "cell-80a50671b2e72667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_value_iteration(env: gym.Env) -> np.ndarray[float]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62953af-0ac2-4064-9731-be0d11e6285a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a6069926e5998ea4f6aa2a76ff753",
     "grade": false,
     "grade_id": "cell-4e33e273961d7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Agora, vamos gerar uma política determinística a partir de um vetor $V$, conforme definido pela equação $\\pi(s)= \\textrm{argmax}_a \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$. Implemente a função `generate_policy`, que recebe um ambiente e um vetor $V$, retornando a política determinística resultante.\n",
    "\n",
    "**Dica:** Utilize a função `compute_expected_value` do exercício anterior para facilitar sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a710-e02c-46be-a93b-56683cfe6593",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee4fc2ed2d51ff97989aaddc8706c51",
     "grade": false,
     "grade_id": "cell-41eb7e70f58ee606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_policy(env: gym.Env, V: np.ndarray[float], gamma: float) -> np.ndarray[int]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76228-99f7-4eb3-a265-e05e0ade9a3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d08d908f6776a4eecbe68786819e0d9",
     "grade": false,
     "grade_id": "cell-eef5f2b3ef2b657d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Por fim, implemente o loop principal do *Value Iteration* na função `value_iteration`. Ela deverá retornar, nesta ordem, o array de valores $V$ e a política obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4be75-e22e-40f7-aca3-31f7b810c32d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1207e1ee95de47ccab3943f810509cc9",
     "grade": false,
     "grade_id": "cell-c48b9185009da819",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env: gym.Env, gamma:float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f621f3f-9261-4f75-93fa-a511c3013aad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3130d567b2d34a7411e80ff13d04238e",
     "grade": false,
     "grade_id": "cell-3a5e438c6988c441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Value Iteration* em um ambiente Frozen Lake determinístico. A política resultante será armazenada a variável `value_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que ele esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13ad40-daa8-4c50-92c2-92336378d91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a507f4b6da72cd0204ce61ef7d995c7",
     "grade": true,
     "grade_id": "cell-65ff2786217d46c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, value_iteration_deterministic = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, value_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(value_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7814681-8791-4066-9205-6bd78043f7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8477abf2d6223cb005a34bf19346d24f",
     "grade": true,
     "grade_id": "cell-90866ba893778afb",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845b158-b075-4b9e-9da3-cc7cffd82b47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0bf756f1c61d1326fd39f832e2599b0",
     "grade": false,
     "grade_id": "cell-f948772c42437869",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac23f89-42f3-44b3-854b-366657919bfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef88d7295863002c1746176f54048cfb",
     "grade": false,
     "grade_id": "cell-129f7cfccf09f7e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora, executaremos seus algoritmos no mesmo ambiente do Frozen Lake, porém escorregadio. As políticas resultante serão armazenadas nas variáveis `policy_iteration_slippery` e `value_iteration_slippery`, que usaremos na tarefa 14. Nesse cenário, o agente tem apenas 1/3 de chance de se mover na direção desejada e 2/3 de chance de se mover em uma direção perpendicular. Observe as políticas resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952605d-7df8-4583-ae30-d27f611b8565",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee3c7f66748e1f0c7e5aded56c485dd5",
     "grade": false,
     "grade_id": "cell-51aba37175d6166c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, policy_iteration_slippery = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Policy Iteration\")\n",
    "print_policy(env, policy_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e214b7-6f93-4f64-813f-23fac2d91eab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28ac89d9a717a5ad6077b10991b0ea62",
     "grade": false,
     "grade_id": "cell-1cd7b2d2c9f5a32e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, value_iteration_slippery = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Value Iteration\")\n",
    "print_policy(env, value_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3cef6-fa32-44a7-b1a1-b59415304dcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f3c44171f967cf4a67342ee8dd96568",
     "grade": false,
     "grade_id": "cell-9ebda4da8134aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Implemente a função `execute_policy` abaixo, que deve executar uma política previamente obtida em um ambiente Frozen Lake por $N$ episódios, retornando a recompensa acumulada de cada episódio e suas durações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a263709-5c41-420f-a35e-4ce3bd2b0af0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65dd2151b7b6eb59037e7c206aebf2cf",
     "grade": true,
     "grade_id": "cell-8c6445e2c40cad0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def execute_policy(env: gym.Env, policy: np.ndarray[int], n_episodes):\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        episode_lengths.append(step_count)\n",
    "    return episode_returns, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359818ef-5844-4300-8e03-f989005f5251",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f138e90efe57b4f18d45fa1f9c7da8e3",
     "grade": false,
     "grade_id": "cell-774fba8dd36ffbe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Utilize a função `execute_policy` para avaliar a política obtida pelo Policy Iteration no Frozen Lake **determinístico** (`policy_iteration_deterministic`) em um ambiente Frozen Lake escorregadio por 10 episódios. Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_1_returns` e a duração dos episódios na variável `agent_1_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dc536-c94c-43fd-9d87-204ea87b04bb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4ae3ec1ce95e41ffe14db005b9ae544",
     "grade": false,
     "grade_id": "cell-5e1db6859a2651d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b773d-73f5-4a22-bda0-5d21a1f264d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e0754bfa1c08878b4c14904ee53a0f",
     "grade": false,
     "grade_id": "cell-bde46ff3fce95520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Repita o procedimento da tarefa anterior, desta vez utilizando a política obtida pelo Policy Iteration no Frozen Lake **escorregadio** (`policy_iteration_slippery`). Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_2_returns` e a duração dos episódios na variável `agent_2_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020b4b0-47d0-4c60-a4f1-7e6b1411ab47",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c50df759f226ac2558ea4d515f343a7",
     "grade": true,
     "grade_id": "cell-d7202ccd7e779b1a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f24dc-2e61-4c83-ad77-50638b8c0e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba9c4b3ee4aec555f44237ab98d95d25",
     "grade": false,
     "grade_id": "cell-eaad1d7c23d3bf5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analise a seguinte comparação entre as recompensas e a duração obtidas por cada uma dessas duas execuções no Frozen Lake escorregadio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1b38f-a55f-4a6a-b334-31380f871153",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbed36db4b67ef8e511dbcec2a153c7f",
     "grade": false,
     "grade_id": "cell-5eec276dbb402e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def compare_policy(\n",
    "    rewards_run1, lengths_run1,\n",
    "    rewards_run2, lengths_run2,\n",
    "    label_run1=\"Agent 1\", label_run2=\"Agent 2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two policy runs using mean return and mean episode length.\n",
    "\n",
    "    Args:\n",
    "        rewards_run1 (list): Episode rewards for run 1.\n",
    "        lengths_run1 (list): Episode lengths for run 1.\n",
    "        rewards_run2 (list): Episode rewards for run 2.\n",
    "        lengths_run2 (list): Episode lengths for run 2.\n",
    "        label_run1 (str): Label for run 1.\n",
    "        label_run2 (str): Label for run 2.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_rewards = [np.mean(rewards_run1), np.mean(rewards_run2)]\n",
    "    std_rewards  = [np.std(rewards_run1), np.std(rewards_run2)]\n",
    "\n",
    "    mean_lengths = [np.mean(lengths_run1), np.mean(lengths_run2)]\n",
    "    std_lengths  = [np.std(lengths_run1), np.std(lengths_run2)]\n",
    "\n",
    "    labels = [label_run1, label_run2]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Mean Rewards\n",
    "    axes[0].bar(x, mean_rewards, yerr=std_rewards, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_ylabel(\"Mean Total Reward\")\n",
    "    axes[0].set_title(\"Mean Episode Return ± Std\")\n",
    "    axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # Mean Episode Lengths\n",
    "    axes[1].bar(x, mean_lengths, yerr=std_lengths, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_ylabel(\"Mean Episode Length\")\n",
    "    axes[1].set_title(\"Mean Episode Length ± Std\")\n",
    "    axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_policy(agent_1_returns, agent_1_lengths, agent_2_returns, agent_2_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa0ba7-455a-41c4-97d4-8c17fa8b0e1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd865599e6154e8061df6dcbe6aa7eca",
     "grade": false,
     "grade_id": "cell-f8680351f981a5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "15. Explique quais fatores levaram às diferenças observadas entre as políticas obtidas no ambiente determinístico e no ambiente escorregadio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dd49c-eb7c-41a1-a3d1-ca7ef0818922",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4304d231930b7c1e752d1520d20c830",
     "grade": true,
     "grade_id": "cell-21cbc643c62202d7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4571b7-ff1b-4c70-9d85-e0de016a6bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5292df5ef29eee183f943505f3ec2e4",
     "grade": false,
     "grade_id": "cell-426ab2d2c4253fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "16. Quais estratégias poderiam ser adotadas para tornar o comportamento do agente menos conservador quando treinado no ambiente escorregadio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce5371-a76a-415b-b282-4d4ab80ff056",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbcb94e28a49b89a5fb7b66e4c166a94",
     "grade": true,
     "grade_id": "cell-70d62e2dd7e44e0e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
