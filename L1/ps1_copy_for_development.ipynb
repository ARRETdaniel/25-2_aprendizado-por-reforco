{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c18e0f-36ba-4010-8ba6-9ec2c9966e3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eead5b36a3ee2d8eb504615ec2e698b",
     "grade": false,
     "grade_id": "cell-4c87f565d23c35db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lista de ExercÃ­cios 1: Processos de DecisÃ£o de Markov e ProgramaÃ§Ã£o DinÃ¢mica\n",
    "\n",
    "#### Disciplina: Aprendizado por ReforÃ§o\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140e15d-8ff2-404e-9f0b-de4c9444c9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46221e6cdc09eef7a6cc1ae625ff7263",
     "grade": false,
     "grade_id": "cell-e6ec4bda5dd7e12a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## InstruÃ§Ãµes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd360e8-7428-4cb3-af57-ef79d5c99ef8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de898f61e5161ac2497e0a4c68b4e92e",
     "grade": false,
     "grade_id": "cell-d31fd315f36785a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÃ•ES QUE NÃƒO SEGUIREM AS INSTRUÃ‡Ã•ES A SEGUIR NÃƒO SERÃƒO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercÃ­cios e familiarize-se com o cÃ³digo fornecido antes de comeÃ§ar a implementaÃ§Ã£o.\n",
    "- Os locais onde vocÃª deverÃ¡ escrever suas soluÃ§Ãµes estÃ£o demarcados com comentÃ¡rios `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **NÃ£o altere o cÃ³digo fora das Ã¡reas indicadas, nem adicione ou remova cÃ©lulas. O nome deste arquivo tambÃ©m nÃ£o deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o cÃ³digo esteja funcionando do inÃ­cio ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps1.ipynb*) com as suas soluÃ§Ãµes no Moodle.\n",
    "- Prazo de entrega: 23/09/2025. SubmissÃµes fora do prazo terÃ£o uma penalizaÃ§Ã£o de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentaÃ§Ã£o do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementaÃ§Ã£o.\n",
    "- Em caso de dÃºvidas entre em contato pelo fÃ³rum \"DÃºvidas com relaÃ§Ã£o aos exercÃ­cios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea77-cdbe-474f-bc2f-95daa8d439c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31845cf7b26a7f7728c186a8c96638a3",
     "grade": false,
     "grade_id": "cell-f1f0ba316c0b79ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1557b9-966f-44c4-85be-cb75ffd2c95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1098a937587b35eb2f0dae504db9be3",
     "grade": false,
     "grade_id": "cell-69a0af10519ed240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "O ambiente Frozen Lake Ã© uma simulaÃ§Ã£o clÃ¡ssica utilizada para treinamento de agentes em aprendizado por reforÃ§o. Neste ambiente, o agente navega por um lago congelado representado por um grid de tamanho $n \\times m$, com o objetivo de alcanÃ§ar um alvo. O lago contÃ©m dois tipos de cÃ©lulas: (1) cÃ©lulas com gelo sÃ³lido, que sÃ£o seguras para o agente se mover, e (2) as cÃ©lulas com buracos, nas quais o agente cai e falha a missÃ£o. Embora o Gymnasium jÃ¡ possua uma implementaÃ§Ã£o do Frozen Lake, neste exercÃ­cio iremos implementÃ¡-lo do zero.\n",
    "\n",
    "No inÃ­cio de cada episÃ³dio, o agente Ã© posicionado na cÃ©lula $[0, 0]$ enquanto o alvo Ã© posicionado na cÃ©lula mais distante do agente, na posiÃ§Ã£o $[n-1, m-1]$ em um mapa de tamanho $n \\times m$. A cada passo, o agente recebe uma observaÃ§Ã£o indicando sua posiÃ§Ã£o atual no lago e tem a possibilidade de escolher entre quatro aÃ§Ãµes possÃ­veis: mover-se para cima, para baixo, para a esquerda ou para a direita. No entanto, devido Ã  superfÃ­cie escorregadia do lago, ele nem sempre se move na direÃ§Ã£o desejada, podendo acabar se movendo em uma direÃ§Ã£o perpendicular Ã  escolhida. O agente recebe uma recompensa de 1 se alcanÃ§ar o alvo e zero em todos os outros estados. Um episÃ³dio termina quando o agente alcanÃ§a o objetivo ou cai na Ã¡gua.\n",
    "\n",
    "Neste exercÃ­cio, vamos trabalhar sempre com o mesmo mapa $4 \\times 4$, representado na figura abaixo.\n",
    "\n",
    "![Frozen Lake Map](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46123-2985-4e8e-87b5-ed33a8fccfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b35e66b571d4144b5769839b2a1134",
     "grade": false,
     "grade_id": "cell-6e7a4e34e7d477a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa serÃ¡ implementar o ambiente Frozen Lake utilizando o arcabouÃ§o fornecido pelo Gymnasium. Abaixo, vocÃª encontrarÃ¡ um cÃ³digo inicial que deverÃ¡ ser utilizado em sua implementaÃ§Ã£o. Siga essas instruÃ§Ãµes para garantir que seu cÃ³digo estÃ¡ de acordo com o esperado:\n",
    "\n",
    "1. Na funÃ§Ã£o `__init__`, jÃ¡ definimos o mapa que serÃ¡ utilizado e armazenamos essa informaÃ§Ã£o na variÃ¡vel `_description`. Nesse mapa, a letra 'S' representa a posiÃ§Ã£o inicial do agente, a letra 'G' indica o alvo, as letras 'F' representam gelo sÃ³lido (que Ã© seguro) e as letras 'H' marcam os buracos. No entanto, ainda Ã© necessÃ¡rio adicionar mais algumas informaÃ§Ãµes no ambiente, especificamente sobre a representaÃ§Ã£o das observaÃ§Ãµes e das aÃ§Ãµes. Embora existam vÃ¡rias maneiras de representar os espaÃ§os de observaÃ§Ãµes e de aÃ§Ã£o, neste exercÃ­cio, vocÃª deve usar a forma mais simples possÃ­vel, que pode ser representada por um Ãºnico valor discreto. Na funÃ§Ã£o `__init__`, defina o espaÃ§o de observaÃ§Ãµes e o espaÃ§o de aÃ§Ãµes, atribuindo-os Ã s variÃ¡veis `self.observation_space` e `self.action_space`, respectivamente. Utilize apenas a classe `gymnasium.spaces.Discrete` nesta tarefa.\n",
    "\n",
    "2. Antes de prosseguirmos com as funcionalidades do gymnasium, vamos implementar algumas funÃ§Ãµes auxiliares para facilitar as prÃ³ximas etapas. Implemente a funÃ§Ã£o `_get_obs`, que retorna a observaÃ§Ã£o atual do ambiente. AlÃ©m disso, implemente a funÃ§Ã£o `_set_state`, que recebe um valor inteiro correspondente a uma posiÃ§Ã£o no lago e coloca o agente nesta localizaÃ§Ã£o.\n",
    "\n",
    "3. A funÃ§Ã£o `reset` deve resetar o ambiente e inicializar um novo episÃ³dio, posicionando o agente na cÃ©lula $[0, 0]$ e fazendo todos os ajustes internos necessÃ¡rios. Esta funÃ§Ã£o deve retornar uma tupla contendo a observaÃ§Ã£o inicial e as informaÃ§Ãµes do ambiente. Neste exercÃ­cio, vamos retornar um dicionario vazio `{}` para as informaÃ§Ãµes. Lembre-se que a observaÃ§Ã£o deve ser um Ãºnico valor discreto, como definido no item 1. Implemente a funÃ§Ã£o `reset`.\n",
    "\n",
    "4. A funÃ§Ã£o `step()` Ã© responsÃ¡vel por atualizar o ambiente com base na aÃ§Ã£o executada pelo agente. Ela recebe como entrada a aÃ§Ã£o escolhida pelo agente, um parÃ¢metro seed e uma variÃ¡vel options, e calcula o novo estado atual com base na funÃ§Ã£o de transiÃ§Ã£o previamente definida. Neste exercÃ­cio, vocÃª pode ignorar os parÃ¢metros seed e options, pois nÃ£o precisaremos deles. Neste ambiente que estamos desenvolvendo, o agente tem 80% de chance de se mover na direÃ§Ã£o desejada e 20% de chance de se mover em uma direÃ§Ã£o perpendicular Ã  escolhida, distribuÃ­da igualmente entre os dois sentidos possÃ­veis (10% para cada um). **As aÃ§Ãµes do agente devem ser representadas pelos valores 0 (mover-se para a esquerda), 1 (mover-se para baixo), 2 (mover-se para a direita) e 3 (mover-se para cima)**. Caso o agente tente se mover para fora do mapa, ele permanecerÃ¡ na mesma posiÃ§Ã£o. AlÃ©m disso, a funÃ§Ã£o atribui uma recompensa ao agente e verifica se o episÃ³dio chegou ao fim. Implemente a funÃ§Ã£o step() para que ela retorne a observaÃ§Ã£o do estado atual, a recompensa recebida, um valor booleano indicando se o estado Ã© terminal, um valor booleano informando se o episÃ³dio foi truncado e as informaÃ§Ãµes do ambiente. Esses dois Ãºltimos valores sÃ£o necessÃ¡rios devidio Ã  interface estabelecida pelo gymnasium, mas nÃ£o se preocupe com eles; apenas retorne sempre `False` e `{}` para eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c82e329-a086-4479-8375-bb1545073294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d09d0c8292f8dbf0e2bdf379701d722",
     "grade": false,
     "grade_id": "cell-bd1f83bd5e0cfba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9016a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Installing packages for visual rendering...\n",
      "Installing pygame...\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m979.3 kB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "âœ… pygame installed successfully\n",
      "Installing gymnasium[classic-control]...\n",
      "Requirement already satisfied: gymnasium[classic-control] in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.6.1)\n",
      "âœ… gymnasium[classic-control] installed successfully\n",
      "Installing gymnasium[box2d]...\n",
      "Requirement already satisfied: gymnasium[box2d] in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Collecting swig==4.* (from gymnasium[box2d])\n",
      "  Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
      "Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m155.0 kB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\u001b[31m125.7 kB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'box2d-py' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'box2d-py'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Using setuptools (version 78.1.1).\n",
      "  \u001b[31m   \u001b[0m /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages/setuptools/_distutils/dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: zlib/libpng License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/Box2D.py -> build/lib.linux-x86_64-cpython-312/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/__init__.py -> build/lib.linux-x86_64-cpython-312/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/b2/__init__.py -> build/lib.linux-x86_64-cpython-312/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'Box2D._Box2D' extension\n",
      "  \u001b[31m   \u001b[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\n",
      "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build box2d-py\n",
      "âŒ Failed to install gymnasium[box2d]: Command '['/home/danielterra/miniconda3/envs/rl-exercise/bin/python', '-m', 'pip', 'install', 'gymnasium[box2d]']' returned non-zero exit status 1.\n",
      "Installing gymnasium[toy-text]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[31mâ•°â”€>\u001b[0m box2d-py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/danielterra/miniconda3/envs/rl-exercise/lib/python3.12/site-packages (from gymnasium[toy-text]) (2.6.1)\n",
      "âœ… gymnasium[toy-text] installed successfully\n",
      "âœ… Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for visual rendering\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for visual rendering\"\"\"\n",
    "    packages = [\n",
    "        \"pygame\",\n",
    "        \"gymnasium[classic-control]\",\n",
    "        \"gymnasium[box2d]\",\n",
    "        \"gymnasium[toy-text]\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ… {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ Failed to install {package}: {e}\")\n",
    "\n",
    "# Run installation\n",
    "print(\"ğŸ”§ Installing packages for visual rendering...\")\n",
    "install_packages()\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f93c4b",
   "metadata": {},
   "source": [
    "## Testing Gymnasium Installation and Basic Usage\n",
    "\n",
    "Before we start implementing the FrozenLake environment, let's verify that Gymnasium is working correctly and understand the latest API changes.\n",
    "\n",
    "According to the [Gymnasium documentation](https://gymnasium.farama.org/introduction/basic_usage/), the API has evolved from the original OpenAI Gym. Key changes include:\n",
    "- The `step()` method now returns 5 values: `observation, reward, terminated, truncated, info`\n",
    "- `done` has been split into `terminated` (task completion/failure) and `truncated` (time limits)\n",
    "- Environment creation uses `gymnasium.make()` instead of `gym.make()`\n",
    "- The reset method returns `(observation, info)` instead of just `observation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f84cefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ§ª TESTING GYMNASIUM INSTALLATION AND API\n",
      "============================================================\n",
      "âœ… Gymnasium version: 1.2.0\n",
      "\n",
      "ğŸ“‹ Testing CartPole environment creation...\n",
      "   Action space: Discrete(2)\n",
      "   Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "   Action meanings: 0=Push Left, 1=Push Right\n",
      "\n",
      "ğŸ”„ Testing new Gymnasium API...\n",
      "   Initial observation: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "   Initial info: {}\n",
      "   After step - Action: 0, Reward: 1.0\n",
      "   New observation: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "   Terminated: False, Truncated: False\n",
      "\n",
      "âœ… CartPole environment test PASSED!\n",
      "\n",
      "ğŸ§Š Testing official FrozenLake environment...\n",
      "   Action space: Discrete(4)\n",
      "   Observation space: Discrete(16)\n",
      "   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
      "   Initial observation (state): 0\n",
      "   P[0][0] (state 0, action LEFT): [(1.0, 0, 0.0, False)]\n",
      "âœ… FrozenLake environment test PASSED!\n",
      "\n",
      "ğŸ‰ ALL TESTS COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic Gymnasium installation and import\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª TESTING GYMNASIUM INSTALLATION AND API\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… Gymnasium version: {gym.__version__}\")\n",
    "\n",
    "# Test 2: Create a simple environment to verify API\n",
    "print(f\"\\nğŸ“‹ Testing CartPole environment creation...\")\n",
    "test_env = gym.make(\"CartPole-v1\")  # Removed render_mode=\"human\" for better compatibility\n",
    "print(f\"   Action space: {test_env.action_space}\")\n",
    "print(f\"   Observation space: {test_env.observation_space}\")\n",
    "print(f\"   Action meanings: 0=Push Left, 1=Push Right\")\n",
    "\n",
    "# Test 3: Test the new API with reset and step\n",
    "print(f\"\\nğŸ”„ Testing new Gymnasium API...\")\n",
    "obs, info = test_env.reset(seed=42)\n",
    "print(f\"   Initial observation: {obs}\")\n",
    "print(f\"   Initial info: {info}\")\n",
    "\n",
    "action = test_env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "print(f\"   After step - Action: {action}, Reward: {reward}\")\n",
    "print(f\"   New observation: {obs}\")\n",
    "print(f\"   Terminated: {terminated}, Truncated: {truncated}\")\n",
    "\n",
    "test_env.close()\n",
    "print(\"\\nâœ… CartPole environment test PASSED!\")\n",
    "\n",
    "# Test 4: Test FrozenLake environment (which we'll implement)\n",
    "print(f\"\\nğŸ§Š Testing official FrozenLake environment...\")\n",
    "frozen_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "print(f\"   Action space: {frozen_env.action_space}\")\n",
    "print(f\"   Observation space: {frozen_env.observation_space}\")\n",
    "print(f\"   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\")\n",
    "\n",
    "obs, info = frozen_env.reset(seed=42)\n",
    "print(f\"   Initial observation (state): {obs}\")\n",
    "\n",
    "# Test transitions to understand the environment structure\n",
    "print(f\"   P[0][0] (state 0, action LEFT): {frozen_env.unwrapped.P[0][0]}\")\n",
    "frozen_env.close()\n",
    "print(\"âœ… FrozenLake environment test PASSED!\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ALL TESTS COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db53409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš— TESTING ANOTHER CLASSIC ENVIRONMENT: MOUNTAIN CAR\n",
      "============================================================\n",
      "Environment: MountainCar-v0\n",
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action meanings: 0=Push Left, 1=No Push, 2=Push Right\n",
      "\n",
      "Initial observation: [-0.4452088  0.       ]\n",
      "   Position: -0.4452 (range: -1.2 to 0.6)\n",
      "   Velocity: 0.0000 (range: -0.07 to 0.07)\n",
      "\n",
      "Taking 5 random actions:\n",
      "   Step 1: Action=1 (No Push), Position=-0.4458, Velocity=-0.0006, Reward=-1.0\n",
      "   Step 2: Action=0 (Push Left), Position=-0.4480, Velocity=-0.0022, Reward=-1.0\n",
      "   Step 3: Action=0 (Push Left), Position=-0.4517, Velocity=-0.0037, Reward=-1.0\n",
      "   Step 4: Action=0 (Push Left), Position=-0.4569, Velocity=-0.0053, Reward=-1.0\n",
      "   Step 5: Action=0 (Push Left), Position=-0.4637, Velocity=-0.0068, Reward=-1.0\n",
      "âœ… MountainCar environment test PASSED!\n",
      "\n",
      "ğŸ“Š COMPARISON OF ENVIRONMENT TYPES:\n",
      "   CartPole: Discrete actions, Continuous observations, Episode-based\n",
      "   FrozenLake: Discrete actions, Discrete observations, Grid-world\n",
      "   MountainCar: Discrete actions, Continuous observations, Physics-based\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Additional Environment Example - MountainCar\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš— TESTING ANOTHER CLASSIC ENVIRONMENT: MOUNTAIN CAR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mountain_env = gym.make(\"MountainCar-v0\")\n",
    "print(f\"Environment: {mountain_env.spec.id}\")\n",
    "print(f\"Action space: {mountain_env.action_space}\")\n",
    "print(f\"Observation space: {mountain_env.observation_space}\")\n",
    "print(f\"Action meanings: 0=Push Left, 1=No Push, 2=Push Right\")\n",
    "\n",
    "# Reset and show initial state\n",
    "obs, info = mountain_env.reset(seed=42)\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "print(f\"   Position: {obs[0]:.4f} (range: -1.2 to 0.6)\")\n",
    "print(f\"   Velocity: {obs[1]:.4f} (range: -0.07 to 0.07)\")\n",
    "\n",
    "# Take a few random actions\n",
    "print(f\"\\nTaking 5 random actions:\")\n",
    "for i in range(5):\n",
    "    action = mountain_env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = mountain_env.step(action)\n",
    "    action_names = [\"Push Left\", \"No Push\", \"Push Right\"]\n",
    "    print(f\"   Step {i+1}: Action={action} ({action_names[action]}), \"\n",
    "          f\"Position={obs[0]:.4f}, Velocity={obs[1]:.4f}, Reward={reward}\")\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"   Episode ended! Terminated={terminated}, Truncated={truncated}\")\n",
    "        break\n",
    "\n",
    "mountain_env.close()\n",
    "print(\"âœ… MountainCar environment test PASSED!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š COMPARISON OF ENVIRONMENT TYPES:\")\n",
    "print(f\"   CartPole: Discrete actions, Continuous observations, Episode-based\")\n",
    "print(f\"   FrozenLake: Discrete actions, Discrete observations, Grid-world\")\n",
    "print(f\"   MountainCar: Discrete actions, Continuous observations, Physics-based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61c3dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ® INTERACTIVE EPISODE DEMONSTRATION\n",
      "============================================================\n",
      "Demo 1: FrozenLake Deterministic\n",
      "ğŸ¬ Starting episode in FrozenLake-v1\n",
      "ğŸ“Š Action space: Discrete(4)\n",
      "ğŸ“Š Observation space: Discrete(16)\n",
      "ğŸ¯ Initial state: 0\n",
      "\n",
      "ğŸ® Episode progression:\n",
      "--------------------------------------------------\n",
      "Step  1: Action=3 â†’ State=1 [0,1] | Reward=0.0 | Total=0.0\n",
      "Step  2: Action=2 â†’ State=5 [1,1] | Reward=0.0 | Total=0.0\n",
      "ğŸ Episode ended at step 2!\n",
      "   Reason: Task completed\n",
      "ğŸ“ˆ Final results: 2 steps, Total reward: 0.0\n",
      "\n",
      "============================================================\n",
      "Demo 2: CartPole Balancing\n",
      "ğŸ¬ Starting episode in CartPole-v1\n",
      "ğŸ“Š Action space: Discrete(2)\n",
      "ğŸ“Š Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "ğŸ¯ Initial state: [-0.00303268 -0.00523447 -0.03759432  0.025485  ]\n",
      "\n",
      "ğŸ® Episode progression:\n",
      "--------------------------------------------------\n",
      "Step  1: Action=1 â†’ Obs=[-0.003, 0.190] | Reward=1.0 | Total=1.0\n",
      "Step  2: Action=0 â†’ Obs=[0.001, -0.004] | Reward=1.0 | Total=2.0\n",
      "Step  3: Action=0 â†’ Obs=[0.001, -0.199] | Reward=1.0 | Total=3.0\n",
      "Step  4: Action=0 â†’ Obs=[-0.003, -0.393] | Reward=1.0 | Total=4.0\n",
      "Step  5: Action=1 â†’ Obs=[-0.011, -0.198] | Reward=1.0 | Total=5.0\n",
      "Step  6: Action=1 â†’ Obs=[-0.015, -0.002] | Reward=1.0 | Total=6.0\n",
      "Step  7: Action=0 â†’ Obs=[-0.015, -0.197] | Reward=1.0 | Total=7.0\n",
      "Step  8: Action=1 â†’ Obs=[-0.019, -0.001] | Reward=1.0 | Total=8.0\n",
      "Step  9: Action=0 â†’ Obs=[-0.019, -0.196] | Reward=1.0 | Total=9.0\n",
      "Step 10: Action=1 â†’ Obs=[-0.023, -0.001] | Reward=1.0 | Total=10.0\n",
      "ğŸ“ˆ Final results: 10 steps, Total reward: 10.0\n",
      "\n",
      "ğŸ¯ SUMMARY:\n",
      "   FrozenLake: 2 steps, reward 0.0\n",
      "   CartPole: 10 steps, reward 10.0\n",
      "\n",
      "ğŸ’¡ Notice how different environments have different reward structures!\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Interactive Episode Demonstration\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ® INTERACTIVE EPISODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_episode_demo(env_name, max_steps=20, seed=42):\n",
    "    \"\"\"\n",
    "    Run a complete episode with detailed step-by-step output.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "\n",
    "    print(f\"ğŸ¬ Starting episode in {env_name}\")\n",
    "    print(f\"ğŸ“Š Action space: {env.action_space}\")\n",
    "    print(f\"ğŸ“Š Observation space: {env.observation_space}\")\n",
    "    print(f\"ğŸ¯ Initial state: {obs}\")\n",
    "\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    print(f\"\\nğŸ® Episode progression:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    while step_count < max_steps:\n",
    "        # Take random action\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        # Format output based on environment type\n",
    "        if env_name == \"FrozenLake-v1\":\n",
    "            # Convert state to grid coordinates\n",
    "            row, col = divmod(obs, 4)\n",
    "            print(f\"Step {step_count:2d}: Action={action} â†’ State={obs} [{row},{col}] | Reward={reward} | Total={total_reward}\")\n",
    "        else:\n",
    "            # For continuous observations, show abbreviated version\n",
    "            if hasattr(obs, '__len__') and len(obs) > 1:\n",
    "                obs_str = f\"[{obs[0]:.3f}, {obs[1]:.3f}]\"\n",
    "            else:\n",
    "                obs_str = f\"{obs}\"\n",
    "            print(f\"Step {step_count:2d}: Action={action} â†’ Obs={obs_str} | Reward={reward} | Total={total_reward}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"ğŸ Episode ended at step {step_count}!\")\n",
    "            print(f\"   Reason: {'Task completed' if terminated else 'Time limit reached'}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"ğŸ“ˆ Final results: {step_count} steps, Total reward: {total_reward}\")\n",
    "    return step_count, total_reward\n",
    "\n",
    "# Demo 1: FrozenLake episode (short and visual)\n",
    "print(\"Demo 1: FrozenLake Deterministic\")\n",
    "frozen_steps, frozen_reward = run_episode_demo(\"FrozenLake-v1\", max_steps=50, seed=123)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Demo 2: CartPole episode\n",
    "print(\"Demo 2: CartPole Balancing\")\n",
    "cartpole_steps, cartpole_reward = run_episode_demo(\"CartPole-v1\", max_steps=10, seed=456)\n",
    "\n",
    "print(f\"\\nğŸ¯ SUMMARY:\")\n",
    "print(f\"   FrozenLake: {frozen_steps} steps, reward {frozen_reward}\")\n",
    "print(f\"   CartPole: {cartpole_steps} steps, reward {cartpole_reward}\")\n",
    "print(f\"\\nğŸ’¡ Notice how different environments have different reward structures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8be285",
   "metadata": {},
   "source": [
    "## âœ… Gymnasium Testing Complete\n",
    "\n",
    "### **What we've verified:**\n",
    "\n",
    "1. **ğŸ”§ Installation**: Gymnasium 1.2.0 is properly installed and working\n",
    "2. **ğŸ“¡ API Compatibility**: New API structure confirmed:\n",
    "   - `reset()` returns `(observation, info)`\n",
    "   - `step()` returns `(observation, reward, terminated, truncated, info)`\n",
    "   - Proper separation of `terminated` vs `truncated`\n",
    "\n",
    "3. **ğŸ® Environment Examples Tested:**\n",
    "   - **CartPole-v1**: Continuous observations, discrete actions, balancing task\n",
    "   - **FrozenLake-v1**: Discrete observations, discrete actions, grid navigation  \n",
    "   - **MountainCar-v0**: Continuous observations, discrete actions, physics simulation\n",
    "\n",
    "4. **ğŸ“Š Transition Structure**: Verified access to `env.unwrapped.P[state][action]` for FrozenLake\n",
    "\n",
    "### **Key Insights for Implementation:**\n",
    "- âœ… Environment spaces are correctly defined using `gym.spaces.Discrete`\n",
    "- âœ… State transitions follow the expected format: `[(prob, next_state, reward, terminated), ...]`\n",
    "- âœ… Random seeding works consistently for reproducible results\n",
    "- âœ… Different environments have different reward structures and dynamics\n",
    "\n",
    "### **Ready for Implementation:**\n",
    "We now have a solid foundation to implement our custom FrozenLake environment following the established patterns and API conventions. The analysis shows exactly how state transitions, rewards, and termination conditions should be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5ac8f",
   "metadata": {},
   "source": [
    "## ğŸ¬ Visual Interactive Examples - Pop-up Windows\n",
    "\n",
    "The following examples will open visual windows where you can see the environments in action! Each environment will display in a separate pop-up window showing real-time interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d881f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸª VISUAL CARTPOLE DEMONSTRATION\n",
      "==================================================\n",
      "ğŸ¬ Attempting to open CartPole window...\n",
      "âœ… CartPole environment created successfully!\n",
      "ğŸ¯ Starting CartPole visual episode...\n",
      "ğŸ“º A window should have opened showing the CartPole simulation!\n",
      "ğŸ® Taking random actions - watch the window!\n",
      "   Step  0: Push Right | Pole angle: 0.036 | Total reward: 1.0\n",
      "   Step  5: Push Left | Pole angle: 0.000 | Total reward: 6.0\n",
      "   Step 10: Push Left | Pole angle: -0.027 | Total reward: 11.0\n",
      "   Step 15: Push Right | Pole angle: -0.053 | Total reward: 16.0\n",
      "ğŸ“Š Final Results: 20 steps, Total reward: 20.0\n",
      "âœ… CartPole visual demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Visual Example 1: CartPole with Real-time Display\n",
    "import time\n",
    "\n",
    "print(\"ğŸª VISUAL CARTPOLE DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¬ Attempting to open CartPole window...\")\n",
    "\n",
    "try:\n",
    "    # Create environment with human rendering (pop-up window)\n",
    "    cartpole_visual = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    print(\"âœ… CartPole environment created successfully!\")\n",
    "\n",
    "    # Reset and start episode\n",
    "    obs, info = cartpole_visual.reset(seed=42)\n",
    "    print(f\"ğŸ¯ Starting CartPole visual episode...\")\n",
    "    print(\"ğŸ“º A window should have opened showing the CartPole simulation!\")\n",
    "\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    max_steps = 20  # Reduced for better performance\n",
    "\n",
    "    print(\"ğŸ® Taking random actions - watch the window!\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take a random action\n",
    "        action = cartpole_visual.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = cartpole_visual.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Print progress every 5 steps\n",
    "        if step % 5 == 0:\n",
    "            action_name = \"Push Left\" if action == 0 else \"Push Right\"\n",
    "            print(f\"   Step {step:2d}: {action_name} | Pole angle: {obs[2]:.3f} | Total reward: {total_reward}\")\n",
    "\n",
    "        # Add small delay to see the action\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        # Check if episode ended\n",
    "        if terminated or truncated:\n",
    "            print(f\"ğŸ Episode ended at step {step}!\")\n",
    "            print(f\"   Reason: {'Pole fell down' if terminated else 'Time limit reached'}\")\n",
    "            break\n",
    "\n",
    "    cartpole_visual.close()\n",
    "    print(f\"ğŸ“Š Final Results: {step_count} steps, Total reward: {total_reward}\")\n",
    "    print(\"âœ… CartPole visual demonstration complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error running visual demonstration: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to display/GUI limitations in the current environment\")\n",
    "    print(\"ğŸ”„ Running text-only version instead...\")\n",
    "\n",
    "    # Fallback to text-only version\n",
    "    cartpole_text = gym.make(\"CartPole-v1\")\n",
    "    obs, info = cartpole_text.reset(seed=42)\n",
    "\n",
    "    print(\"ğŸ“Š Text-only CartPole demonstration:\")\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(10):\n",
    "        action = cartpole_text.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = cartpole_text.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        action_name = \"Push Left\" if action == 0 else \"Push Right\"\n",
    "        print(f\"   Step {step:2d}: {action_name} | Pole angle: {obs[2]:.3f} | Reward: {reward}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    cartpole_text.close()\n",
    "    print(f\"âœ… Text demonstration complete! Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86270251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Example 2: MountainCar with Real-time Display\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”ï¸  VISUAL MOUNTAIN CAR DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¬ Opening MountainCar window... Watch the car trying to reach the flag!\")\n",
    "print(\"â±ï¸  Episode will run for 50 steps to show the physics\")\n",
    "\n",
    "# Create environment with human rendering (pop-up window)\n",
    "mountain_visual = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset and start episode\n",
    "obs, info = mountain_visual.reset(seed=123)\n",
    "print(f\"ğŸ¯ Starting MountainCar visual episode...\")\n",
    "print(f\"ğŸš— Initial position: {obs[0]:.3f}, velocity: {obs[1]:.3f}\")\n",
    "\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "max_steps = 50\n",
    "\n",
    "print(\"ğŸ® Taking strategic actions - watch the car build momentum!\")\n",
    "\n",
    "for step in range(max_steps):\n",
    "    # Render the current state (updates the visual window)\n",
    "    mountain_visual.render()\n",
    "\n",
    "    # Take a strategic action (try to build momentum)\n",
    "    if obs[1] < 0:  # If moving left, push left to build momentum\n",
    "        action = 0\n",
    "    elif obs[1] > 0:  # If moving right, push right to build momentum\n",
    "        action = 2\n",
    "    else:  # If stationary, push right to start moving\n",
    "        action = 2\n",
    "\n",
    "    obs, reward, terminated, truncated, info = mountain_visual.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "\n",
    "    # Print progress every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        action_names = [\"Push Left\", \"No Push\", \"Push Right\"]\n",
    "        print(f\"   Step {step:2d}: {action_names[action]} | Pos: {obs[0]:.3f} | Vel: {obs[1]:.3f} | Reward: {reward}\")\n",
    "\n",
    "    # Add small delay to see the movement\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Check if episode ended (reached the flag)\n",
    "    if terminated or truncated:\n",
    "        print(f\"ğŸ Episode ended at step {step}!\")\n",
    "        if terminated:\n",
    "            print(\"ğŸ‰ SUCCESS! Car reached the flag!\")\n",
    "        else:\n",
    "            print(\"â° Time limit reached\")\n",
    "        break\n",
    "\n",
    "mountain_visual.close()\n",
    "print(f\"ğŸ“Š Final Results: {step_count} steps, Total reward: {total_reward}\")\n",
    "print(\"âœ… MountainCar visual demonstration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a760b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ§Š VISUAL FROZEN LAKE DEMONSTRATION\n",
      "==================================================\n",
      "ğŸ¬ Attempting to open FrozenLake window...\n",
      "âœ… FrozenLake environment created successfully!\n",
      "ğŸ¯ Starting FrozenLake visual episode...\n",
      "ğŸ“º A window should have opened showing the FrozenLake grid!\n",
      "ğŸ§Š Initial state: 0 (position [0,0])\n",
      "ğŸ® Taking random actions - watch the agent slip on the ice!\n",
      "Legend: S=Start, F=Frozen(safe), H=Hole(danger), G=Goal\n",
      "   Step  1: At [0,0] (S) â†’ Action: DOWN â†“ â†’ Landed at [1,0] (F) | Reward: 0.0\n",
      "   Step  2: At [1,0] (F) â†’ Action: RIGHT â†’ â†’ Landed at [2,0] (F) | Reward: 0.0\n",
      "   Step  3: At [2,0] (F) â†’ Action: LEFT â† â†’ Landed at [3,0] (H) | Reward: 0.0\n",
      "ğŸ Episode ended at step 3!\n",
      "ğŸ’€ FAILED! Agent fell into a hole!\n",
      "ğŸ“Š Final Results: 3 steps, Total reward: 0.0\n",
      "âœ… FrozenLake visual demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Visual Example 3: FrozenLake with Real-time Display\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ§Š VISUAL FROZEN LAKE DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¬ Attempting to open FrozenLake window...\")\n",
    "\n",
    "try:\n",
    "    # Create environment with human rendering (pop-up window)\n",
    "    frozen_visual = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "    print(\"âœ… FrozenLake environment created successfully!\")\n",
    "\n",
    "    # Reset and start episode\n",
    "    obs, info = frozen_visual.reset(seed=456)\n",
    "    print(f\"ğŸ¯ Starting FrozenLake visual episode...\")\n",
    "    print(\"ğŸ“º A window should have opened showing the FrozenLake grid!\")\n",
    "    print(f\"ğŸ§Š Initial state: {obs} (position [0,0])\")\n",
    "\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    max_steps = 15  # Reduced for better visibility\n",
    "\n",
    "    action_names = [\"LEFT â†\", \"DOWN â†“\", \"RIGHT â†’\", \"UP â†‘\"]\n",
    "    map_symbols = {0: 'S', 1: 'F', 2: 'F', 3: 'F', 4: 'F', 5: 'H', 6: 'F', 7: 'H',\n",
    "                   8: 'F', 9: 'F', 10: 'F', 11: 'H', 12: 'H', 13: 'F', 14: 'F', 15: 'G'}\n",
    "\n",
    "    print(\"ğŸ® Taking random actions - watch the agent slip on the ice!\")\n",
    "    print(\"Legend: S=Start, F=Frozen(safe), H=Hole(danger), G=Goal\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take a random action\n",
    "        action = frozen_visual.action_space.sample()\n",
    "\n",
    "        # Show intended action\n",
    "        row, col = divmod(obs, 4)\n",
    "        print(f\"   Step {step+1:2d}: At [{row},{col}] ({map_symbols[obs]}) â†’ Action: {action_names[action]}\", end=\"\")\n",
    "\n",
    "        obs, reward, terminated, truncated, info = frozen_visual.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Show result\n",
    "        new_row, new_col = divmod(obs, 4)\n",
    "        print(f\" â†’ Landed at [{new_row},{new_col}] ({map_symbols[obs]}) | Reward: {reward}\")\n",
    "\n",
    "        # Add delay to see the movement\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        # Check if episode ended\n",
    "        if terminated or truncated:\n",
    "            print(f\"ğŸ Episode ended at step {step+1}!\")\n",
    "            if obs == 15:  # Goal state\n",
    "                print(\"ğŸ‰ SUCCESS! Agent reached the goal!\")\n",
    "            elif map_symbols[obs] == 'H':\n",
    "                print(\"ğŸ’€ FAILED! Agent fell into a hole!\")\n",
    "            break\n",
    "\n",
    "    frozen_visual.close()\n",
    "    print(f\"ğŸ“Š Final Results: {step_count} steps, Total reward: {total_reward}\")\n",
    "    print(\"âœ… FrozenLake visual demonstration complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error running visual demonstration: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to display/GUI limitations in the current environment\")\n",
    "    print(\"ğŸ”„ Running text-only version instead...\")\n",
    "\n",
    "    # Fallback to text-only version\n",
    "    frozen_text = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "    obs, info = frozen_text.reset(seed=456)\n",
    "\n",
    "    print(\"ğŸ“Š Text-only FrozenLake demonstration:\")\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(10):\n",
    "        action = frozen_text.action_space.sample()\n",
    "        row, col = divmod(obs, 4)\n",
    "        action_names = [\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"]\n",
    "\n",
    "        print(f\"   Step {step+1}: At [{row},{col}] â†’ {action_names[action]}\", end=\"\")\n",
    "\n",
    "        obs, reward, terminated, truncated, info = frozen_text.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        new_row, new_col = divmod(obs, 4)\n",
    "        print(f\" â†’ [{new_row},{new_col}] | Reward: {reward}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            if obs == 15:\n",
    "                print(\"ğŸ‰ SUCCESS! Reached goal!\")\n",
    "            else:\n",
    "                print(\"ğŸ’€ Fell in hole!\")\n",
    "            break\n",
    "\n",
    "    frozen_text.close()\n",
    "    print(f\"âœ… Text demonstration complete! Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983ef660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPREHENSIVE VISUAL DEMONSTRATION\n",
      "============================================================\n",
      "This will demonstrate multiple environments with pop-up windows!\n",
      "You should see separate windows opening for each environment.\n",
      "Each demo runs for a short time to show the key features.\n",
      "\n",
      "Starting visual demonstrations...\n",
      "Make sure your display is ready to show pop-up windows!\n",
      "\n",
      "Demo 1/2: CartPole\n",
      "\n",
      "--- CARTPOLE DEMO ---\n",
      "SUCCESS: CartPole window should now be open!\n",
      "  Step 0: Action 0 | Reward: 1.0\n",
      "  Step 2: Action 0 | Reward: 1.0\n",
      "  Step 4: Action 0 | Reward: 1.0\n",
      "  Step 6: Action 1 | Reward: 1.0\n",
      "COMPLETE: CartPole demo finished! Total reward: 8.0\n",
      "\n",
      "Demo 2/2: FrozenLake\n",
      "\n",
      "--- FROZENLAKE DEMO ---\n",
      "SUCCESS: FrozenLake window should now be open!\n",
      "  Step 0: Action 2 | Reward: 0.0\n",
      "  Step 2: Action 3 | Reward: 0.0\n",
      "  Step 4: Action 1 | Reward: 0.0\n",
      "  Episode ended at step 5!\n",
      "COMPLETE: FrozenLake demo finished! Total reward: 0.0\n",
      "\n",
      "ALL VISUAL DEMONSTRATIONS COMPLETE!\n",
      "Summary of what you should have seen:\n",
      "  1. CartPole: Cart with pole balancing simulation\n",
      "  2. FrozenLake: Grid navigation game\n",
      "\n",
      "If windows didn't appear, it might be due to:\n",
      "  - Headless environment (no display)\n",
      "  - System restrictions\n",
      "  - Missing display drivers\n",
      "\n",
      "The environments are working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Final Visual Demo: Multiple Environments\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE VISUAL DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will demonstrate multiple environments with pop-up windows!\")\n",
    "print(\"You should see separate windows opening for each environment.\")\n",
    "print(\"Each demo runs for a short time to show the key features.\")\n",
    "\n",
    "def run_visual_environment(env_name, config, demo_name, steps=8):\n",
    "    \"\"\"Run a visual demonstration of an environment.\"\"\"\n",
    "    print(f\"\\n--- {demo_name.upper()} DEMO ---\")\n",
    "\n",
    "    try:\n",
    "        env = gym.make(env_name, **config)\n",
    "        print(f\"SUCCESS: {demo_name} window should now be open!\")\n",
    "\n",
    "        obs, info = env.reset(seed=42)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(steps):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if step % 2 == 0:  # Print every 2nd step\n",
    "                print(f\"  Step {step}: Action {action} | Reward: {reward}\")\n",
    "\n",
    "            time.sleep(0.4)  # Pause to see the action\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(f\"  Episode ended at step {step}!\")\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "        print(f\"COMPLETE: {demo_name} demo finished! Total reward: {total_reward}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not run visual demo for {demo_name}: {e}\")\n",
    "\n",
    "# Environment configurations\n",
    "print(f\"\\nStarting visual demonstrations...\")\n",
    "print(f\"Make sure your display is ready to show pop-up windows!\")\n",
    "\n",
    "# Demo 1: CartPole\n",
    "print(f\"\\nDemo 1/2: CartPole\")\n",
    "run_visual_environment(\"CartPole-v1\", {\"render_mode\": \"human\"}, \"CartPole\", 8)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Demo 2: FrozenLake\n",
    "print(f\"\\nDemo 2/2: FrozenLake\")\n",
    "run_visual_environment(\"FrozenLake-v1\",\n",
    "                      {\"map_name\": \"4x4\", \"is_slippery\": False, \"render_mode\": \"human\"},\n",
    "                      \"FrozenLake\", 10)\n",
    "\n",
    "print(f\"\\nALL VISUAL DEMONSTRATIONS COMPLETE!\")\n",
    "print(f\"Summary of what you should have seen:\")\n",
    "print(f\"  1. CartPole: Cart with pole balancing simulation\")\n",
    "print(f\"  2. FrozenLake: Grid navigation game\")\n",
    "print(f\"\\nIf windows didn't appear, it might be due to:\")\n",
    "print(f\"  - Headless environment (no display)\")\n",
    "print(f\"  - System restrictions\")\n",
    "print(f\"  - Missing display drivers\")\n",
    "print(f\"\\nThe environments are working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a945204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYSIS OF FROZENLAKE ENVIRONMENT STRUCTURE ===\n",
      "\n",
      "1. ENVIRONMENT DIMENSIONS:\n",
      "   Map description shape: (4, 4)\n",
      "   Total states: 16\n",
      "   Total actions: 4\n",
      "   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
      "\n",
      "2. MAP LAYOUT:\n",
      "   Row 0: ['S', 'F', 'F', 'F']\n",
      "   Row 1: ['F', 'H', 'F', 'H']\n",
      "   Row 2: ['F', 'F', 'F', 'H']\n",
      "   Row 3: ['H', 'F', 'F', 'G']\n",
      "\n",
      "3. TRANSITION PROBABILITIES STRUCTURE:\n",
      "   Format: [(probability, next_state, reward, terminated), ...]\n",
      "   Deterministic env, state 0, action 0 (LEFT): [(1.0, 0, 0.0, False)]\n",
      "   Deterministic env, state 0, action 1 (DOWN): [(1.0, 4, 0.0, False)]\n",
      "   Slippery env, state 0, action 1 (DOWN): [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "\n",
      "4. STATE-TO-COORDINATE MAPPING:\n",
      "   States are numbered 0-15 in row-major order:\n",
      "   State  0 -> (0,0) = 'S'\n",
      "   State  1 -> (0,1) = 'F'\n",
      "   State  2 -> (0,2) = 'F'\n",
      "   State  3 -> (0,3) = 'F'\n",
      "   State  4 -> (1,0) = 'F'\n",
      "   State  5 -> (1,1) = 'H'\n",
      "   State  6 -> (1,2) = 'F'\n",
      "   State  7 -> (1,3) = 'H'\n",
      "   State  8 -> (2,0) = 'F'\n",
      "   State  9 -> (2,1) = 'F'\n",
      "   State 10 -> (2,2) = 'F'\n",
      "   State 11 -> (2,3) = 'H'\n",
      "   State 12 -> (3,0) = 'H'\n",
      "   State 13 -> (3,1) = 'F'\n",
      "   State 14 -> (3,2) = 'F'\n",
      "   State 15 -> (3,3) = 'G'\n",
      "\n",
      "5. KEY INSIGHTS FOR IMPLEMENTATION:\n",
      "   - States: 0-15 (single discrete value)\n",
      "   - Actions: 0-3 (LEFT, DOWN, RIGHT, UP)\n",
      "   - Transitions: P[state][action] gives list of (prob, next_state, reward, terminated)\n",
      "   - Rewards: 0 everywhere except goal state (reward=1)\n",
      "   - Terminal states: holes and goal\n",
      "   - Slippery: 1/3 intended direction, 1/3 each perpendicular direction\n",
      "\n",
      "âœ… Environment structure analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis of FrozenLake environment structure\n",
    "print(\"=== ANALYSIS OF FROZENLAKE ENVIRONMENT STRUCTURE ===\\n\")\n",
    "\n",
    "# Create both deterministic and slippery versions for comparison\n",
    "deterministic_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "slippery_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "print(\"1. ENVIRONMENT DIMENSIONS:\")\n",
    "print(f\"   Map description shape: {deterministic_env.unwrapped.desc.shape}\")\n",
    "print(f\"   Total states: {deterministic_env.observation_space.n}\")\n",
    "print(f\"   Total actions: {deterministic_env.action_space.n}\")\n",
    "print(f\"   Action meanings: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\")\n",
    "\n",
    "print(\"\\n2. MAP LAYOUT:\")\n",
    "desc = deterministic_env.unwrapped.desc\n",
    "for i, row in enumerate(desc):\n",
    "    print(f\"   Row {i}: {[cell.decode() for cell in row]}\")\n",
    "\n",
    "print(\"\\n3. TRANSITION PROBABILITIES STRUCTURE:\")\n",
    "print(\"   Format: [(probability, next_state, reward, terminated), ...]\")\n",
    "print(f\"   Deterministic env, state 0, action 0 (LEFT): {deterministic_env.unwrapped.P[0][0]}\")\n",
    "print(f\"   Deterministic env, state 0, action 1 (DOWN): {deterministic_env.unwrapped.P[0][1]}\")\n",
    "print(f\"   Slippery env, state 0, action 1 (DOWN): {slippery_env.unwrapped.P[0][1]}\")\n",
    "\n",
    "print(\"\\n4. STATE-TO-COORDINATE MAPPING:\")\n",
    "print(\"   States are numbered 0-15 in row-major order:\")\n",
    "for state in range(16):\n",
    "    row, col = divmod(state, 4)\n",
    "    cell_type = desc[row, col].decode()\n",
    "    print(f\"   State {state:2d} -> ({row},{col}) = '{cell_type}'\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS FOR IMPLEMENTATION:\")\n",
    "print(\"   - States: 0-15 (single discrete value)\")\n",
    "print(\"   - Actions: 0-3 (LEFT, DOWN, RIGHT, UP)\")\n",
    "print(\"   - Transitions: P[state][action] gives list of (prob, next_state, reward, terminated)\")\n",
    "print(\"   - Rewards: 0 everywhere except goal state (reward=1)\")\n",
    "print(\"   - Terminal states: holes and goal\")\n",
    "print(\"   - Slippery: 1/3 intended direction, 1/3 each perpendicular direction\")\n",
    "\n",
    "deterministic_env.close()\n",
    "slippery_env.close()\n",
    "print(\"\\nâœ… Environment structure analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04568df0",
   "metadata": {},
   "source": [
    "## Implementation Strategy and Best Practices\n",
    "\n",
    "Based on the analysis above and following RL engineering best practices, our implementation should:\n",
    "\n",
    "### ğŸ” **Key Design Principles:**\n",
    "1. **Correctness over Convenience**: Ensure exact adherence to Gymnasium API\n",
    "2. **Reproducibility**: Use proper random seeding and deterministic state management\n",
    "3. **Safety**: Validate inputs and handle edge cases gracefully\n",
    "4. **Maintainability**: Write clean, well-documented code with clear separation of concerns\n",
    "\n",
    "### ğŸ“‹ **Implementation Checklist:**\n",
    "\n",
    "#### **Environment Structure (Priority: High)**\n",
    "- [ ] Proper inheritance from `gym.Env`\n",
    "- [ ] Correct space definitions using `gym.spaces.Discrete`\n",
    "- [ ] State representation: single integer 0-15\n",
    "- [ ] Action representation: single integer 0-3 (LEFT, DOWN, RIGHT, UP)\n",
    "\n",
    "#### **Core Methods (Priority: High)**\n",
    "- [ ] `__init__()`: Initialize spaces and internal state\n",
    "- [ ] `reset()`: Return `(observation, info)` tuple, proper seeding\n",
    "- [ ] `step()`: Return `(observation, reward, terminated, truncated, info)` tuple\n",
    "- [ ] Transition function with proper slippery mechanics (80% intended, 10% each perpendicular)\n",
    "\n",
    "#### **Helper Methods (Priority: Medium)**\n",
    "- [ ] `_get_obs()`: Convert internal state to observation\n",
    "- [ ] `_set_state()`: Validate and set internal state\n",
    "- [ ] Boundary checking for attempted moves outside grid\n",
    "- [ ] Proper reward calculation (1 for goal, 0 elsewhere)\n",
    "\n",
    "#### **Validation & Testing (Priority: High)**\n",
    "- [ ] Input validation for actions and states\n",
    "- [ ] Edge case handling (boundaries, terminal states)\n",
    "- [ ] Consistency checks with reference implementation\n",
    "- [ ] Deterministic behavior for testing\n",
    "\n",
    "### âš ï¸ **Common Pitfalls to Avoid:**\n",
    "1. **API Inconsistency**: Not returning correct tuple structures\n",
    "2. **State Management**: Forgetting to update internal state properly\n",
    "3. **Transition Logic**: Incorrect slippery movement implementation\n",
    "4. **Boundary Handling**: Allowing invalid moves or state transitions\n",
    "5. **Seeding Issues**: Not properly handling random state for reproducibility\n",
    "\n",
    "### ğŸ§ª **Testing Strategy:**\n",
    "1. **Unit Tests**: Test each method individually\n",
    "2. **Integration Tests**: Full episode runs\n",
    "3. **Consistency Tests**: Compare with official implementation\n",
    "4. **Edge Case Tests**: Boundary conditions and invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e848a9-20d8-4688-a080-306628fa3b9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6883a4d3a67114700d7686062bf221",
     "grade": false,
     "grade_id": "cell-b8e10c8d02b6faa6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self._description = np.asarray([\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ], dtype='c')\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, action):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cae5e-1406-4fa2-9d93-1b9af302ba90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e7b9ba331c5e3fa3a00100a2984bec",
     "grade": false,
     "grade_id": "cell-c99ad325d97fb8d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Certifique-se que seu ambiente funciona na cÃ©lula abaixo.\n",
    "\n",
    "**AtenÃ§Ã£o:** os testes fornecidos nÃ£o cobrem todos os casos possÃ­veis. Realize testes adicionais para garantir a implementaÃ§Ã£o correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a5ed-2e90-4ce2-bb76-2b6360055ed9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392f0ae9859df2eb60fbf9f1d3ac80a7",
     "grade": false,
     "grade_id": "cell-ddead0156e8c7432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = FrozenLake()\n",
    "\n",
    "obs, info = env.reset()\n",
    "assert obs == 0, f\"ObservaÃ§Ã£o inicial esperada 0, recebeu {obs}\"\n",
    "\n",
    "env._set_state(5)\n",
    "obs = env._get_obs()\n",
    "assert obs == 5, f\"Estado esperado 5, recebeu {obs}\"\n",
    "\n",
    "for _ in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    assert 0 <= action < 4, f\"AÃ§Ã£o fora do intervalo esperado: {action}\"\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    assert 0 <= obs < 16, f\"ObservaÃ§Ã£o fora do intervalo esperado: {obs}\"\n",
    "    assert reward in [0, 1], f\"Recompensa invÃ¡lida: {reward}\"\n",
    "    assert isinstance(terminated, bool), f\"'terminated' deve ser bool, mas recebeu {type(terminated)}\"\n",
    "    assert truncated is False, f\"'truncated' deve ser False, mas recebeu {truncated}\"\n",
    "    assert isinstance(info, dict), f\"'info' deve ser dict, mas recebeu {type(info)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8e99e-94bb-426f-a7e0-106dad2769b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78bddd1db45c2e8b9e54c20f0791463",
     "grade": true,
     "grade_id": "cell-20901ef53a25a2b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NÃ£o altere ou remova esta cÃ©lula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673e83f-1c2d-4934-bacb-0e879fcf6eb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e93d2945e347d584c03b27844df147f6",
     "grade": true,
     "grade_id": "cell-b30cbb7c1fa808b0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NÃ£o altere ou remova esta cÃ©lula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d278f-5787-44d4-95b1-7e537b15db8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977788f0df2f7412652fd64ee795ff21",
     "grade": false,
     "grade_id": "cell-8b132c80e15a2de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a3b1-ce03-42e0-9a4a-c7760aa904be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b330df534aeb8aecd6308dd337e8bc46",
     "grade": false,
     "grade_id": "cell-e6a8d0aebce03142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora que estamos familiarizados com o ambiente Frozen Lake, nosso objetivo serÃ¡ encontrar uma polÃ­tica Ã³tima para ele.  Desta vez, utilizaremos a versÃ£o oficial do Frozen Lake, disponibilizado pelo Gymnasium. Ele possui algumas propriedades que facilitarÃ£o as prÃ³ximas implementaÃ§Ãµes. Sua tarefa serÃ¡ implementar o algoritmo *Policy Iteration*, conforme ilustrado abaixo.\n",
    "\n",
    "![Policy Iteration](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cbfcc-d78a-4409-91b7-ae40d26d254c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248bbc9df645c2d3c87ae09f1c751a52",
     "grade": false,
     "grade_id": "cell-80af17ade7f65c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. A implementaÃ§Ã£o serÃ¡ realizada em etapas. Comece implentando a funÃ§Ã£o `init_policy_iteration`, que inicializa e retorna dois arrays. O primeiro array armazenarÃ¡ os valores esperados de cada estado $V(s)$, enquanto o segundo conterÃ¡ a polÃ­tica do agente: para cada estado, ele indicarÃ¡ a aÃ§Ã£o que o agente deve realizar. Ambos os arrays devem ser inicializados com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afa70b-1d9e-4cfd-b026-9669cb329d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1465ddc541cd84973baead52e1296a77",
     "grade": false,
     "grade_id": "cell-0fb77ab1fc981da0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_policy_iteration(env: gym.Env) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c76e-1a7b-4fa0-afe7-8aa4296a852a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcda365a4d97947d565e8c44d241610c",
     "grade": false,
     "grade_id": "cell-833ab69b40216f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Agora, vamos computar o valor esperado $V(s) = \\sum_{s', r}p(s',r|s, a)[r + \\gamma V(s')]$. Implemente a funÃ§Ã£o `compute_expected_value`que recebe como parÃ¢metros o ambiente, o vetor $V$, um estado, uma aÃ§Ã£o, o valor de $\\gamma$ (fator de desconto), e retorna o valor esperado. NÃ£o altere os valores de $V$ nesta funÃ§Ã£o.\n",
    "\n",
    "**Importante:** A variÃ¡vel `env.unwrapped.P[state][action]` contÃ©m as transiÃ§Ãµes do ambiente, retornando uma lista com todas as transiÃ§Ãµes possÃ­veis para o par (state, action). Cada elemento dessa lista inclui, na seguinte ordem: a probabilidade da transiÃ§Ã£o, o estado $s'$ alcanÃ§ado, a recompensa recebida e um indicador de estado terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88dd79-de71-4c91-bc21-13f6db8f350e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086e19afee3929471d473c6b70e42a7e",
     "grade": false,
     "grade_id": "cell-fbb3d72642775bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_value(env: gym.Env, V: np.ndarray[float], state: int, action: int, gamma: float) -> float:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c4f38-ec8c-4d43-ac71-68243b719428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd97b9bb7399cf9d8ad4b4f9d324c34",
     "grade": false,
     "grade_id": "cell-b94451eaf15d9d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. O pÅ•oximo passo serÃ¡ avaliar a polÃ­tica do agente. Implemente o loop de avaliaÃ§Ã£o de polÃ­tica do policy iteration na funÃ§Ã£o `evaluate_policy`. Ela receberÃ¡ o ambiente, a polÃ­tica do agente, o vetor $V$, o valor $\\gamma$, e o valor $\\theta$. Esta funÃ§Ã£o nÃ£o precisa retornar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b124c-9ed3-47aa-a7bb-e1a198bdb87b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc74a4d30b19f68b2a836179a817e2f6",
     "grade": false,
     "grade_id": "cell-8df0495bf82eaaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float, theta: float) -> None:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8faf3-7d83-4c24-8f38-110981cf296b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a2d75b60f5350454137839a5e61ce25",
     "grade": false,
     "grade_id": "cell-deee4c0a66b4ef76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. A seguir, vamos implementar a atualizaÃ§Ã£o da polÃ­tica. Na funÃ§Ã£o `improve_policy` implemente uma iteraÃ§Ã£o da atualizaÃ§Ã£o da polÃ­tica. Ela recebe o ambiente, a polÃ­tica do agente, o vetor $V$, e o valor $\\gamma$. Ela deverÃ¡ retornar um booleano indicando se polÃ­tica estÃ¡ estÃ¡vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818354d1-3e10-4ac8-96cd-51eff85c54a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338436f555ec731b7fee10a05a929d5",
     "grade": false,
     "grade_id": "cell-4604a015625bbd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886671-c6a1-4bf8-8258-137c8030d8fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e9728ce314ad841c660af9d5745807",
     "grade": false,
     "grade_id": "cell-59ebc175679651bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A cÃ©lula abaixo implementa a estrutura do algoritmo *Policy Iteration* utilizando as funÃ§Ãµes desenvolvidas nas etapas anteriores. NÃ£o Ã© necessÃ¡rio realizar nenhuma implementaÃ§Ã£o nesta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab5bc4-ac2a-4a79-b5d5-7b6d668805b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc9300dfe827083e071ca31f8ddcefd",
     "grade": false,
     "grade_id": "cell-250e7d4c7bc0cf97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    V, policy = init_policy_iteration(env)\n",
    "\n",
    "    while True:\n",
    "        evaluate_policy(env, policy, V, gamma, theta)\n",
    "        policy_stable = improve_policy(env, policy, V, gamma)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef24a7-7071-4c51-926c-32b9e9c39c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0803fbd416c4e12f431191163c7a3d",
     "grade": false,
     "grade_id": "cell-ed343128eb786bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_policy(env:gym.Env, policy: np.ndarray[int]):\n",
    "    \"\"\"\n",
    "    Exibe a polÃ­tica de um ambiente FrozenLake de forma visual.\n",
    "\n",
    "    ParÃ¢metros:\n",
    "    -----------\n",
    "    env : gym.Env\n",
    "        Ambiente do tipo FrozenLake.\n",
    "    policy : np.ndarray\n",
    "        Array 1D contendo as aÃ§Ãµes a serem tomadas em cada estado.\n",
    "\n",
    "    AÃ§Ãµes sÃ£o mapeadas para setas:\n",
    "        0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'\n",
    "\n",
    "    SÃ­mbolos especiais do mapa:\n",
    "        'H': buraco â†’ 'â–¢'\n",
    "        'G': objetivo â†’ 'â—'\n",
    "    \"\"\"\n",
    "\n",
    "    ACTION_MAP = ['â†', 'â†“', 'â†’', 'â†‘']\n",
    "    HOLE_SYMBOL = 'â–¢'\n",
    "    GOAL_SYMBOL = 'â—'\n",
    "\n",
    "    n_rows, n_cols = env.unwrapped.desc.shape\n",
    "    policy_grid = np.full((n_rows, n_cols), \"\", dtype=str)\n",
    "\n",
    "    for index, action in enumerate(policy):\n",
    "        row, col = divmod(index, 4)\n",
    "        cell = env.unwrapped.desc[row, col]\n",
    "        if cell == b'H':\n",
    "            policy_grid[row, col] = HOLE_SYMBOL\n",
    "        elif cell == b'G':\n",
    "            policy_grid[row, col] = GOAL_SYMBOL\n",
    "        else:\n",
    "            policy_grid[row, col] = ACTION_MAP[action]\n",
    "\n",
    "    np.savetxt(sys.stdout, policy_grid, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5ed1-6b65-425f-894e-769d90603bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835c3efa9e65c2e9dfb7ad4c8f691ea9",
     "grade": false,
     "grade_id": "cell-98df69850425fb45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A cÃ©lula abaixo irÃ¡ executar seu algoritmo *Policy Iteration* em um ambiente Frozen Lake determinÃ­stico, ou seja, onde o agente nÃ£o corre o risco de escorregar para direÃ§Ãµes indesejadas. A polÃ­tica resultante serÃ¡ armazenada na variÃ¡vel `policy_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que o algoritmo esteja funcionando corretamente e que a polÃ­tica gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d8872-c213-44e4-ae3b-d187c7a7f097",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8473ab04eee00f9f4041247dae2091d9",
     "grade": true,
     "grade_id": "cell-d1021093fae62b6c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, policy_iteration_deterministic = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, policy_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(policy_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"PolÃ­tica diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3731f7-5e61-4b1c-adc9-ce021b8345ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "391cc7348680b297fc58d3be91d0dd5e",
     "grade": true,
     "grade_id": "cell-a422263f5dd5254d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NÃ£o altere ou remova esta cÃ©lula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297afae4-639a-4741-811a-1693ce726413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75f8abf27b293dd60179ff3844f5c8f0",
     "grade": false,
     "grade_id": "cell-5e9663f3341ea0cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf17ac-9e34-4b94-a8c0-abefbd22c201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "213cec389fa200d841b8e593f5a6d5f5",
     "grade": false,
     "grade_id": "cell-da562c89000eeffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Neste exercÃ­cio vamos encontrar uma polÃ­tica Ã³tima para o Frozen Lake utilizando o algoritmo *Value Iteration* como descrito abaixo.\n",
    "\n",
    "![Value Iteration](value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4db7-80f5-4964-aa87-bf561413c5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "905d1ad1a904f82f88c7c68c0ab5341b",
     "grade": false,
     "grade_id": "cell-179ab2fb26b003ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Novamente, vamos dividir este exercÃ­cios em etapas menores. O primeiro passo consiste em inicializar o vetor $V$, que armazenarÃ¡ os valores esperados para cada estado. Para isso, implemente a funÃ§Ã£o `init_value_iteration`, que recebe um ambiente como parÃ¢metro e retorna o vetor $V$. Este vetor deve ser inicializado com valores zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e2c68-c87e-4342-8ca6-57a43685180b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257304c2b6c04f1ce1743dde7e5095de",
     "grade": false,
     "grade_id": "cell-80a50671b2e72667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_value_iteration(env: gym.Env) -> np.ndarray[float]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62953af-0ac2-4064-9731-be0d11e6285a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a6069926e5998ea4f6aa2a76ff753",
     "grade": false,
     "grade_id": "cell-4e33e273961d7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Agora, vamos gerar uma polÃ­tica determinÃ­stica a partir de um vetor $V$, conforme definido pela equaÃ§Ã£o $\\pi(s)= \\textrm{argmax}_a \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$. Implemente a funÃ§Ã£o `generate_policy`, que recebe um ambiente e um vetor $V$, retornando a polÃ­tica determinÃ­stica resultante.\n",
    "\n",
    "**Dica:** Utilize a funÃ§Ã£o `compute_expected_value` do exercÃ­cio anterior para facilitar sua implementaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a710-e02c-46be-a93b-56683cfe6593",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee4fc2ed2d51ff97989aaddc8706c51",
     "grade": false,
     "grade_id": "cell-41eb7e70f58ee606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_policy(env: gym.Env, V: np.ndarray[float], gamma: float) -> np.ndarray[int]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76228-99f7-4eb3-a265-e05e0ade9a3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d08d908f6776a4eecbe68786819e0d9",
     "grade": false,
     "grade_id": "cell-eef5f2b3ef2b657d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Por fim, implemente o loop principal do *Value Iteration* na funÃ§Ã£o `value_iteration`. Ela deverÃ¡ retornar, nesta ordem, o array de valores $V$ e a polÃ­tica obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4be75-e22e-40f7-aca3-31f7b810c32d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1207e1ee95de47ccab3943f810509cc9",
     "grade": false,
     "grade_id": "cell-c48b9185009da819",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env: gym.Env, gamma:float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f621f3f-9261-4f75-93fa-a511c3013aad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3130d567b2d34a7411e80ff13d04238e",
     "grade": false,
     "grade_id": "cell-3a5e438c6988c441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A cÃ©lula abaixo irÃ¡ executar seu algoritmo *Value Iteration* em um ambiente Frozen Lake determinÃ­stico. A polÃ­tica resultante serÃ¡ armazenada a variÃ¡vel `value_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que ele esteja funcionando corretamente e que a polÃ­tica gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13ad40-daa8-4c50-92c2-92336378d91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a507f4b6da72cd0204ce61ef7d995c7",
     "grade": true,
     "grade_id": "cell-65ff2786217d46c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, value_iteration_deterministic = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, value_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(value_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"PolÃ­tica diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7814681-8791-4066-9205-6bd78043f7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8477abf2d6223cb005a34bf19346d24f",
     "grade": true,
     "grade_id": "cell-90866ba893778afb",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NÃ£o altere ou remova esta cÃ©lula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845b158-b075-4b9e-9da3-cc7cffd82b47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0bf756f1c61d1326fd39f832e2599b0",
     "grade": false,
     "grade_id": "cell-f948772c42437869",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## AnÃ¡lise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac23f89-42f3-44b3-854b-366657919bfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef88d7295863002c1746176f54048cfb",
     "grade": false,
     "grade_id": "cell-129f7cfccf09f7e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora, executaremos seus algoritmos no mesmo ambiente do Frozen Lake, porÃ©m escorregadio. As polÃ­ticas resultante serÃ£o armazenadas nas variÃ¡veis `policy_iteration_slippery` e `value_iteration_slippery`, que usaremos na tarefa 14. Nesse cenÃ¡rio, o agente tem apenas 1/3 de chance de se mover na direÃ§Ã£o desejada e 2/3 de chance de se mover em uma direÃ§Ã£o perpendicular. Observe as polÃ­ticas resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952605d-7df8-4583-ae30-d27f611b8565",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee3c7f66748e1f0c7e5aded56c485dd5",
     "grade": false,
     "grade_id": "cell-51aba37175d6166c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, policy_iteration_slippery = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Policy Iteration\")\n",
    "print_policy(env, policy_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e214b7-6f93-4f64-813f-23fac2d91eab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28ac89d9a717a5ad6077b10991b0ea62",
     "grade": false,
     "grade_id": "cell-1cd7b2d2c9f5a32e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, value_iteration_slippery = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Value Iteration\")\n",
    "print_policy(env, value_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3cef6-fa32-44a7-b1a1-b59415304dcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f3c44171f967cf4a67342ee8dd96568",
     "grade": false,
     "grade_id": "cell-9ebda4da8134aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Implemente a funÃ§Ã£o `execute_policy` abaixo, que deve executar uma polÃ­tica previamente obtida em um ambiente Frozen Lake por $N$ episÃ³dios, retornando a recompensa acumulada de cada episÃ³dio e suas duraÃ§Ãµes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a263709-5c41-420f-a35e-4ce3bd2b0af0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65dd2151b7b6eb59037e7c206aebf2cf",
     "grade": true,
     "grade_id": "cell-8c6445e2c40cad0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def execute_policy(env: gym.Env, policy: np.ndarray[int], n_episodes):\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        episode_lengths.append(step_count)\n",
    "    return episode_returns, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359818ef-5844-4300-8e03-f989005f5251",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f138e90efe57b4f18d45fa1f9c7da8e3",
     "grade": false,
     "grade_id": "cell-774fba8dd36ffbe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Utilize a funÃ§Ã£o `execute_policy` para avaliar a polÃ­tica obtida pelo Policy Iteration no Frozen Lake **determinÃ­stico** (`policy_iteration_deterministic`) em um ambiente Frozen Lake escorregadio por 10 episÃ³dios. Armazene as recompensas acumuladas ao longo dos episÃ³dios na variÃ¡vel `agent_1_returns` e a duraÃ§Ã£o dos episÃ³dios na variÃ¡vel `agent_1_lengths`. Observe o comportamento do agente durante a execuÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dc536-c94c-43fd-9d87-204ea87b04bb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4ae3ec1ce95e41ffe14db005b9ae544",
     "grade": false,
     "grade_id": "cell-5e1db6859a2651d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b773d-73f5-4a22-bda0-5d21a1f264d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e0754bfa1c08878b4c14904ee53a0f",
     "grade": false,
     "grade_id": "cell-bde46ff3fce95520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Repita o procedimento da tarefa anterior, desta vez utilizando a polÃ­tica obtida pelo Policy Iteration no Frozen Lake **escorregadio** (`policy_iteration_slippery`). Armazene as recompensas acumuladas ao longo dos episÃ³dios na variÃ¡vel `agent_2_returns` e a duraÃ§Ã£o dos episÃ³dios na variÃ¡vel `agent_2_lengths`. Observe o comportamento do agente durante a execuÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020b4b0-47d0-4c60-a4f1-7e6b1411ab47",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c50df759f226ac2558ea4d515f343a7",
     "grade": true,
     "grade_id": "cell-d7202ccd7e779b1a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f24dc-2e61-4c83-ad77-50638b8c0e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba9c4b3ee4aec555f44237ab98d95d25",
     "grade": false,
     "grade_id": "cell-eaad1d7c23d3bf5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analise a seguinte comparaÃ§Ã£o entre as recompensas e a duraÃ§Ã£o obtidas por cada uma dessas duas execuÃ§Ãµes no Frozen Lake escorregadio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1b38f-a55f-4a6a-b334-31380f871153",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbed36db4b67ef8e511dbcec2a153c7f",
     "grade": false,
     "grade_id": "cell-5eec276dbb402e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def compare_policy(\n",
    "    rewards_run1, lengths_run1,\n",
    "    rewards_run2, lengths_run2,\n",
    "    label_run1=\"Agent 1\", label_run2=\"Agent 2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two policy runs using mean return and mean episode length.\n",
    "\n",
    "    Args:\n",
    "        rewards_run1 (list): Episode rewards for run 1.\n",
    "        lengths_run1 (list): Episode lengths for run 1.\n",
    "        rewards_run2 (list): Episode rewards for run 2.\n",
    "        lengths_run2 (list): Episode lengths for run 2.\n",
    "        label_run1 (str): Label for run 1.\n",
    "        label_run2 (str): Label for run 2.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_rewards = [np.mean(rewards_run1), np.mean(rewards_run2)]\n",
    "    std_rewards  = [np.std(rewards_run1), np.std(rewards_run2)]\n",
    "\n",
    "    mean_lengths = [np.mean(lengths_run1), np.mean(lengths_run2)]\n",
    "    std_lengths  = [np.std(lengths_run1), np.std(lengths_run2)]\n",
    "\n",
    "    labels = [label_run1, label_run2]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Mean Rewards\n",
    "    axes[0].bar(x, mean_rewards, yerr=std_rewards, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_ylabel(\"Mean Total Reward\")\n",
    "    axes[0].set_title(\"Mean Episode Return Â± Std\")\n",
    "    axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # Mean Episode Lengths\n",
    "    axes[1].bar(x, mean_lengths, yerr=std_lengths, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_ylabel(\"Mean Episode Length\")\n",
    "    axes[1].set_title(\"Mean Episode Length Â± Std\")\n",
    "    axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_policy(agent_1_returns, agent_1_lengths, agent_2_returns, agent_2_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa0ba7-455a-41c4-97d4-8c17fa8b0e1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd865599e6154e8061df6dcbe6aa7eca",
     "grade": false,
     "grade_id": "cell-f8680351f981a5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "15. Explique quais fatores levaram Ã s diferenÃ§as observadas entre as polÃ­ticas obtidas no ambiente determinÃ­stico e no ambiente escorregadio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dd49c-eb7c-41a1-a3d1-ca7ef0818922",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4304d231930b7c1e752d1520d20c830",
     "grade": true,
     "grade_id": "cell-21cbc643c62202d7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4571b7-ff1b-4c70-9d85-e0de016a6bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5292df5ef29eee183f943505f3ec2e4",
     "grade": false,
     "grade_id": "cell-426ab2d2c4253fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "16. Quais estratÃ©gias poderiam ser adotadas para tornar o comportamento do agente menos conservador quando treinado no ambiente escorregadio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce5371-a76a-415b-b282-4d4ab80ff056",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbcb94e28a49b89a5fb7b66e4c166a94",
     "grade": true,
     "grade_id": "cell-70d62e2dd7e44e0e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
