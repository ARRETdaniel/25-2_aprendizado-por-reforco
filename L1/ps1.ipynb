{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c18e0f-36ba-4010-8ba6-9ec2c9966e3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eead5b36a3ee2d8eb504615ec2e698b",
     "grade": false,
     "grade_id": "cell-4c87f565d23c35db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lista de Exercícios 1: Processos de Decisão de Markov e Programação Dinâmica\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140e15d-8ff2-404e-9f0b-de4c9444c9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46221e6cdc09eef7a6cc1ae625ff7263",
     "grade": false,
     "grade_id": "cell-e6ec4bda5dd7e12a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd360e8-7428-4cb3-af57-ef79d5c99ef8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de898f61e5161ac2497e0a4c68b4e92e",
     "grade": false,
     "grade_id": "cell-d31fd315f36785a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÕES QUE NÃO SEGUIREM AS INSTRUÇÕES A SEGUIR NÃO SERÃO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido antes de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps1.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 23/09/2025. Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea77-cdbe-474f-bc2f-95daa8d439c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31845cf7b26a7f7728c186a8c96638a3",
     "grade": false,
     "grade_id": "cell-f1f0ba316c0b79ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1557b9-966f-44c4-85be-cb75ffd2c95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1098a937587b35eb2f0dae504db9be3",
     "grade": false,
     "grade_id": "cell-69a0af10519ed240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "O ambiente Frozen Lake é uma simulação clássica utilizada para treinamento de agentes em aprendizado por reforço. Neste ambiente, o agente navega por um lago congelado representado por um grid de tamanho $n \\times m$, com o objetivo de alcançar um alvo. O lago contém dois tipos de células: (1) células com gelo sólido, que são seguras para o agente se mover, e (2) as células com buracos, nas quais o agente cai e falha a missão. Embora o Gymnasium já possua uma implementação do Frozen Lake, neste exercício iremos implementá-lo do zero.\n",
    "\n",
    "No início de cada episódio, o agente é posicionado na célula $[0, 0]$ enquanto o alvo é posicionado na célula mais distante do agente, na posição $[n-1, m-1]$ em um mapa de tamanho $n \\times m$. A cada passo, o agente recebe uma observação indicando sua posição atual no lago e tem a possibilidade de escolher entre quatro ações possíveis: mover-se para cima, para baixo, para a esquerda ou para a direita. No entanto, devido à superfície escorregadia do lago, ele nem sempre se move na direção desejada, podendo acabar se movendo em uma direção perpendicular à escolhida. O agente recebe uma recompensa de 1 se alcançar o alvo e zero em todos os outros estados. Um episódio termina quando o agente alcança o objetivo ou cai na água.\n",
    "\n",
    "Neste exercício, vamos trabalhar sempre com o mesmo mapa $4 \\times 4$, representado na figura abaixo.\n",
    "\n",
    "![Frozen Lake Map](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46123-2985-4e8e-87b5-ed33a8fccfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b35e66b571d4144b5769839b2a1134",
     "grade": false,
     "grade_id": "cell-6e7a4e34e7d477a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa será implementar o ambiente Frozen Lake utilizando o arcabouço fornecido pelo Gymnasium. Abaixo, você encontrará um código inicial que deverá ser utilizado em sua implementação. Siga essas instruções para garantir que seu código está de acordo com o esperado:\n",
    "\n",
    "1. Na função `__init__`, já definimos o mapa que será utilizado e armazenamos essa informação na variável `_description`. Nesse mapa, a letra 'S' representa a posição inicial do agente, a letra 'G' indica o alvo, as letras 'F' representam gelo sólido (que é seguro) e as letras 'H' marcam os buracos. No entanto, ainda é necessário adicionar mais algumas informações no ambiente, especificamente sobre a representação das observações e das ações. Embora existam várias maneiras de representar os espaços de observações e de ação, neste exercício, você deve usar a forma mais simples possível, que pode ser representada por um único valor discreto. Na função `__init__`, defina o espaço de observações e o espaço de ações, atribuindo-os às variáveis `self.observation_space` e `self.action_space`, respectivamente. Utilize apenas a classe `gymnasium.spaces.Discrete` nesta tarefa.\n",
    "\n",
    "2. Antes de prosseguirmos com as funcionalidades do gymnasium, vamos implementar algumas funções auxiliares para facilitar as próximas etapas. Implemente a função `_get_obs`, que retorna a observação atual do ambiente. Além disso, implemente a função `_set_state`, que recebe um valor inteiro correspondente a uma posição no lago e coloca o agente nesta localização.\n",
    "\n",
    "3. A função `reset` deve resetar o ambiente e inicializar um novo episódio, posicionando o agente na célula $[0, 0]$ e fazendo todos os ajustes internos necessários. Esta função deve retornar uma tupla contendo a observação inicial e as informações do ambiente. Neste exercício, vamos retornar um dicionario vazio `{}` para as informações. Lembre-se que a observação deve ser um único valor discreto, como definido no item 1. Implemente a função `reset`.\n",
    "\n",
    "4. A função `step()` é responsável por atualizar o ambiente com base na ação executada pelo agente. Ela recebe como entrada a ação escolhida pelo agente, um parâmetro seed e uma variável options, e calcula o novo estado atual com base na função de transição previamente definida. Neste exercício, você pode ignorar os parâmetros seed e options, pois não precisaremos deles. Neste ambiente que estamos desenvolvendo, o agente tem 80% de chance de se mover na direção desejada e 20% de chance de se mover em uma direção perpendicular à escolhida, distribuída igualmente entre os dois sentidos possíveis (10% para cada um). **As ações do agente devem ser representadas pelos valores 0 (mover-se para a esquerda), 1 (mover-se para baixo), 2 (mover-se para a direita) e 3 (mover-se para cima)**. Caso o agente tente se mover para fora do mapa, ele permanecerá na mesma posição. Além disso, a função atribui uma recompensa ao agente e verifica se o episódio chegou ao fim. Implemente a função step() para que ela retorne a observação do estado atual, a recompensa recebida, um valor booleano indicando se o estado é terminal, um valor booleano informando se o episódio foi truncado e as informações do ambiente. Esses dois últimos valores são necessários devidio à interface estabelecida pelo gymnasium, mas não se preocupe com eles; apenas retorne sempre `False` e `{}` para eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c82e329-a086-4479-8375-bb1545073294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d09d0c8292f8dbf0e2bdf379701d722",
     "grade": false,
     "grade_id": "cell-bd1f83bd5e0cfba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e848a9-20d8-4688-a080-306628fa3b9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6883a4d3a67114700d7686062bf221",
     "grade": false,
     "grade_id": "cell-b8e10c8d02b6faa6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Frozen Lake environment.\n",
    "        [cite_start]This follows the setup for a standard MDP as described in the course materials. [cite: 1773, 1779]\n",
    "        \"\"\"\n",
    "        self._description = np.asarray([\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ], dtype='c')\n",
    "\n",
    "        # Get the dimensions of the grid from the description.\n",
    "        self.n_rows, self.n_cols = self._description.shape\n",
    "\n",
    "        # The number of states is the total number of cells in the grid.\n",
    "        n_states = self.n_rows * self.n_cols\n",
    "        # There are four discrete actions: Left, Down, Right, Up.\n",
    "        n_actions = 4\n",
    "\n",
    "        # Define the observation space as a discrete set of states from 0 to n_states-1.\n",
    "        # [cite_start]This is a standard representation for tabular methods. [cite: 1789]\n",
    "        self.observation_space = gym.spaces.Discrete(n_states)\n",
    "\n",
    "        # Define the action space as a discrete set of actions from 0 to n_actions-1.\n",
    "        self.action_space = gym.spaces.Discrete(n_actions)\n",
    "\n",
    "        # Initialize the agent's position. This will be updated in reset().\n",
    "        self._agent_pos = 0\n",
    "\n",
    "    def _get_obs(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the current observation of the environment, which is the agent's position.\n",
    "        \"\"\"\n",
    "        # The observation is the integer representation of the agent's current state.\n",
    "        return self._agent_pos\n",
    "\n",
    "    def _set_state(self, state: int):\n",
    "        \"\"\"\n",
    "        Sets the agent's current state (position).\n",
    "\n",
    "        Args:\n",
    "            state (int): The integer representing the new state.\n",
    "        \"\"\"\n",
    "        # Validate that the state is within the bounds of the observation space.\n",
    "        if not self.observation_space.contains(state):\n",
    "            raise ValueError(f\"Invalid state {state} for this environment.\")\n",
    "\n",
    "        # Update the agent's position.\n",
    "        self._agent_pos = state\n",
    "\n",
    "    def reset(self, seed: int = None, options: dict = None) -> tuple[int, dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state for a new episode.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the initial observation and an empty info dictionary.\n",
    "        \"\"\"\n",
    "        # The super().reset() call handles the seeding of the random number generator.\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # The agent always starts at state 0, corresponding to the 'S' tile.\n",
    "        self._agent_pos = 0\n",
    "\n",
    "        # Return the initial observation and an empty info dictionary as per Gymnasium API.\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool, dict]:\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment based on the given action.\n",
    "        [cite_start]This function implements the transition probability p(s', r|s, a). [cite: 1781]\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the next observation, reward, terminated flag, truncated flag, and an empty info dictionary.\n",
    "        \"\"\"\n",
    "        # Mapping of actions to coordinate changes (row, col).\n",
    "        # 0: Left, 1: Down, 2: Right, 3: Up\n",
    "        action_to_delta = {\n",
    "            0: (0, -1),  # Left\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, 1),   # Right\n",
    "            3: (-1, 0)   # Up\n",
    "        }\n",
    "\n",
    "        # Define probabilities for the slippery (stochastic) transitions.\n",
    "        # The agent moves in the intended direction with 80% probability.\n",
    "        # The remaining 20% is split between the two perpendicular directions.\n",
    "        intended_prob = 0.8\n",
    "        #perp_prob = 0.1 used np.random.choice() for uniform choice distribution over the perpendicular actions.\n",
    "\n",
    "        # The two perpendicular actions for each intended action.\n",
    "        perp_actions = {0: [3, 1], 1: [0, 2], 2: [1, 3], 3: [0, 1]}\n",
    "\n",
    "        # Determine the actual direction of movement based on stochasticity.\n",
    "        if self.np_random.random() < intended_prob:\n",
    "            # Move in the intended direction.\n",
    "            move = action\n",
    "        else:\n",
    "            # Slip and move in one of the perpendicular directions.\n",
    "            move = self.np_random.choice(perp_actions[action])\n",
    "\n",
    "        # Get the current position (row, col) from the agent's state.\n",
    "        current_row, current_col = divmod(self._agent_pos, self.n_cols)\n",
    "\n",
    "        # Calculate the potential new position based on the chosen move.\n",
    "        delta_row, delta_col = action_to_delta[move]\n",
    "        new_row = current_row + delta_row\n",
    "        new_col = current_col + delta_col\n",
    "\n",
    "        # Check for boundary conditions. If the agent hits a wall, it stays put.\n",
    "        if not (0 <= new_row < self.n_rows and 0 <= new_col < self.n_cols):\n",
    "            new_row, new_col = current_row, current_col\n",
    "\n",
    "        # Update the agent's state (position).\n",
    "        self._agent_pos = new_row * self.n_cols + new_col\n",
    "\n",
    "        # Determine the outcome of the move.\n",
    "        cell_type = self._description[new_row, new_col]\n",
    "\n",
    "        # A terminal state is reached if the agent is at the goal or in a hole.\n",
    "        terminated = (cell_type == b'G' or cell_type == b'H')\n",
    "\n",
    "        # Reward is 1 only if the goal is reached.\n",
    "        reward = 1.0 if cell_type == b'G' else 0.0\n",
    "\n",
    "        # Truncation is not used in this environment.\n",
    "        truncated = False\n",
    "\n",
    "        # The info dictionary is empty.\n",
    "        info = {}\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cae5e-1406-4fa2-9d93-1b9af302ba90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e7b9ba331c5e3fa3a00100a2984bec",
     "grade": false,
     "grade_id": "cell-c99ad325d97fb8d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Certifique-se que seu ambiente funciona na célula abaixo.\n",
    "\n",
    "**Atenção:** os testes fornecidos não cobrem todos os casos possíveis. Realize testes adicionais para garantir a implementação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2848a5ed-2e90-4ce2-bb76-2b6360055ed9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392f0ae9859df2eb60fbf9f1d3ac80a7",
     "grade": false,
     "grade_id": "cell-ddead0156e8c7432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = FrozenLake()\n",
    "\n",
    "obs, info = env.reset()\n",
    "assert obs == 0, f\"Observação inicial esperada 0, recebeu {obs}\"\n",
    "\n",
    "env._set_state(5)\n",
    "obs = env._get_obs()\n",
    "assert obs == 5, f\"Estado esperado 5, recebeu {obs}\"\n",
    "\n",
    "for _ in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    assert 0 <= action < 4, f\"Ação fora do intervalo esperado: {action}\"\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    assert 0 <= obs < 16, f\"Observação fora do intervalo esperado: {obs}\"\n",
    "    assert reward in [0, 1], f\"Recompensa inválida: {reward}\"\n",
    "    assert isinstance(terminated, bool), f\"'terminated' deve ser bool, mas recebeu {type(terminated)}\"\n",
    "    assert truncated is False, f\"'truncated' deve ser False, mas recebeu {truncated}\"\n",
    "    assert isinstance(info, dict), f\"'info' deve ser dict, mas recebeu {type(info)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb8e99e-94bb-426f-a7e0-106dad2769b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78bddd1db45c2e8b9e54c20f0791463",
     "grade": true,
     "grade_id": "cell-20901ef53a25a2b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3673e83f-1c2d-4934-bacb-0e879fcf6eb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e93d2945e347d584c03b27844df147f6",
     "grade": true,
     "grade_id": "cell-b30cbb7c1fa808b0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d278f-5787-44d4-95b1-7e537b15db8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977788f0df2f7412652fd64ee795ff21",
     "grade": false,
     "grade_id": "cell-8b132c80e15a2de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a3b1-ce03-42e0-9a4a-c7760aa904be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b330df534aeb8aecd6308dd337e8bc46",
     "grade": false,
     "grade_id": "cell-e6a8d0aebce03142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora que estamos familiarizados com o ambiente Frozen Lake, nosso objetivo será encontrar uma política ótima para ele.  Desta vez, utilizaremos a versão oficial do Frozen Lake, disponibilizado pelo Gymnasium. Ele possui algumas propriedades que facilitarão as próximas implementações. Sua tarefa será implementar o algoritmo *Policy Iteration*, conforme ilustrado abaixo.\n",
    "\n",
    "![Policy Iteration](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cbfcc-d78a-4409-91b7-ae40d26d254c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248bbc9df645c2d3c87ae09f1c751a52",
     "grade": false,
     "grade_id": "cell-80af17ade7f65c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. A implementação será realizada em etapas. Comece implentando a função `init_policy_iteration`, que inicializa e retorna dois arrays. O primeiro array armazenará os valores esperados de cada estado $V(s)$, enquanto o segundo conterá a política do agente: para cada estado, ele indicará a ação que o agente deve realizar. Ambos os arrays devem ser inicializados com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8afa70b-1d9e-4cfd-b026-9669cb329d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1465ddc541cd84973baead52e1296a77",
     "grade": false,
     "grade_id": "cell-0fb77ab1fc981da0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_policy_iteration(env: gym.Env) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Initializes the value function (V) and policy arrays for the Policy Iteration algorithm.\n",
    "    This corresponds to the 'Initialization' step in the Policy Iteration pseudocode.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the initialized V-table and policy array.\n",
    "    \"\"\"\n",
    "    # Get the total number of states from the environment's observation space.\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the value function array (V-table) with zeros for all states.\n",
    "    # This provides a neutral starting point for evaluation.\n",
    "    V = np.zeros(n_states, dtype=float)\n",
    "\n",
    "    # Initialize the policy array with zeros. This means the initial policy\n",
    "    # is to take action 0 (Left) for all states, but it will be updated.\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c76e-1a7b-4fa0-afe7-8aa4296a852a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcda365a4d97947d565e8c44d241610c",
     "grade": false,
     "grade_id": "cell-833ab69b40216f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Agora, vamos computar o valor esperado $V(s) = \\sum_{s', r}p(s',r|s, a)[r + \\gamma V(s')]$. Implemente a função `compute_expected_value`que recebe como parâmetros o ambiente, o vetor $V$, um estado, uma ação, o valor de $\\gamma$ (fator de desconto), e retorna o valor esperado. Não altere os valores de $V$ nesta função.\n",
    "\n",
    "**Importante:** A variável `env.unwrapped.P[state][action]` contém as transições do ambiente, retornando uma lista com todas as transições possíveis para o par (state, action). Cada elemento dessa lista inclui, na seguinte ordem: a probabilidade da transição, o estado $s'$ alcançado, a recompensa recebida e um indicador de estado terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca88dd79-de71-4c91-bc21-13f6db8f350e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086e19afee3929471d473c6b70e42a7e",
     "grade": false,
     "grade_id": "cell-fbb3d72642775bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_value(env: gym.Env, V: np.ndarray[float], state: int, action: int, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the expected value of a state-action pair, q(s, a).\n",
    "    This function calculates the sum over all possible next states and rewards:\n",
    "    sum_{s', r} p(s', r|s, a)[r + gamma * V(s')].\n",
    "    This is a core calculation used in both policy evaluation and improvement.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        V: The current value function array.\n",
    "        state: The current state (s).\n",
    "        action: The action being evaluated (a).\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        The expected value (q-value) of the state-action pair.\n",
    "    \"\"\"\n",
    "    expected_value = 0.0\n",
    "\n",
    "    # env.unwrapped.P[state][action] provides the transition model p(s', r|s, a).\n",
    "    # It returns a list of tuples: (probability, next_state, reward, terminated_flag).\n",
    "    for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "        # This line directly implements the formula's core component.\n",
    "        expected_value += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c4f38-ec8c-4d43-ac71-68243b719428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd97b9bb7399cf9d8ad4b4f9d324c34",
     "grade": false,
     "grade_id": "cell-b94451eaf15d9d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. O pŕoximo passo será avaliar a política do agente. Implemente o loop de avaliação de política do policy iteration na função `evaluate_policy`. Ela receberá o ambiente, a política do agente, o vetor $V$, o valor $\\gamma$, e o valor $\\theta$. Esta função não precisa retornar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705b124c-9ed3-47aa-a7bb-e1a198bdb87b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc74a4d30b19f68b2a836179a817e2f6",
     "grade": false,
     "grade_id": "cell-8df0495bf82eaaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float, theta: float) -> None:\n",
    "    \"\"\"\n",
    "    Performs the Policy Evaluation step of the Policy Iteration algorithm.\n",
    "    It iteratively updates the value function V for a given policy until the\n",
    "    change in value is smaller than the threshold theta.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        policy: The policy to be evaluated.\n",
    "        V: The value function array to be updated.\n",
    "        gamma: The discount factor.\n",
    "        theta: The convergence threshold.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Initialize delta to track the maximum change in V in a single sweep.\n",
    "        delta = 0\n",
    "\n",
    "        # Iterate through all states in the environment.\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Store the old value of the state to measure the change.\n",
    "            old_v = V[state]\n",
    "\n",
    "            # Get the action to be taken in the current state according to the policy.\n",
    "            action = policy[state]\n",
    "\n",
    "            # Update the value of the current state using the Bellman equation for v_pi.\n",
    "            # This is an 'expected update' as it averages over all possible outcomes.\n",
    "            V[state] = compute_expected_value(env, V, state, action, gamma)\n",
    "\n",
    "            # Update delta with the absolute difference.\n",
    "            delta = max(delta, abs(old_v - V[state]))\n",
    "\n",
    "        # If the maximum change is less than the threshold, the value function has converged.\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8faf3-7d83-4c24-8f38-110981cf296b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a2d75b60f5350454137839a5e61ce25",
     "grade": false,
     "grade_id": "cell-deee4c0a66b4ef76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. A seguir, vamos implementar a atualização da política. Na função `improve_policy` implemente uma iteração da atualização da política. Ela recebe o ambiente, a política do agente, o vetor $V$, e o valor $\\gamma$. Ela deverá retornar um booleano indicando se política está estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818354d1-3e10-4ac8-96cd-51eff85c54a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338436f555ec731b7fee10a05a929d5",
     "grade": false,
     "grade_id": "cell-4604a015625bbd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float) -> bool:\n",
    "    \"\"\"\n",
    "    Performs the Policy Improvement step of the Policy Iteration algorithm.\n",
    "    It updates the policy to be greedy with respect to the current value function V.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        policy: The policy array to be updated.\n",
    "        V: The current value function array.\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        A boolean indicating whether the policy remained stable (True) or was changed (False).\n",
    "    \"\"\"\n",
    "    policy_stable = True\n",
    "\n",
    "    # Iterate through all states to update the policy for each one.\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Store the action specified by the old policy.\n",
    "        old_action = policy[state]\n",
    "\n",
    "        # Find the best action by calculating the expected value for all possible actions\n",
    "        # and selecting the one with the highest value (argmax).\n",
    "        action_values = [compute_expected_value(env, V, state, action, gamma) for action in range(env.action_space.n)]\n",
    "        policy[state] = np.argmax(action_values)\n",
    "\n",
    "        # If the action for this state has changed, the policy is not yet stable.\n",
    "        if old_action != policy[state]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886671-c6a1-4bf8-8258-137c8030d8fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e9728ce314ad841c660af9d5745807",
     "grade": false,
     "grade_id": "cell-59ebc175679651bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo implementa a estrutura do algoritmo *Policy Iteration* utilizando as funções desenvolvidas nas etapas anteriores. Não é necessário realizar nenhuma implementação nesta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aab5bc4-ac2a-4a79-b5d5-7b6d668805b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc9300dfe827083e071ca31f8ddcefd",
     "grade": false,
     "grade_id": "cell-250e7d4c7bc0cf97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    V, policy = init_policy_iteration(env)\n",
    "\n",
    "    while True:\n",
    "        evaluate_policy(env, policy, V, gamma, theta)\n",
    "        policy_stable = improve_policy(env, policy, V, gamma)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ef24a7-7071-4c51-926c-32b9e9c39c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0803fbd416c4e12f431191163c7a3d",
     "grade": false,
     "grade_id": "cell-ed343128eb786bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_policy(env:gym.Env, policy: np.ndarray[int]):\n",
    "    \"\"\"\n",
    "    Exibe a política de um ambiente FrozenLake de forma visual.\n",
    "\n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    env : gym.Env\n",
    "        Ambiente do tipo FrozenLake.\n",
    "    policy : np.ndarray\n",
    "        Array 1D contendo as ações a serem tomadas em cada estado.\n",
    "\n",
    "    Ações são mapeadas para setas:\n",
    "        0: '←', 1: '↓', 2: '→', 3: '↑'\n",
    "\n",
    "    Símbolos especiais do mapa:\n",
    "        'H': buraco → '▢'\n",
    "        'G': objetivo → '◎'\n",
    "    \"\"\"\n",
    "\n",
    "    ACTION_MAP = ['←', '↓', '→', '↑']\n",
    "    HOLE_SYMBOL = '▢'\n",
    "    GOAL_SYMBOL = '◎'\n",
    "\n",
    "    n_rows, n_cols = env.unwrapped.desc.shape\n",
    "    policy_grid = np.full((n_rows, n_cols), \"\", dtype=str)\n",
    "\n",
    "    for index, action in enumerate(policy):\n",
    "        row, col = divmod(index, 4)\n",
    "        cell = env.unwrapped.desc[row, col]\n",
    "        if cell == b'H':\n",
    "            policy_grid[row, col] = HOLE_SYMBOL\n",
    "        elif cell == b'G':\n",
    "            policy_grid[row, col] = GOAL_SYMBOL\n",
    "        else:\n",
    "            policy_grid[row, col] = ACTION_MAP[action]\n",
    "\n",
    "    np.savetxt(sys.stdout, policy_grid, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5ed1-6b65-425f-894e-769d90603bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835c3efa9e65c2e9dfb7ad4c8f691ea9",
     "grade": false,
     "grade_id": "cell-98df69850425fb45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Policy Iteration* em um ambiente Frozen Lake determinístico, ou seja, onde o agente não corre o risco de escorregar para direções indesejadas. A política resultante será armazenada na variável `policy_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que o algoritmo esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1d8872-c213-44e4-ae3b-d187c7a7f097",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8473ab04eee00f9f4041247dae2091d9",
     "grade": true,
     "grade_id": "cell-d1021093fae62b6c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ → ↓ ←\n",
      "↓ ▢ ↓ ▢\n",
      "→ ↓ ↓ ▢\n",
      "▢ → → ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, policy_iteration_deterministic = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, policy_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(policy_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3731f7-5e61-4b1c-adc9-ce021b8345ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "391cc7348680b297fc58d3be91d0dd5e",
     "grade": true,
     "grade_id": "cell-a422263f5dd5254d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297afae4-639a-4741-811a-1693ce726413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75f8abf27b293dd60179ff3844f5c8f0",
     "grade": false,
     "grade_id": "cell-5e9663f3341ea0cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf17ac-9e34-4b94-a8c0-abefbd22c201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "213cec389fa200d841b8e593f5a6d5f5",
     "grade": false,
     "grade_id": "cell-da562c89000eeffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Neste exercício vamos encontrar uma política ótima para o Frozen Lake utilizando o algoritmo *Value Iteration* como descrito abaixo.\n",
    "\n",
    "![Value Iteration](value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4db7-80f5-4964-aa87-bf561413c5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "905d1ad1a904f82f88c7c68c0ab5341b",
     "grade": false,
     "grade_id": "cell-179ab2fb26b003ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Novamente, vamos dividir este exercícios em etapas menores. O primeiro passo consiste em inicializar o vetor $V$, que armazenará os valores esperados para cada estado. Para isso, implemente a função `init_value_iteration`, que recebe um ambiente como parâmetro e retorna o vetor $V$. Este vetor deve ser inicializado com valores zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "110e2c68-c87e-4342-8ca6-57a43685180b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257304c2b6c04f1ce1743dde7e5095de",
     "grade": false,
     "grade_id": "cell-80a50671b2e72667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_value_iteration(env: gym.Env) -> np.ndarray[float]:\n",
    "    \"\"\"\n",
    "    Initializes the value function (V) array for the Value Iteration algorithm.\n",
    "    This corresponds to the 'Initialization' step in the Value Iteration pseudocode.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "\n",
    "    Returns:\n",
    "        The initialized V-table.\n",
    "    \"\"\"\n",
    "    # Get the total number of states from the environment's observation space.\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the value function array (V-table) with zeros for all states.\n",
    "    # A zero initialization is a standard starting point for this algorithm.\n",
    "    V = np.zeros(n_states, dtype=float)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62953af-0ac2-4064-9731-be0d11e6285a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a6069926e5998ea4f6aa2a76ff753",
     "grade": false,
     "grade_id": "cell-4e33e273961d7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Agora, vamos gerar uma política determinística a partir de um vetor $V$, conforme definido pela equação $\\pi(s)= \\textrm{argmax}_a \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$. Implemente a função `generate_policy`, que recebe um ambiente e um vetor $V$, retornando a política determinística resultante.\n",
    "\n",
    "**Dica:** Utilize a função `compute_expected_value` do exercício anterior para facilitar sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc3a710-e02c-46be-a93b-56683cfe6593",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee4fc2ed2d51ff97989aaddc8706c51",
     "grade": false,
     "grade_id": "cell-41eb7e70f58ee606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_policy(env: gym.Env, V: np.ndarray[float], gamma: float) -> np.ndarray[int]:\n",
    "    \"\"\"\n",
    "    Generates a deterministic policy from a given value function V.\n",
    "    This function implements the policy extraction step shown in the Value Iteration pseudocode.\n",
    "    pi(s) = argmax_a sum_{s', r} p(s', r|s, a)[r + gamma * V(s')]\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        V: The value function array.\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        The resulting deterministic policy array.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "\n",
    "    # Iterate over all states to determine the best action for each.\n",
    "    for state in range(n_states):\n",
    "        # For each state, compute the expected value of taking each possible action.\n",
    "        action_values = [compute_expected_value(env, V, state, action, gamma)\n",
    "                         for action in range(env.action_space.n)]\n",
    "\n",
    "        # The best action is the one that maximizes the expected value.\n",
    "        policy[state] = np.argmax(action_values)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76228-99f7-4eb3-a265-e05e0ade9a3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d08d908f6776a4eecbe68786819e0d9",
     "grade": false,
     "grade_id": "cell-eef5f2b3ef2b657d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Por fim, implemente o loop principal do *Value Iteration* na função `value_iteration`. Ela deverá retornar, nesta ordem, o array de valores $V$ e a política obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a4be75-e22e-40f7-aca3-31f7b810c32d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1207e1ee95de47ccab3943f810509cc9",
     "grade": false,
     "grade_id": "cell-c48b9185009da819",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Performs the Value Iteration algorithm to find the optimal value function and policy.\n",
    "    This implementation directly follows the complete algorithm shown in the pseudocode.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        gamma: The discount factor.\n",
    "        theta: The convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the optimal value function array and the optimal policy array.\n",
    "    \"\"\"\n",
    "    # Initialize the value function.\n",
    "    V = init_value_iteration(env)\n",
    "\n",
    "    while True:\n",
    "        # Initialize delta to track the maximum change in V in a single sweep.\n",
    "        delta = 0\n",
    "\n",
    "        # Iterate through each state for the update.\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Store the old value to measure the change.\n",
    "            old_v = V[state]\n",
    "\n",
    "            # Compute the expected value for all possible actions from the current state.\n",
    "            action_values = [compute_expected_value(env, V, state, action, gamma)\n",
    "                             for action in range(env.action_space.n)]\n",
    "\n",
    "            # The Bellman optimality update: V(s) is updated with the max action value.\n",
    "            # This combines the evaluation and improvement steps into one.\n",
    "            V[state] = np.max(action_values)\n",
    "\n",
    "            # Update delta with the absolute difference.\n",
    "            delta = max(delta, abs(old_v - V[state]))\n",
    "\n",
    "        # If the value function has converged, exit the loop.\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Once the optimal value function is found, extract the optimal policy.\n",
    "    policy = generate_policy(env, V, gamma)\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f621f3f-9261-4f75-93fa-a511c3013aad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3130d567b2d34a7411e80ff13d04238e",
     "grade": false,
     "grade_id": "cell-3a5e438c6988c441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Value Iteration* em um ambiente Frozen Lake determinístico. A política resultante será armazenada a variável `value_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que ele esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b13ad40-daa8-4c50-92c2-92336378d91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a507f4b6da72cd0204ce61ef7d995c7",
     "grade": true,
     "grade_id": "cell-65ff2786217d46c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ → ↓ ←\n",
      "↓ ▢ ↓ ▢\n",
      "→ ↓ ↓ ▢\n",
      "▢ → → ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, value_iteration_deterministic = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, value_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(value_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7814681-8791-4066-9205-6bd78043f7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8477abf2d6223cb005a34bf19346d24f",
     "grade": true,
     "grade_id": "cell-90866ba893778afb",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845b158-b075-4b9e-9da3-cc7cffd82b47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0bf756f1c61d1326fd39f832e2599b0",
     "grade": false,
     "grade_id": "cell-f948772c42437869",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac23f89-42f3-44b3-854b-366657919bfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef88d7295863002c1746176f54048cfb",
     "grade": false,
     "grade_id": "cell-129f7cfccf09f7e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora, executaremos seus algoritmos no mesmo ambiente do Frozen Lake, porém escorregadio. As políticas resultante serão armazenadas nas variáveis `policy_iteration_slippery` e `value_iteration_slippery`, que usaremos na tarefa 14. Nesse cenário, o agente tem apenas 1/3 de chance de se mover na direção desejada e 2/3 de chance de se mover em uma direção perpendicular. Observe as políticas resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4952605d-7df8-4583-ae30-d27f611b8565",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee3c7f66748e1f0c7e5aded56c485dd5",
     "grade": false,
     "grade_id": "cell-51aba37175d6166c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration\n",
      "← ↑ ↑ ↑\n",
      "← ▢ ← ▢\n",
      "↑ ↓ ← ▢\n",
      "▢ → ↓ ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, policy_iteration_slippery = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Policy Iteration\")\n",
    "print_policy(env, policy_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e214b7-6f93-4f64-813f-23fac2d91eab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28ac89d9a717a5ad6077b10991b0ea62",
     "grade": false,
     "grade_id": "cell-1cd7b2d2c9f5a32e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "← ↑ ↑ ↑\n",
      "← ▢ ← ▢\n",
      "↑ ↓ ← ▢\n",
      "▢ → ↓ ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, value_iteration_slippery = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Value Iteration\")\n",
    "print_policy(env, value_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3cef6-fa32-44a7-b1a1-b59415304dcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f3c44171f967cf4a67342ee8dd96568",
     "grade": false,
     "grade_id": "cell-9ebda4da8134aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Implemente a função `execute_policy` abaixo, que deve executar uma política previamente obtida em um ambiente Frozen Lake por $N$ episódios, retornando a recompensa acumulada de cada episódio e suas durações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a263709-5c41-420f-a35e-4ce3bd2b0af0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65dd2151b7b6eb59037e7c206aebf2cf",
     "grade": true,
     "grade_id": "cell-8c6445e2c40cad0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def execute_policy(env: gym.Env, policy: np.ndarray[int], n_episodes):\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select the action for the current state according to the given policy.\n",
    "            # This is the core of executing a deterministic policy.\n",
    "            action = policy[state]\n",
    "\n",
    "            # Take the action in the environment to get the next state, reward, and termination status.\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Accumulate the reward for the current episode.\n",
    "            total_reward += reward\n",
    "\n",
    "            # Increment the step counter.\n",
    "            step_count += 1\n",
    "\n",
    "            # The episode ends if the state is terminal (terminated or truncated).\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Move to the next state for the next iteration.\n",
    "            state = next_state\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        episode_lengths.append(step_count)\n",
    "    return episode_returns, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359818ef-5844-4300-8e03-f989005f5251",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f138e90efe57b4f18d45fa1f9c7da8e3",
     "grade": false,
     "grade_id": "cell-774fba8dd36ffbe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Utilize a função `execute_policy` para avaliar a política obtida pelo Policy Iteration no Frozen Lake **determinístico** (`policy_iteration_deterministic`) em um ambiente Frozen Lake escorregadio por 10 episódios. Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_1_returns` e a duração dos episódios na variável `agent_1_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20dc536-c94c-43fd-9d87-204ea87b04bb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4ae3ec1ce95e41ffe14db005b9ae544",
     "grade": false,
     "grade_id": "cell-5e1db6859a2651d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# Execute the deterministic policy in the slippery environment for 10 episodes.\n",
    "# The results (rewards and lengths) are stored in the specified variables.\n",
    "#\n",
    "agent_1_returns, agent_1_lengths = execute_policy(env, policy_iteration_deterministic, n_episodes=10)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b773d-73f5-4a22-bda0-5d21a1f264d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e0754bfa1c08878b4c14904ee53a0f",
     "grade": false,
     "grade_id": "cell-bde46ff3fce95520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Repita o procedimento da tarefa anterior, desta vez utilizando a política obtida pelo Policy Iteration no Frozen Lake **escorregadio** (`policy_iteration_slippery`). Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_2_returns` e a duração dos episódios na variável `agent_2_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8020b4b0-47d0-4c60-a4f1-7e6b1411ab47",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c50df759f226ac2558ea4d515f343a7",
     "grade": true,
     "grade_id": "cell-d7202ccd7e779b1a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# Execute the slippery-trained policy in the slippery environment for 10 episodes.\n",
    "# This evaluates how the policy optimized for stochasticity performs.\n",
    "agent_2_returns, agent_2_lengths = execute_policy(env, policy_iteration_slippery, n_episodes=10)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f24dc-2e61-4c83-ad77-50638b8c0e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba9c4b3ee4aec555f44237ab98d95d25",
     "grade": false,
     "grade_id": "cell-eaad1d7c23d3bf5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analise a seguinte comparação entre as recompensas e a duração obtidas por cada uma dessas duas execuções no Frozen Lake escorregadio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd1b38f-a55f-4a6a-b334-31380f871153",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbed36db4b67ef8e511dbcec2a153c7f",
     "grade": false,
     "grade_id": "cell-5eec276dbb402e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compare_policy(\n",
    "    rewards_run1, lengths_run1,\n",
    "    rewards_run2, lengths_run2,\n",
    "    label_run1=\"Agent 1\", label_run2=\"Agent 2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two policy runs using mean return and mean episode length.\n",
    "\n",
    "    Args:\n",
    "        rewards_run1 (list): Episode rewards for run 1.\n",
    "        lengths_run1 (list): Episode lengths for run 1.\n",
    "        rewards_run2 (list): Episode rewards for run 2.\n",
    "        lengths_run2 (list): Episode lengths for run 2.\n",
    "        label_run1 (str): Label for run 1.\n",
    "        label_run2 (str): Label for run 2.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_rewards = [np.mean(rewards_run1), np.mean(rewards_run2)]\n",
    "    std_rewards  = [np.std(rewards_run1), np.std(rewards_run2)]\n",
    "\n",
    "    mean_lengths = [np.mean(lengths_run1), np.mean(lengths_run2)]\n",
    "    std_lengths  = [np.std(lengths_run1), np.std(lengths_run2)]\n",
    "\n",
    "    labels = [label_run1, label_run2]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Mean Rewards\n",
    "    axes[0].bar(x, mean_rewards, yerr=std_rewards, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_ylabel(\"Mean Total Reward\")\n",
    "    axes[0].set_title(\"Mean Episode Return ± Std\")\n",
    "    axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # Mean Episode Lengths\n",
    "    axes[1].bar(x, mean_lengths, yerr=std_lengths, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_ylabel(\"Mean Episode Length\")\n",
    "    axes[1].set_title(\"Mean Episode Length ± Std\")\n",
    "    axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_policy(agent_1_returns, agent_1_lengths, agent_2_returns, agent_2_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa0ba7-455a-41c4-97d4-8c17fa8b0e1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd865599e6154e8061df6dcbe6aa7eca",
     "grade": false,
     "grade_id": "cell-f8680351f981a5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "15. Explique quais fatores levaram às diferenças observadas entre as políticas obtidas no ambiente determinístico e no ambiente escorregadio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dd49c-eb7c-41a1-a3d1-ca7ef0818922",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4304d231930b7c1e752d1520d20c830",
     "grade": true,
     "grade_id": "cell-21cbc643c62202d7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. Natureza do Ambiente: Determinismo vs. Estocasticidade O fator principal é a propriedade escorregadia (is_slippery) do ambiente. Ambiente Determinístico não escorregadio (is_slippery=False): neste cenário, a ação escolhida pelo agente leva ao resultado esperado com 100% de certeza. Se o agente decide ir para a \"direita\", ele se move para a direita. A política ótima, portanto, traça o caminho mais curto e direto para o objetivo, evitando apenas os buracos de forma precisa. Não há necessidade de se preocupar com movimentos não intencionais, conforme podemos identificar no gráfico da Figura 6 da Documentação. Ambiente Escorregadio (is_slippery=True): nesse caso, o ambiente é estocástico. Uma ação escolhida tem apenas uma chance de levar ao resultado pretendido, com as demais possibilidades sendo movimentos perpendiculares. Isso introduz um risco significativo: uma ação aparentemente segura pode levar o agente a um buraco.\n",
    "2. Estratégia de Mitigação de Risco: devido à incerteza, a política ótima no ambiente escorregadio muda de uma estratégia de \"caminho mais curto\"para uma de mitigação de risco. Na Política Determinística, a política gerada é \"gananciosa\"e otimista. Ela assume que o controle do agente sobre o ambiente é absoluto e, por isso, se aproxima das bordas e dos buracos sem hesitação para encurtar o caminho. Enquanto, na Política Estocástica, a política gerada é visivelmente mais conservadora. Ela aprende que ações executadas perto de buracos têm uma alta probabilidade de falha, mesmo que a ação \"correta\"aponte para longe do perigo. Por exemplo, no estado 5 (segunda linha, segunda coluna), a política determinística pode seguramente ir para a direita. No entanto, na versão escorregadia, essa ação pode resultar em um movimento para baixo, levando ao buraco no estado 9. A política ótima, então, aprende a evitar essas zonas de risco, preferindo caminhos mais longos, porém mais seguros, que mantêm o agente longe das \"bordas\"dos buracos e até mesmo se chocando contra a parede para se aproveitar do ambiente escorregadio e escorregar para o estado desejado, conforme identifico pelos gráficos da Figura 7 e visto na Figura 3 da Documentação. \n",
    "3. Impacto na Função de Valor: os algoritmos de Policy e Value Iteration buscam maximizar o retorno esperado, conforme vimos nas seções 1.2 e 1.3 da Documentação. No ambiente determinístico, o valor de um estado é alto se ele estiver em um caminho curto para a recompensa. Enquanto que no ambiente estocástico, o valor de um estado (V(s)) passa a ser descontado não apenas pelo fator gamma, mas também pela probabilidade de falha. Estados adjacentes a buracos terão valores inerentemente mais baixos, pois o retorno esperado a partir deles é menor devido à chance de cair e receber recompensa zero. O algoritmo então favorece ações que levam a estados com valores esperados mais altos e mais seguros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4571b7-ff1b-4c70-9d85-e0de016a6bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5292df5ef29eee183f943505f3ec2e4",
     "grade": false,
     "grade_id": "cell-426ab2d2c4253fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "16. Quais estratégias poderiam ser adotadas para tornar o comportamento do agente menos conservador quando treinado no ambiente escorregadio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce5371-a76a-415b-b282-4d4ab80ff056",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbcb94e28a49b89a5fb7b66e4c166a94",
     "grade": true,
     "grade_id": "cell-70d62e2dd7e44e0e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Para induzir um comportamento menos conservador, que aceite mais riscos em troca de caminhos potencialmente mais curtos. Podemos estipular as seguintes estratégias para reduzir o comportamento conservador:\n",
    " 1. Modificação da Função de Recompensa: a principal razão para o conservadorismo é que o custo de cair em um buraco (fim do episódio, recompensa zero) é muito maior do que o benefício de economizar alguns passos. Podemos incentivar caminhos mais curtos introduzindo uma pequena penalidade a cada passo. Em nosso problema, em vez de uma recompensa de 0 para cada transição, poderíamos usar uma recompensa de -0,01, por exemplo. Para usar um exemplo dado em aula, seria como um chão quente, e o agente precisaria encontrar o caminho mais curto para não queimar os pés; uma pressão para que o agente chegue ao objetivo o mais rápido possível para minimizar a penalidade acumulada, forçando-o a considerar caminhos mais curtos e, consequentemente, mais arriscados.\n",
    " 2. Ajuste do Fator de Desconto γ (Gamma): o parâmetro γ (fator de desconto) determina a importância de recompensas futuras no cálculo do ganho total. Um γ próximo de 1 torna o agente \"cauteloso\"(com visão de longo prazo), pois as recompensas distantes são pouco descontadas, levando-o a valorizar o ganho cumulativo total. Por outro lado, um γ menor o tornaria mais \"impaciente\"e \"míope\", focando em ganhos de curto prazo, já que as recompensas futuras perdem seu valor rapidamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
