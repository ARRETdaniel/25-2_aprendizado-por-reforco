{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c18e0f-36ba-4010-8ba6-9ec2c9966e3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eead5b36a3ee2d8eb504615ec2e698b",
     "grade": false,
     "grade_id": "cell-4c87f565d23c35db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lista de Exercícios 1: Processos de Decisão de Markov e Programação Dinâmica\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140e15d-8ff2-404e-9f0b-de4c9444c9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46221e6cdc09eef7a6cc1ae625ff7263",
     "grade": false,
     "grade_id": "cell-e6ec4bda5dd7e12a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd360e8-7428-4cb3-af57-ef79d5c99ef8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de898f61e5161ac2497e0a4c68b4e92e",
     "grade": false,
     "grade_id": "cell-d31fd315f36785a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÕES QUE NÃO SEGUIREM AS INSTRUÇÕES A SEGUIR NÃO SERÃO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido antes de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps1.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 23/09/2025. Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea77-cdbe-474f-bc2f-95daa8d439c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31845cf7b26a7f7728c186a8c96638a3",
     "grade": false,
     "grade_id": "cell-f1f0ba316c0b79ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1557b9-966f-44c4-85be-cb75ffd2c95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1098a937587b35eb2f0dae504db9be3",
     "grade": false,
     "grade_id": "cell-69a0af10519ed240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "O ambiente Frozen Lake é uma simulação clássica utilizada para treinamento de agentes em aprendizado por reforço. Neste ambiente, o agente navega por um lago congelado representado por um grid de tamanho $n \\times m$, com o objetivo de alcançar um alvo. O lago contém dois tipos de células: (1) células com gelo sólido, que são seguras para o agente se mover, e (2) as células com buracos, nas quais o agente cai e falha a missão. Embora o Gymnasium já possua uma implementação do Frozen Lake, neste exercício iremos implementá-lo do zero.\n",
    "\n",
    "No início de cada episódio, o agente é posicionado na célula $[0, 0]$ enquanto o alvo é posicionado na célula mais distante do agente, na posição $[n-1, m-1]$ em um mapa de tamanho $n \\times m$. A cada passo, o agente recebe uma observação indicando sua posição atual no lago e tem a possibilidade de escolher entre quatro ações possíveis: mover-se para cima, para baixo, para a esquerda ou para a direita. No entanto, devido à superfície escorregadia do lago, ele nem sempre se move na direção desejada, podendo acabar se movendo em uma direção perpendicular à escolhida. O agente recebe uma recompensa de 1 se alcançar o alvo e zero em todos os outros estados. Um episódio termina quando o agente alcança o objetivo ou cai na água.\n",
    "\n",
    "Neste exercício, vamos trabalhar sempre com o mesmo mapa $4 \\times 4$, representado na figura abaixo.\n",
    "\n",
    "![Frozen Lake Map](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46123-2985-4e8e-87b5-ed33a8fccfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b35e66b571d4144b5769839b2a1134",
     "grade": false,
     "grade_id": "cell-6e7a4e34e7d477a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa será implementar o ambiente Frozen Lake utilizando o arcabouço fornecido pelo Gymnasium. Abaixo, você encontrará um código inicial que deverá ser utilizado em sua implementação. Siga essas instruções para garantir que seu código está de acordo com o esperado:\n",
    "\n",
    "1. Na função `__init__`, já definimos o mapa que será utilizado e armazenamos essa informação na variável `_description`. Nesse mapa, a letra 'S' representa a posição inicial do agente, a letra 'G' indica o alvo, as letras 'F' representam gelo sólido (que é seguro) e as letras 'H' marcam os buracos. No entanto, ainda é necessário adicionar mais algumas informações no ambiente, especificamente sobre a representação das observações e das ações. Embora existam várias maneiras de representar os espaços de observações e de ação, neste exercício, você deve usar a forma mais simples possível, que pode ser representada por um único valor discreto. Na função `__init__`, defina o espaço de observações e o espaço de ações, atribuindo-os às variáveis `self.observation_space` e `self.action_space`, respectivamente. Utilize apenas a classe `gymnasium.spaces.Discrete` nesta tarefa.\n",
    "\n",
    "2. Antes de prosseguirmos com as funcionalidades do gymnasium, vamos implementar algumas funções auxiliares para facilitar as próximas etapas. Implemente a função `_get_obs`, que retorna a observação atual do ambiente. Além disso, implemente a função `_set_state`, que recebe um valor inteiro correspondente a uma posição no lago e coloca o agente nesta localização.\n",
    "\n",
    "3. A função `reset` deve resetar o ambiente e inicializar um novo episódio, posicionando o agente na célula $[0, 0]$ e fazendo todos os ajustes internos necessários. Esta função deve retornar uma tupla contendo a observação inicial e as informações do ambiente. Neste exercício, vamos retornar um dicionario vazio `{}` para as informações. Lembre-se que a observação deve ser um único valor discreto, como definido no item 1. Implemente a função `reset`.\n",
    "\n",
    "4. A função `step()` é responsável por atualizar o ambiente com base na ação executada pelo agente. Ela recebe como entrada a ação escolhida pelo agente, um parâmetro seed e uma variável options, e calcula o novo estado atual com base na função de transição previamente definida. Neste exercício, você pode ignorar os parâmetros seed e options, pois não precisaremos deles. Neste ambiente que estamos desenvolvendo, o agente tem 80% de chance de se mover na direção desejada e 20% de chance de se mover em uma direção perpendicular à escolhida, distribuída igualmente entre os dois sentidos possíveis (10% para cada um). **As ações do agente devem ser representadas pelos valores 0 (mover-se para a esquerda), 1 (mover-se para baixo), 2 (mover-se para a direita) e 3 (mover-se para cima)**. Caso o agente tente se mover para fora do mapa, ele permanecerá na mesma posição. Além disso, a função atribui uma recompensa ao agente e verifica se o episódio chegou ao fim. Implemente a função step() para que ela retorne a observação do estado atual, a recompensa recebida, um valor booleano indicando se o estado é terminal, um valor booleano informando se o episódio foi truncado e as informações do ambiente. Esses dois últimos valores são necessários devidio à interface estabelecida pelo gymnasium, mas não se preocupe com eles; apenas retorne sempre `False` e `{}` para eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82e329-a086-4479-8375-bb1545073294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d09d0c8292f8dbf0e2bdf379701d722",
     "grade": false,
     "grade_id": "cell-bd1f83bd5e0cfba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e848a9-20d8-4688-a080-306628fa3b9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6883a4d3a67114700d7686062bf221",
     "grade": false,
     "grade_id": "cell-b8e10c8d02b6faa6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self._description = np.asarray([\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ], dtype='c')\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, action):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cae5e-1406-4fa2-9d93-1b9af302ba90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e7b9ba331c5e3fa3a00100a2984bec",
     "grade": false,
     "grade_id": "cell-c99ad325d97fb8d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Certifique-se que seu ambiente funciona na célula abaixo.\n",
    "\n",
    "**Atenção:** os testes fornecidos não cobrem todos os casos possíveis. Realize testes adicionais para garantir a implementação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a5ed-2e90-4ce2-bb76-2b6360055ed9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392f0ae9859df2eb60fbf9f1d3ac80a7",
     "grade": false,
     "grade_id": "cell-ddead0156e8c7432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = FrozenLake()\n",
    "\n",
    "obs, info = env.reset()\n",
    "assert obs == 0, f\"Observação inicial esperada 0, recebeu {obs}\"\n",
    "\n",
    "env._set_state(5)\n",
    "obs = env._get_obs()\n",
    "assert obs == 5, f\"Estado esperado 5, recebeu {obs}\"\n",
    "\n",
    "for _ in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    assert 0 <= action < 4, f\"Ação fora do intervalo esperado: {action}\"\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    assert 0 <= obs < 16, f\"Observação fora do intervalo esperado: {obs}\"\n",
    "    assert reward in [0, 1], f\"Recompensa inválida: {reward}\"\n",
    "    assert isinstance(terminated, bool), f\"'terminated' deve ser bool, mas recebeu {type(terminated)}\"\n",
    "    assert truncated is False, f\"'truncated' deve ser False, mas recebeu {truncated}\"\n",
    "    assert isinstance(info, dict), f\"'info' deve ser dict, mas recebeu {type(info)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8e99e-94bb-426f-a7e0-106dad2769b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78bddd1db45c2e8b9e54c20f0791463",
     "grade": true,
     "grade_id": "cell-20901ef53a25a2b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673e83f-1c2d-4934-bacb-0e879fcf6eb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e93d2945e347d584c03b27844df147f6",
     "grade": true,
     "grade_id": "cell-b30cbb7c1fa808b0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d278f-5787-44d4-95b1-7e537b15db8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977788f0df2f7412652fd64ee795ff21",
     "grade": false,
     "grade_id": "cell-8b132c80e15a2de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a3b1-ce03-42e0-9a4a-c7760aa904be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b330df534aeb8aecd6308dd337e8bc46",
     "grade": false,
     "grade_id": "cell-e6a8d0aebce03142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora que estamos familiarizados com o ambiente Frozen Lake, nosso objetivo será encontrar uma política ótima para ele.  Desta vez, utilizaremos a versão oficial do Frozen Lake, disponibilizado pelo Gymnasium. Ele possui algumas propriedades que facilitarão as próximas implementações. Sua tarefa será implementar o algoritmo *Policy Iteration*, conforme ilustrado abaixo.\n",
    "\n",
    "![Policy Iteration](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cbfcc-d78a-4409-91b7-ae40d26d254c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248bbc9df645c2d3c87ae09f1c751a52",
     "grade": false,
     "grade_id": "cell-80af17ade7f65c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. A implementação será realizada em etapas. Comece implentando a função `init_policy_iteration`, que inicializa e retorna dois arrays. O primeiro array armazenará os valores esperados de cada estado $V(s)$, enquanto o segundo conterá a política do agente: para cada estado, ele indicará a ação que o agente deve realizar. Ambos os arrays devem ser inicializados com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afa70b-1d9e-4cfd-b026-9669cb329d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1465ddc541cd84973baead52e1296a77",
     "grade": false,
     "grade_id": "cell-0fb77ab1fc981da0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_policy_iteration(env: gym.Env) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c76e-1a7b-4fa0-afe7-8aa4296a852a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcda365a4d97947d565e8c44d241610c",
     "grade": false,
     "grade_id": "cell-833ab69b40216f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Agora, vamos computar o valor esperado $V(s) = \\sum_{s', r}p(s',r|s, a)[r + \\gamma V(s')]$. Implemente a função `compute_expected_value`que recebe como parâmetros o ambiente, o vetor $V$, um estado, uma ação, o valor de $\\gamma$ (fator de desconto), e retorna o valor esperado. Não altere os valores de $V$ nesta função.\n",
    "\n",
    "**Importante:** A variável `env.unwrapped.P[state][action]` contém as transições do ambiente, retornando uma lista com todas as transições possíveis para o par (state, action). Cada elemento dessa lista inclui, na seguinte ordem: a probabilidade da transição, o estado $s'$ alcançado, a recompensa recebida e um indicador de estado terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88dd79-de71-4c91-bc21-13f6db8f350e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086e19afee3929471d473c6b70e42a7e",
     "grade": false,
     "grade_id": "cell-fbb3d72642775bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_value(env: gym.Env, V: np.ndarray[float], state: int, action: int, gamma: float) -> float:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c4f38-ec8c-4d43-ac71-68243b719428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd97b9bb7399cf9d8ad4b4f9d324c34",
     "grade": false,
     "grade_id": "cell-b94451eaf15d9d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. O pŕoximo passo será avaliar a política do agente. Implemente o loop de avaliação de política do policy iteration na função `evaluate_policy`. Ela receberá o ambiente, a política do agente, o vetor $V$, o valor $\\gamma$, e o valor $\\theta$. Esta função não precisa retornar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b124c-9ed3-47aa-a7bb-e1a198bdb87b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc74a4d30b19f68b2a836179a817e2f6",
     "grade": false,
     "grade_id": "cell-8df0495bf82eaaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float, theta: float) -> None:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8faf3-7d83-4c24-8f38-110981cf296b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a2d75b60f5350454137839a5e61ce25",
     "grade": false,
     "grade_id": "cell-deee4c0a66b4ef76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. A seguir, vamos implementar a atualização da política. Na função `improve_policy` implemente uma iteração da atualização da política. Ela recebe o ambiente, a política do agente, o vetor $V$, e o valor $\\gamma$. Ela deverá retornar um booleano indicando se política está estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818354d1-3e10-4ac8-96cd-51eff85c54a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338436f555ec731b7fee10a05a929d5",
     "grade": false,
     "grade_id": "cell-4604a015625bbd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886671-c6a1-4bf8-8258-137c8030d8fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e9728ce314ad841c660af9d5745807",
     "grade": false,
     "grade_id": "cell-59ebc175679651bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo implementa a estrutura do algoritmo *Policy Iteration* utilizando as funções desenvolvidas nas etapas anteriores. Não é necessário realizar nenhuma implementação nesta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab5bc4-ac2a-4a79-b5d5-7b6d668805b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc9300dfe827083e071ca31f8ddcefd",
     "grade": false,
     "grade_id": "cell-250e7d4c7bc0cf97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    V, policy = init_policy_iteration(env)\n",
    "    \n",
    "    while True:\n",
    "        evaluate_policy(env, policy, V, gamma, theta)\n",
    "        policy_stable = improve_policy(env, policy, V, gamma)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef24a7-7071-4c51-926c-32b9e9c39c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0803fbd416c4e12f431191163c7a3d",
     "grade": false,
     "grade_id": "cell-ed343128eb786bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_policy(env:gym.Env, policy: np.ndarray[int]):\n",
    "    \"\"\"\n",
    "    Exibe a política de um ambiente FrozenLake de forma visual.\n",
    "\n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    env : gym.Env\n",
    "        Ambiente do tipo FrozenLake.\n",
    "    policy : np.ndarray\n",
    "        Array 1D contendo as ações a serem tomadas em cada estado.\n",
    "\n",
    "    Ações são mapeadas para setas:\n",
    "        0: '←', 1: '↓', 2: '→', 3: '↑'\n",
    "    \n",
    "    Símbolos especiais do mapa:\n",
    "        'H': buraco → '▢'\n",
    "        'G': objetivo → '◎'\n",
    "    \"\"\"\n",
    "    \n",
    "    ACTION_MAP = ['←', '↓', '→', '↑']\n",
    "    HOLE_SYMBOL = '▢'\n",
    "    GOAL_SYMBOL = '◎'\n",
    "    \n",
    "    n_rows, n_cols = env.unwrapped.desc.shape\n",
    "    policy_grid = np.full((n_rows, n_cols), \"\", dtype=str)\n",
    "\n",
    "    for index, action in enumerate(policy):\n",
    "        row, col = divmod(index, 4)\n",
    "        cell = env.unwrapped.desc[row, col]\n",
    "        if cell == b'H':\n",
    "            policy_grid[row, col] = HOLE_SYMBOL\n",
    "        elif cell == b'G':\n",
    "            policy_grid[row, col] = GOAL_SYMBOL\n",
    "        else:\n",
    "            policy_grid[row, col] = ACTION_MAP[action]\n",
    "    \n",
    "    np.savetxt(sys.stdout, policy_grid, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5ed1-6b65-425f-894e-769d90603bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835c3efa9e65c2e9dfb7ad4c8f691ea9",
     "grade": false,
     "grade_id": "cell-98df69850425fb45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Policy Iteration* em um ambiente Frozen Lake determinístico, ou seja, onde o agente não corre o risco de escorregar para direções indesejadas. A política resultante será armazenada na variável `policy_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que o algoritmo esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d8872-c213-44e4-ae3b-d187c7a7f097",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8473ab04eee00f9f4041247dae2091d9",
     "grade": true,
     "grade_id": "cell-d1021093fae62b6c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, policy_iteration_deterministic = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, policy_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(policy_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3731f7-5e61-4b1c-adc9-ce021b8345ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "391cc7348680b297fc58d3be91d0dd5e",
     "grade": true,
     "grade_id": "cell-a422263f5dd5254d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297afae4-639a-4741-811a-1693ce726413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75f8abf27b293dd60179ff3844f5c8f0",
     "grade": false,
     "grade_id": "cell-5e9663f3341ea0cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf17ac-9e34-4b94-a8c0-abefbd22c201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "213cec389fa200d841b8e593f5a6d5f5",
     "grade": false,
     "grade_id": "cell-da562c89000eeffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Neste exercício vamos encontrar uma política ótima para o Frozen Lake utilizando o algoritmo *Value Iteration* como descrito abaixo.\n",
    "\n",
    "![Value Iteration](value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4db7-80f5-4964-aa87-bf561413c5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "905d1ad1a904f82f88c7c68c0ab5341b",
     "grade": false,
     "grade_id": "cell-179ab2fb26b003ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Novamente, vamos dividir este exercícios em etapas menores. O primeiro passo consiste em inicializar o vetor $V$, que armazenará os valores esperados para cada estado. Para isso, implemente a função `init_value_iteration`, que recebe um ambiente como parâmetro e retorna o vetor $V$. Este vetor deve ser inicializado com valores zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e2c68-c87e-4342-8ca6-57a43685180b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257304c2b6c04f1ce1743dde7e5095de",
     "grade": false,
     "grade_id": "cell-80a50671b2e72667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_value_iteration(env: gym.Env) -> np.ndarray[float]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62953af-0ac2-4064-9731-be0d11e6285a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a6069926e5998ea4f6aa2a76ff753",
     "grade": false,
     "grade_id": "cell-4e33e273961d7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Agora, vamos gerar uma política determinística a partir de um vetor $V$, conforme definido pela equação $\\pi(s)= \\textrm{argmax}_a \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$. Implemente a função `generate_policy`, que recebe um ambiente e um vetor $V$, retornando a política determinística resultante.\n",
    "\n",
    "**Dica:** Utilize a função `compute_expected_value` do exercício anterior para facilitar sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a710-e02c-46be-a93b-56683cfe6593",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee4fc2ed2d51ff97989aaddc8706c51",
     "grade": false,
     "grade_id": "cell-41eb7e70f58ee606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_policy(env: gym.Env, V: np.ndarray[float], gamma: float) -> np.ndarray[int]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76228-99f7-4eb3-a265-e05e0ade9a3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d08d908f6776a4eecbe68786819e0d9",
     "grade": false,
     "grade_id": "cell-eef5f2b3ef2b657d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Por fim, implemente o loop principal do *Value Iteration* na função `value_iteration`. Ela deverá retornar, nesta ordem, o array de valores $V$ e a política obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4be75-e22e-40f7-aca3-31f7b810c32d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1207e1ee95de47ccab3943f810509cc9",
     "grade": false,
     "grade_id": "cell-c48b9185009da819",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env: gym.Env, gamma:float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f621f3f-9261-4f75-93fa-a511c3013aad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3130d567b2d34a7411e80ff13d04238e",
     "grade": false,
     "grade_id": "cell-3a5e438c6988c441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Value Iteration* em um ambiente Frozen Lake determinístico. A política resultante será armazenada a variável `value_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que ele esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13ad40-daa8-4c50-92c2-92336378d91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a507f4b6da72cd0204ce61ef7d995c7",
     "grade": true,
     "grade_id": "cell-65ff2786217d46c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, value_iteration_deterministic = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, value_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(value_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7814681-8791-4066-9205-6bd78043f7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8477abf2d6223cb005a34bf19346d24f",
     "grade": true,
     "grade_id": "cell-90866ba893778afb",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845b158-b075-4b9e-9da3-cc7cffd82b47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0bf756f1c61d1326fd39f832e2599b0",
     "grade": false,
     "grade_id": "cell-f948772c42437869",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac23f89-42f3-44b3-854b-366657919bfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef88d7295863002c1746176f54048cfb",
     "grade": false,
     "grade_id": "cell-129f7cfccf09f7e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora, executaremos seus algoritmos no mesmo ambiente do Frozen Lake, porém escorregadio. As políticas resultante serão armazenadas nas variáveis `policy_iteration_slippery` e `value_iteration_slippery`, que usaremos na tarefa 14. Nesse cenário, o agente tem apenas 1/3 de chance de se mover na direção desejada e 2/3 de chance de se mover em uma direção perpendicular. Observe as políticas resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952605d-7df8-4583-ae30-d27f611b8565",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee3c7f66748e1f0c7e5aded56c485dd5",
     "grade": false,
     "grade_id": "cell-51aba37175d6166c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, policy_iteration_slippery = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Policy Iteration\")\n",
    "print_policy(env, policy_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e214b7-6f93-4f64-813f-23fac2d91eab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28ac89d9a717a5ad6077b10991b0ea62",
     "grade": false,
     "grade_id": "cell-1cd7b2d2c9f5a32e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, value_iteration_slippery = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Value Iteration\")\n",
    "print_policy(env, value_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3cef6-fa32-44a7-b1a1-b59415304dcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f3c44171f967cf4a67342ee8dd96568",
     "grade": false,
     "grade_id": "cell-9ebda4da8134aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Implemente a função `execute_policy` abaixo, que deve executar uma política previamente obtida em um ambiente Frozen Lake por $N$ episódios, retornando a recompensa acumulada de cada episódio e suas durações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a263709-5c41-420f-a35e-4ce3bd2b0af0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65dd2151b7b6eb59037e7c206aebf2cf",
     "grade": true,
     "grade_id": "cell-8c6445e2c40cad0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def execute_policy(env: gym.Env, policy: np.ndarray[int], n_episodes):\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        episode_returns.append(total_reward)\n",
    "        episode_lengths.append(step_count)\n",
    "    return episode_returns, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359818ef-5844-4300-8e03-f989005f5251",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f138e90efe57b4f18d45fa1f9c7da8e3",
     "grade": false,
     "grade_id": "cell-774fba8dd36ffbe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Utilize a função `execute_policy` para avaliar a política obtida pelo Policy Iteration no Frozen Lake **determinístico** (`policy_iteration_deterministic`) em um ambiente Frozen Lake escorregadio por 10 episódios. Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_1_returns` e a duração dos episódios na variável `agent_1_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dc536-c94c-43fd-9d87-204ea87b04bb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4ae3ec1ce95e41ffe14db005b9ae544",
     "grade": false,
     "grade_id": "cell-5e1db6859a2651d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b773d-73f5-4a22-bda0-5d21a1f264d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e0754bfa1c08878b4c14904ee53a0f",
     "grade": false,
     "grade_id": "cell-bde46ff3fce95520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Repita o procedimento da tarefa anterior, desta vez utilizando a política obtida pelo Policy Iteration no Frozen Lake **escorregadio** (`policy_iteration_slippery`). Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_2_returns` e a duração dos episódios na variável `agent_2_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020b4b0-47d0-4c60-a4f1-7e6b1411ab47",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c50df759f226ac2558ea4d515f343a7",
     "grade": true,
     "grade_id": "cell-d7202ccd7e779b1a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f24dc-2e61-4c83-ad77-50638b8c0e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba9c4b3ee4aec555f44237ab98d95d25",
     "grade": false,
     "grade_id": "cell-eaad1d7c23d3bf5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analise a seguinte comparação entre as recompensas e a duração obtidas por cada uma dessas duas execuções no Frozen Lake escorregadio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1b38f-a55f-4a6a-b334-31380f871153",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbed36db4b67ef8e511dbcec2a153c7f",
     "grade": false,
     "grade_id": "cell-5eec276dbb402e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def compare_policy(\n",
    "    rewards_run1, lengths_run1,\n",
    "    rewards_run2, lengths_run2,\n",
    "    label_run1=\"Agent 1\", label_run2=\"Agent 2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two policy runs using mean return and mean episode length.\n",
    "\n",
    "    Args:\n",
    "        rewards_run1 (list): Episode rewards for run 1.\n",
    "        lengths_run1 (list): Episode lengths for run 1.\n",
    "        rewards_run2 (list): Episode rewards for run 2.\n",
    "        lengths_run2 (list): Episode lengths for run 2.\n",
    "        label_run1 (str): Label for run 1.\n",
    "        label_run2 (str): Label for run 2.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_rewards = [np.mean(rewards_run1), np.mean(rewards_run2)]\n",
    "    std_rewards  = [np.std(rewards_run1), np.std(rewards_run2)]\n",
    "\n",
    "    mean_lengths = [np.mean(lengths_run1), np.mean(lengths_run2)]\n",
    "    std_lengths  = [np.std(lengths_run1), np.std(lengths_run2)]\n",
    "\n",
    "    labels = [label_run1, label_run2]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Mean Rewards\n",
    "    axes[0].bar(x, mean_rewards, yerr=std_rewards, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_ylabel(\"Mean Total Reward\")\n",
    "    axes[0].set_title(\"Mean Episode Return ± Std\")\n",
    "    axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # Mean Episode Lengths\n",
    "    axes[1].bar(x, mean_lengths, yerr=std_lengths, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_ylabel(\"Mean Episode Length\")\n",
    "    axes[1].set_title(\"Mean Episode Length ± Std\")\n",
    "    axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_policy(agent_1_returns, agent_1_lengths, agent_2_returns, agent_2_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa0ba7-455a-41c4-97d4-8c17fa8b0e1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd865599e6154e8061df6dcbe6aa7eca",
     "grade": false,
     "grade_id": "cell-f8680351f981a5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "15. Explique quais fatores levaram às diferenças observadas entre as políticas obtidas no ambiente determinístico e no ambiente escorregadio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dd49c-eb7c-41a1-a3d1-ca7ef0818922",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4304d231930b7c1e752d1520d20c830",
     "grade": true,
     "grade_id": "cell-21cbc643c62202d7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4571b7-ff1b-4c70-9d85-e0de016a6bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5292df5ef29eee183f943505f3ec2e4",
     "grade": false,
     "grade_id": "cell-426ab2d2c4253fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "16. Quais estratégias poderiam ser adotadas para tornar o comportamento do agente menos conservador quando treinado no ambiente escorregadio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce5371-a76a-415b-b282-4d4ab80ff056",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbcb94e28a49b89a5fb7b66e4c166a94",
     "grade": true,
     "grade_id": "cell-70d62e2dd7e44e0e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
