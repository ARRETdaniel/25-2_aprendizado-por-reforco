{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c18e0f-36ba-4010-8ba6-9ec2c9966e3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eead5b36a3ee2d8eb504615ec2e698b",
     "grade": false,
     "grade_id": "cell-4c87f565d23c35db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lista de Exercícios 1: Processos de Decisão de Markov e Programação Dinâmica\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140e15d-8ff2-404e-9f0b-de4c9444c9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46221e6cdc09eef7a6cc1ae625ff7263",
     "grade": false,
     "grade_id": "cell-e6ec4bda5dd7e12a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd360e8-7428-4cb3-af57-ef79d5c99ef8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de898f61e5161ac2497e0a4c68b4e92e",
     "grade": false,
     "grade_id": "cell-d31fd315f36785a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÕES QUE NÃO SEGUIREM AS INSTRUÇÕES A SEGUIR NÃO SERÃO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido antes de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps1.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 23/09/2025. Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea77-cdbe-474f-bc2f-95daa8d439c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31845cf7b26a7f7728c186a8c96638a3",
     "grade": false,
     "grade_id": "cell-f1f0ba316c0b79ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1557b9-966f-44c4-85be-cb75ffd2c95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1098a937587b35eb2f0dae504db9be3",
     "grade": false,
     "grade_id": "cell-69a0af10519ed240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "O ambiente Frozen Lake é uma simulação clássica utilizada para treinamento de agentes em aprendizado por reforço. Neste ambiente, o agente navega por um lago congelado representado por um grid de tamanho $n \\times m$, com o objetivo de alcançar um alvo. O lago contém dois tipos de células: (1) células com gelo sólido, que são seguras para o agente se mover, e (2) as células com buracos, nas quais o agente cai e falha a missão. Embora o Gymnasium já possua uma implementação do Frozen Lake, neste exercício iremos implementá-lo do zero.\n",
    "\n",
    "No início de cada episódio, o agente é posicionado na célula $[0, 0]$ enquanto o alvo é posicionado na célula mais distante do agente, na posição $[n-1, m-1]$ em um mapa de tamanho $n \\times m$. A cada passo, o agente recebe uma observação indicando sua posição atual no lago e tem a possibilidade de escolher entre quatro ações possíveis: mover-se para cima, para baixo, para a esquerda ou para a direita. No entanto, devido à superfície escorregadia do lago, ele nem sempre se move na direção desejada, podendo acabar se movendo em uma direção perpendicular à escolhida. O agente recebe uma recompensa de 1 se alcançar o alvo e zero em todos os outros estados. Um episódio termina quando o agente alcança o objetivo ou cai na água.\n",
    "\n",
    "Neste exercício, vamos trabalhar sempre com o mesmo mapa $4 \\times 4$, representado na figura abaixo.\n",
    "\n",
    "![Frozen Lake Map](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46123-2985-4e8e-87b5-ed33a8fccfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b35e66b571d4144b5769839b2a1134",
     "grade": false,
     "grade_id": "cell-6e7a4e34e7d477a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa será implementar o ambiente Frozen Lake utilizando o arcabouço fornecido pelo Gymnasium. Abaixo, você encontrará um código inicial que deverá ser utilizado em sua implementação. Siga essas instruções para garantir que seu código está de acordo com o esperado:\n",
    "\n",
    "1. Na função `__init__`, já definimos o mapa que será utilizado e armazenamos essa informação na variável `_description`. Nesse mapa, a letra 'S' representa a posição inicial do agente, a letra 'G' indica o alvo, as letras 'F' representam gelo sólido (que é seguro) e as letras 'H' marcam os buracos. No entanto, ainda é necessário adicionar mais algumas informações no ambiente, especificamente sobre a representação das observações e das ações. Embora existam várias maneiras de representar os espaços de observações e de ação, neste exercício, você deve usar a forma mais simples possível, que pode ser representada por um único valor discreto. Na função `__init__`, defina o espaço de observações e o espaço de ações, atribuindo-os às variáveis `self.observation_space` e `self.action_space`, respectivamente. Utilize apenas a classe `gymnasium.spaces.Discrete` nesta tarefa.\n",
    "\n",
    "2. Antes de prosseguirmos com as funcionalidades do gymnasium, vamos implementar algumas funções auxiliares para facilitar as próximas etapas. Implemente a função `_get_obs`, que retorna a observação atual do ambiente. Além disso, implemente a função `_set_state`, que recebe um valor inteiro correspondente a uma posição no lago e coloca o agente nesta localização.\n",
    "\n",
    "3. A função `reset` deve resetar o ambiente e inicializar um novo episódio, posicionando o agente na célula $[0, 0]$ e fazendo todos os ajustes internos necessários. Esta função deve retornar uma tupla contendo a observação inicial e as informações do ambiente. Neste exercício, vamos retornar um dicionario vazio `{}` para as informações. Lembre-se que a observação deve ser um único valor discreto, como definido no item 1. Implemente a função `reset`.\n",
    "\n",
    "4. A função `step()` é responsável por atualizar o ambiente com base na ação executada pelo agente. Ela recebe como entrada a ação escolhida pelo agente, um parâmetro seed e uma variável options, e calcula o novo estado atual com base na função de transição previamente definida. Neste exercício, você pode ignorar os parâmetros seed e options, pois não precisaremos deles. Neste ambiente que estamos desenvolvendo, o agente tem 80% de chance de se mover na direção desejada e 20% de chance de se mover em uma direção perpendicular à escolhida, distribuída igualmente entre os dois sentidos possíveis (10% para cada um). **As ações do agente devem ser representadas pelos valores 0 (mover-se para a esquerda), 1 (mover-se para baixo), 2 (mover-se para a direita) e 3 (mover-se para cima)**. Caso o agente tente se mover para fora do mapa, ele permanecerá na mesma posição. Além disso, a função atribui uma recompensa ao agente e verifica se o episódio chegou ao fim. Implemente a função step() para que ela retorne a observação do estado atual, a recompensa recebida, um valor booleano indicando se o estado é terminal, um valor booleano informando se o episódio foi truncado e as informações do ambiente. Esses dois últimos valores são necessários devidio à interface estabelecida pelo gymnasium, mas não se preocupe com eles; apenas retorne sempre `False` e `{}` para eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c82e329-a086-4479-8375-bb1545073294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d09d0c8292f8dbf0e2bdf379701d722",
     "grade": false,
     "grade_id": "cell-bd1f83bd5e0cfba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e848a9-20d8-4688-a080-306628fa3b9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6883a4d3a67114700d7686062bf221",
     "grade": false,
     "grade_id": "cell-b8e10c8d02b6faa6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Frozen Lake environment.\n",
    "        [cite_start]This follows the setup for a standard MDP as described in the course materials. [cite: 1773, 1779]\n",
    "        \"\"\"\n",
    "        self._description = np.asarray([\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ], dtype='c')\n",
    "\n",
    "        # Get the dimensions of the grid from the description.\n",
    "        self.n_rows, self.n_cols = self._description.shape\n",
    "\n",
    "        # The number of states is the total number of cells in the grid.\n",
    "        n_states = self.n_rows * self.n_cols\n",
    "        # There are four discrete actions: Left, Down, Right, Up.\n",
    "        n_actions = 4\n",
    "\n",
    "        # Define the observation space as a discrete set of states from 0 to n_states-1.\n",
    "        # [cite_start]This is a standard representation for tabular methods. [cite: 1789]\n",
    "        self.observation_space = gym.spaces.Discrete(n_states)\n",
    "\n",
    "        # Define the action space as a discrete set of actions from 0 to n_actions-1.\n",
    "        self.action_space = gym.spaces.Discrete(n_actions)\n",
    "\n",
    "        # Initialize the agent's position. This will be updated in reset().\n",
    "        self._agent_pos = 0\n",
    "\n",
    "    def _get_obs(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the current observation of the environment, which is the agent's position.\n",
    "        \"\"\"\n",
    "        # The observation is the integer representation of the agent's current state.\n",
    "        return self._agent_pos\n",
    "\n",
    "    def _set_state(self, state: int):\n",
    "        \"\"\"\n",
    "        Sets the agent's current state (position).\n",
    "\n",
    "        Args:\n",
    "            state (int): The integer representing the new state.\n",
    "        \"\"\"\n",
    "        # Validate that the state is within the bounds of the observation space.\n",
    "        if not self.observation_space.contains(state):\n",
    "            raise ValueError(f\"Invalid state {state} for this environment.\")\n",
    "\n",
    "        # Update the agent's position.\n",
    "        self._agent_pos = state\n",
    "\n",
    "    def reset(self, seed: int = None, options: dict = None) -> tuple[int, dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state for a new episode.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the initial observation and an empty info dictionary.\n",
    "        \"\"\"\n",
    "        # The super().reset() call handles the seeding of the random number generator.\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # The agent always starts at state 0, corresponding to the 'S' tile.\n",
    "        self._agent_pos = 0\n",
    "\n",
    "        # Return the initial observation and an empty info dictionary as per Gymnasium API.\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool, dict]:\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment based on the given action.\n",
    "        [cite_start]This function implements the transition probability p(s', r|s, a). [cite: 1781]\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the next observation, reward, terminated flag, truncated flag, and an empty info dictionary.\n",
    "        \"\"\"\n",
    "        # Mapping of actions to coordinate changes (row, col).\n",
    "        # 0: Left, 1: Down, 2: Right, 3: Up\n",
    "        action_to_delta = {\n",
    "            0: (0, -1),  # Left\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, 1),   # Right\n",
    "            3: (-1, 0)   # Up\n",
    "        }\n",
    "\n",
    "        # Define probabilities for the slippery (stochastic) transitions.\n",
    "        # The agent moves in the intended direction with 80% probability.\n",
    "        # The remaining 20% is split between the two perpendicular directions.\n",
    "        intended_prob = 0.8\n",
    "        perp_prob = 0.1\n",
    "\n",
    "        # The two perpendicular actions for each intended action.\n",
    "        perp_actions = {0: [3, 1], 1: [0, 2], 2: [1, 3], 3: [0, 1]}\n",
    "\n",
    "        # Determine the actual direction of movement based on stochasticity.\n",
    "        if self.np_random.random() < intended_prob:\n",
    "            # Move in the intended direction.\n",
    "            move = action\n",
    "        else:\n",
    "            # Slip and move in one of the perpendicular directions.\n",
    "            move = self.np_random.choice(perp_actions[action])\n",
    "\n",
    "        # Get the current position (row, col) from the agent's state.\n",
    "        current_row, current_col = divmod(self._agent_pos, self.n_cols)\n",
    "\n",
    "        # Calculate the potential new position based on the chosen move.\n",
    "        delta_row, delta_col = action_to_delta[move]\n",
    "        new_row = current_row + delta_row\n",
    "        new_col = current_col + delta_col\n",
    "\n",
    "        # Check for boundary conditions. If the agent hits a wall, it stays put.\n",
    "        if not (0 <= new_row < self.n_rows and 0 <= new_col < self.n_cols):\n",
    "            new_row, new_col = current_row, current_col\n",
    "\n",
    "        # Update the agent's state (position).\n",
    "        self._agent_pos = new_row * self.n_cols + new_col\n",
    "\n",
    "        # Determine the outcome of the move.\n",
    "        cell_type = self._description[new_row, new_col]\n",
    "\n",
    "        # A terminal state is reached if the agent is at the goal or in a hole.\n",
    "        terminated = (cell_type == b'G' or cell_type == b'H')\n",
    "\n",
    "        # Reward is 1 only if the goal is reached.\n",
    "        reward = 1.0 if cell_type == b'G' else 0.0\n",
    "\n",
    "        # Truncation is not used in this environment.\n",
    "        truncated = False\n",
    "\n",
    "        # The info dictionary is empty.\n",
    "        info = {}\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cae5e-1406-4fa2-9d93-1b9af302ba90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e7b9ba331c5e3fa3a00100a2984bec",
     "grade": false,
     "grade_id": "cell-c99ad325d97fb8d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Certifique-se que seu ambiente funciona na célula abaixo.\n",
    "\n",
    "**Atenção:** os testes fornecidos não cobrem todos os casos possíveis. Realize testes adicionais para garantir a implementação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2848a5ed-2e90-4ce2-bb76-2b6360055ed9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392f0ae9859df2eb60fbf9f1d3ac80a7",
     "grade": false,
     "grade_id": "cell-ddead0156e8c7432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = FrozenLake()\n",
    "\n",
    "obs, info = env.reset()\n",
    "assert obs == 0, f\"Observação inicial esperada 0, recebeu {obs}\"\n",
    "\n",
    "env._set_state(5)\n",
    "obs = env._get_obs()\n",
    "assert obs == 5, f\"Estado esperado 5, recebeu {obs}\"\n",
    "\n",
    "for _ in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    assert 0 <= action < 4, f\"Ação fora do intervalo esperado: {action}\"\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    assert 0 <= obs < 16, f\"Observação fora do intervalo esperado: {obs}\"\n",
    "    assert reward in [0, 1], f\"Recompensa inválida: {reward}\"\n",
    "    assert isinstance(terminated, bool), f\"'terminated' deve ser bool, mas recebeu {type(terminated)}\"\n",
    "    assert truncated is False, f\"'truncated' deve ser False, mas recebeu {truncated}\"\n",
    "    assert isinstance(info, dict), f\"'info' deve ser dict, mas recebeu {type(info)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb8e99e-94bb-426f-a7e0-106dad2769b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78bddd1db45c2e8b9e54c20f0791463",
     "grade": true,
     "grade_id": "cell-20901ef53a25a2b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3673e83f-1c2d-4934-bacb-0e879fcf6eb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e93d2945e347d584c03b27844df147f6",
     "grade": true,
     "grade_id": "cell-b30cbb7c1fa808b0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d278f-5787-44d4-95b1-7e537b15db8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977788f0df2f7412652fd64ee795ff21",
     "grade": false,
     "grade_id": "cell-8b132c80e15a2de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a3b1-ce03-42e0-9a4a-c7760aa904be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b330df534aeb8aecd6308dd337e8bc46",
     "grade": false,
     "grade_id": "cell-e6a8d0aebce03142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora que estamos familiarizados com o ambiente Frozen Lake, nosso objetivo será encontrar uma política ótima para ele.  Desta vez, utilizaremos a versão oficial do Frozen Lake, disponibilizado pelo Gymnasium. Ele possui algumas propriedades que facilitarão as próximas implementações. Sua tarefa será implementar o algoritmo *Policy Iteration*, conforme ilustrado abaixo.\n",
    "\n",
    "![Policy Iteration](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cbfcc-d78a-4409-91b7-ae40d26d254c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248bbc9df645c2d3c87ae09f1c751a52",
     "grade": false,
     "grade_id": "cell-80af17ade7f65c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. A implementação será realizada em etapas. Comece implentando a função `init_policy_iteration`, que inicializa e retorna dois arrays. O primeiro array armazenará os valores esperados de cada estado $V(s)$, enquanto o segundo conterá a política do agente: para cada estado, ele indicará a ação que o agente deve realizar. Ambos os arrays devem ser inicializados com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8afa70b-1d9e-4cfd-b026-9669cb329d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1465ddc541cd84973baead52e1296a77",
     "grade": false,
     "grade_id": "cell-0fb77ab1fc981da0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_policy_iteration(env: gym.Env) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Initializes the value function (V) and policy arrays for the Policy Iteration algorithm.\n",
    "    This corresponds to the 'Initialization' step in the Policy Iteration pseudocode.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the initialized V-table and policy array.\n",
    "    \"\"\"\n",
    "    # Get the total number of states from the environment's observation space.\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the value function array (V-table) with zeros for all states.\n",
    "    # This provides a neutral starting point for evaluation.\n",
    "    V = np.zeros(n_states, dtype=float)\n",
    "\n",
    "    # Initialize the policy array with zeros. This means the initial policy\n",
    "    # is to take action 0 (Left) for all states, but it will be updated.\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c76e-1a7b-4fa0-afe7-8aa4296a852a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcda365a4d97947d565e8c44d241610c",
     "grade": false,
     "grade_id": "cell-833ab69b40216f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Agora, vamos computar o valor esperado $V(s) = \\sum_{s', r}p(s',r|s, a)[r + \\gamma V(s')]$. Implemente a função `compute_expected_value`que recebe como parâmetros o ambiente, o vetor $V$, um estado, uma ação, o valor de $\\gamma$ (fator de desconto), e retorna o valor esperado. Não altere os valores de $V$ nesta função.\n",
    "\n",
    "**Importante:** A variável `env.unwrapped.P[state][action]` contém as transições do ambiente, retornando uma lista com todas as transições possíveis para o par (state, action). Cada elemento dessa lista inclui, na seguinte ordem: a probabilidade da transição, o estado $s'$ alcançado, a recompensa recebida e um indicador de estado terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca88dd79-de71-4c91-bc21-13f6db8f350e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086e19afee3929471d473c6b70e42a7e",
     "grade": false,
     "grade_id": "cell-fbb3d72642775bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_value(env: gym.Env, V: np.ndarray[float], state: int, action: int, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the expected value of a state-action pair, q(s, a).\n",
    "    This function calculates the sum over all possible next states and rewards:\n",
    "    sum_{s', r} p(s', r|s, a)[r + gamma * V(s')].\n",
    "    This is a core calculation used in both policy evaluation and improvement.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        V: The current value function array.\n",
    "        state: The current state (s).\n",
    "        action: The action being evaluated (a).\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        The expected value (q-value) of the state-action pair.\n",
    "    \"\"\"\n",
    "    expected_value = 0.0\n",
    "\n",
    "    # env.unwrapped.P[state][action] provides the transition model p(s', r|s, a).\n",
    "    # It returns a list of tuples: (probability, next_state, reward, terminated_flag).\n",
    "    for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "        # This line directly implements the formula's core component.\n",
    "        expected_value += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c4f38-ec8c-4d43-ac71-68243b719428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd97b9bb7399cf9d8ad4b4f9d324c34",
     "grade": false,
     "grade_id": "cell-b94451eaf15d9d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. O pŕoximo passo será avaliar a política do agente. Implemente o loop de avaliação de política do policy iteration na função `evaluate_policy`. Ela receberá o ambiente, a política do agente, o vetor $V$, o valor $\\gamma$, e o valor $\\theta$. Esta função não precisa retornar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705b124c-9ed3-47aa-a7bb-e1a198bdb87b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc74a4d30b19f68b2a836179a817e2f6",
     "grade": false,
     "grade_id": "cell-8df0495bf82eaaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float, theta: float) -> None:\n",
    "    \"\"\"\n",
    "    Performs the Policy Evaluation step of the Policy Iteration algorithm.\n",
    "    It iteratively updates the value function V for a given policy until the\n",
    "    change in value is smaller than the threshold theta.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        policy: The policy to be evaluated.\n",
    "        V: The value function array to be updated.\n",
    "        gamma: The discount factor.\n",
    "        theta: The convergence threshold.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Initialize delta to track the maximum change in V in a single sweep.\n",
    "        delta = 0\n",
    "\n",
    "        # Iterate through all states in the environment.\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Store the old value of the state to measure the change.\n",
    "            old_v = V[state]\n",
    "\n",
    "            # Get the action to be taken in the current state according to the policy.\n",
    "            action = policy[state]\n",
    "\n",
    "            # Update the value of the current state using the Bellman equation for v_pi.\n",
    "            # This is an 'expected update' as it averages over all possible outcomes.\n",
    "            V[state] = compute_expected_value(env, V, state, action, gamma)\n",
    "\n",
    "            # Update delta with the absolute difference.\n",
    "            delta = max(delta, abs(old_v - V[state]))\n",
    "\n",
    "        # If the maximum change is less than the threshold, the value function has converged.\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8faf3-7d83-4c24-8f38-110981cf296b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a2d75b60f5350454137839a5e61ce25",
     "grade": false,
     "grade_id": "cell-deee4c0a66b4ef76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. A seguir, vamos implementar a atualização da política. Na função `improve_policy` implemente uma iteração da atualização da política. Ela recebe o ambiente, a política do agente, o vetor $V$, e o valor $\\gamma$. Ela deverá retornar um booleano indicando se política está estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818354d1-3e10-4ac8-96cd-51eff85c54a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338436f555ec731b7fee10a05a929d5",
     "grade": false,
     "grade_id": "cell-4604a015625bbd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float) -> bool:\n",
    "    \"\"\"\n",
    "    Performs the Policy Improvement step of the Policy Iteration algorithm.\n",
    "    It updates the policy to be greedy with respect to the current value function V.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        policy: The policy array to be updated.\n",
    "        V: The current value function array.\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        A boolean indicating whether the policy remained stable (True) or was changed (False).\n",
    "    \"\"\"\n",
    "    policy_stable = True\n",
    "\n",
    "    # Iterate through all states to update the policy for each one.\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Store the action specified by the old policy.\n",
    "        old_action = policy[state]\n",
    "\n",
    "        # Find the best action by calculating the expected value for all possible actions\n",
    "        # and selecting the one with the highest value (argmax).\n",
    "        action_values = [compute_expected_value(env, V, state, action, gamma) for action in range(env.action_space.n)]\n",
    "        policy[state] = np.argmax(action_values)\n",
    "\n",
    "        # If the action for this state has changed, the policy is not yet stable.\n",
    "        if old_action != policy[state]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886671-c6a1-4bf8-8258-137c8030d8fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e9728ce314ad841c660af9d5745807",
     "grade": false,
     "grade_id": "cell-59ebc175679651bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo implementa a estrutura do algoritmo *Policy Iteration* utilizando as funções desenvolvidas nas etapas anteriores. Não é necessário realizar nenhuma implementação nesta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aab5bc4-ac2a-4a79-b5d5-7b6d668805b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc9300dfe827083e071ca31f8ddcefd",
     "grade": false,
     "grade_id": "cell-250e7d4c7bc0cf97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    V, policy = init_policy_iteration(env)\n",
    "\n",
    "    while True:\n",
    "        evaluate_policy(env, policy, V, gamma, theta)\n",
    "        policy_stable = improve_policy(env, policy, V, gamma)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ef24a7-7071-4c51-926c-32b9e9c39c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0803fbd416c4e12f431191163c7a3d",
     "grade": false,
     "grade_id": "cell-ed343128eb786bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_policy(env:gym.Env, policy: np.ndarray[int]):\n",
    "    \"\"\"\n",
    "    Exibe a política de um ambiente FrozenLake de forma visual.\n",
    "\n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    env : gym.Env\n",
    "        Ambiente do tipo FrozenLake.\n",
    "    policy : np.ndarray\n",
    "        Array 1D contendo as ações a serem tomadas em cada estado.\n",
    "\n",
    "    Ações são mapeadas para setas:\n",
    "        0: '←', 1: '↓', 2: '→', 3: '↑'\n",
    "\n",
    "    Símbolos especiais do mapa:\n",
    "        'H': buraco → '▢'\n",
    "        'G': objetivo → '◎'\n",
    "    \"\"\"\n",
    "\n",
    "    ACTION_MAP = ['←', '↓', '→', '↑']\n",
    "    HOLE_SYMBOL = '▢'\n",
    "    GOAL_SYMBOL = '◎'\n",
    "\n",
    "    n_rows, n_cols = env.unwrapped.desc.shape\n",
    "    policy_grid = np.full((n_rows, n_cols), \"\", dtype=str)\n",
    "\n",
    "    for index, action in enumerate(policy):\n",
    "        row, col = divmod(index, 4)\n",
    "        cell = env.unwrapped.desc[row, col]\n",
    "        if cell == b'H':\n",
    "            policy_grid[row, col] = HOLE_SYMBOL\n",
    "        elif cell == b'G':\n",
    "            policy_grid[row, col] = GOAL_SYMBOL\n",
    "        else:\n",
    "            policy_grid[row, col] = ACTION_MAP[action]\n",
    "\n",
    "    np.savetxt(sys.stdout, policy_grid, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5ed1-6b65-425f-894e-769d90603bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835c3efa9e65c2e9dfb7ad4c8f691ea9",
     "grade": false,
     "grade_id": "cell-98df69850425fb45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Policy Iteration* em um ambiente Frozen Lake determinístico, ou seja, onde o agente não corre o risco de escorregar para direções indesejadas. A política resultante será armazenada na variável `policy_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que o algoritmo esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1d8872-c213-44e4-ae3b-d187c7a7f097",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8473ab04eee00f9f4041247dae2091d9",
     "grade": true,
     "grade_id": "cell-d1021093fae62b6c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ → ↓ ←\n",
      "↓ ▢ ↓ ▢\n",
      "→ ↓ ↓ ▢\n",
      "▢ → → ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, policy_iteration_deterministic = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, policy_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(policy_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3731f7-5e61-4b1c-adc9-ce021b8345ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "391cc7348680b297fc58d3be91d0dd5e",
     "grade": true,
     "grade_id": "cell-a422263f5dd5254d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297afae4-639a-4741-811a-1693ce726413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75f8abf27b293dd60179ff3844f5c8f0",
     "grade": false,
     "grade_id": "cell-5e9663f3341ea0cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf17ac-9e34-4b94-a8c0-abefbd22c201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "213cec389fa200d841b8e593f5a6d5f5",
     "grade": false,
     "grade_id": "cell-da562c89000eeffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Neste exercício vamos encontrar uma política ótima para o Frozen Lake utilizando o algoritmo *Value Iteration* como descrito abaixo.\n",
    "\n",
    "![Value Iteration](value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4db7-80f5-4964-aa87-bf561413c5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "905d1ad1a904f82f88c7c68c0ab5341b",
     "grade": false,
     "grade_id": "cell-179ab2fb26b003ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Novamente, vamos dividir este exercícios em etapas menores. O primeiro passo consiste em inicializar o vetor $V$, que armazenará os valores esperados para cada estado. Para isso, implemente a função `init_value_iteration`, que recebe um ambiente como parâmetro e retorna o vetor $V$. Este vetor deve ser inicializado com valores zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "110e2c68-c87e-4342-8ca6-57a43685180b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257304c2b6c04f1ce1743dde7e5095de",
     "grade": false,
     "grade_id": "cell-80a50671b2e72667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_value_iteration(env: gym.Env) -> np.ndarray[float]:\n",
    "    \"\"\"\n",
    "    Initializes the value function (V) array for the Value Iteration algorithm.\n",
    "    This corresponds to the 'Initialization' step in the Value Iteration pseudocode.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "\n",
    "    Returns:\n",
    "        The initialized V-table.\n",
    "    \"\"\"\n",
    "    # Get the total number of states from the environment's observation space.\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the value function array (V-table) with zeros for all states.\n",
    "    # A zero initialization is a standard starting point for this algorithm.\n",
    "    V = np.zeros(n_states, dtype=float)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62953af-0ac2-4064-9731-be0d11e6285a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a6069926e5998ea4f6aa2a76ff753",
     "grade": false,
     "grade_id": "cell-4e33e273961d7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Agora, vamos gerar uma política determinística a partir de um vetor $V$, conforme definido pela equação $\\pi(s)= \\textrm{argmax}_a \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$. Implemente a função `generate_policy`, que recebe um ambiente e um vetor $V$, retornando a política determinística resultante.\n",
    "\n",
    "**Dica:** Utilize a função `compute_expected_value` do exercício anterior para facilitar sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc3a710-e02c-46be-a93b-56683cfe6593",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee4fc2ed2d51ff97989aaddc8706c51",
     "grade": false,
     "grade_id": "cell-41eb7e70f58ee606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_policy(env: gym.Env, V: np.ndarray[float], gamma: float) -> np.ndarray[int]:\n",
    "    \"\"\"\n",
    "    Generates a deterministic policy from a given value function V.\n",
    "    This function implements the policy extraction step shown in the Value Iteration pseudocode.\n",
    "    pi(s) = argmax_a sum_{s', r} p(s', r|s, a)[r + gamma * V(s')]\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        V: The value function array.\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        The resulting deterministic policy array.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "\n",
    "    # Iterate over all states to determine the best action for each.\n",
    "    for state in range(n_states):\n",
    "        # For each state, compute the expected value of taking each possible action.\n",
    "        action_values = [compute_expected_value(env, V, state, action, gamma)\n",
    "                         for action in range(env.action_space.n)]\n",
    "\n",
    "        # The best action is the one that maximizes the expected value.\n",
    "        policy[state] = np.argmax(action_values)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76228-99f7-4eb3-a265-e05e0ade9a3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d08d908f6776a4eecbe68786819e0d9",
     "grade": false,
     "grade_id": "cell-eef5f2b3ef2b657d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Por fim, implemente o loop principal do *Value Iteration* na função `value_iteration`. Ela deverá retornar, nesta ordem, o array de valores $V$ e a política obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a4be75-e22e-40f7-aca3-31f7b810c32d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1207e1ee95de47ccab3943f810509cc9",
     "grade": false,
     "grade_id": "cell-c48b9185009da819",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Performs the Value Iteration algorithm to find the optimal value function and policy.\n",
    "    This implementation directly follows the complete algorithm shown in the pseudocode.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env: The Gymnasium environment.\n",
    "        gamma: The discount factor.\n",
    "        theta: The convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the optimal value function array and the optimal policy array.\n",
    "    \"\"\"\n",
    "    # Initialize the value function.\n",
    "    V = init_value_iteration(env)\n",
    "\n",
    "    while True:\n",
    "        # Initialize delta to track the maximum change in V in a single sweep.\n",
    "        delta = 0\n",
    "\n",
    "        # Iterate through each state for the update.\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Store the old value to measure the change.\n",
    "            old_v = V[state]\n",
    "\n",
    "            # Compute the expected value for all possible actions from the current state.\n",
    "            action_values = [compute_expected_value(env, V, state, action, gamma)\n",
    "                             for action in range(env.action_space.n)]\n",
    "\n",
    "            # The Bellman optimality update: V(s) is updated with the max action value.\n",
    "            # This combines the evaluation and improvement steps into one.\n",
    "            V[state] = np.max(action_values)\n",
    "\n",
    "            # Update delta with the absolute difference.\n",
    "            delta = max(delta, abs(old_v - V[state]))\n",
    "\n",
    "        # If the value function has converged, exit the loop.\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Once the optimal value function is found, extract the optimal policy.\n",
    "    policy = generate_policy(env, V, gamma)\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f621f3f-9261-4f75-93fa-a511c3013aad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3130d567b2d34a7411e80ff13d04238e",
     "grade": false,
     "grade_id": "cell-3a5e438c6988c441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo irá executar seu algoritmo *Value Iteration* em um ambiente Frozen Lake determinístico. A política resultante será armazenada a variável `value_iteration_deterministic`, que usaremos em outra tarefa. Certifique-se que ele esteja funcionando corretamente e que a política gerada corresponda ao comportamento esperado neste ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b13ad40-daa8-4c50-92c2-92336378d91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a507f4b6da72cd0204ce61ef7d995c7",
     "grade": true,
     "grade_id": "cell-65ff2786217d46c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ → ↓ ←\n",
      "↓ ▢ ↓ ▢\n",
      "→ ↓ ↓ ▢\n",
      "▢ → → ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "V, value_iteration_deterministic = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print_policy(env, value_iteration_deterministic)\n",
    "env.close()\n",
    "\n",
    "assert np.array_equal(value_iteration_deterministic, [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]), \"Política diferente da esperada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7814681-8791-4066-9205-6bd78043f7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8477abf2d6223cb005a34bf19346d24f",
     "grade": true,
     "grade_id": "cell-90866ba893778afb",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845b158-b075-4b9e-9da3-cc7cffd82b47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0bf756f1c61d1326fd39f832e2599b0",
     "grade": false,
     "grade_id": "cell-f948772c42437869",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac23f89-42f3-44b3-854b-366657919bfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef88d7295863002c1746176f54048cfb",
     "grade": false,
     "grade_id": "cell-129f7cfccf09f7e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora, executaremos seus algoritmos no mesmo ambiente do Frozen Lake, porém escorregadio. As políticas resultante serão armazenadas nas variáveis `policy_iteration_slippery` e `value_iteration_slippery`, que usaremos na tarefa 14. Nesse cenário, o agente tem apenas 1/3 de chance de se mover na direção desejada e 2/3 de chance de se mover em uma direção perpendicular. Observe as políticas resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4952605d-7df8-4583-ae30-d27f611b8565",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee3c7f66748e1f0c7e5aded56c485dd5",
     "grade": false,
     "grade_id": "cell-51aba37175d6166c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration\n",
      "← ↑ ↑ ↑\n",
      "← ▢ ← ▢\n",
      "↑ ↓ ← ▢\n",
      "▢ → ↓ ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, policy_iteration_slippery = policy_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Policy Iteration\")\n",
    "print_policy(env, policy_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e214b7-6f93-4f64-813f-23fac2d91eab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28ac89d9a717a5ad6077b10991b0ea62",
     "grade": false,
     "grade_id": "cell-1cd7b2d2c9f5a32e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "← ↑ ↑ ↑\n",
      "← ▢ ← ▢\n",
      "↑ ↓ ← ▢\n",
      "▢ → ↓ ◎\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "V, value_iteration_slippery = value_iteration(env, gamma=0.99, theta=1e-8)\n",
    "print(\"Value Iteration\")\n",
    "print_policy(env, value_iteration_slippery)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3cef6-fa32-44a7-b1a1-b59415304dcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f3c44171f967cf4a67342ee8dd96568",
     "grade": false,
     "grade_id": "cell-9ebda4da8134aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Implemente a função `execute_policy` abaixo, que deve executar uma política previamente obtida em um ambiente Frozen Lake por $N$ episódios, retornando a recompensa acumulada de cada episódio e suas durações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a263709-5c41-420f-a35e-4ce3bd2b0af0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65dd2151b7b6eb59037e7c206aebf2cf",
     "grade": true,
     "grade_id": "cell-8c6445e2c40cad0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def execute_policy(env: gym.Env, policy: np.ndarray[int], n_episodes):\n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select the action for the current state according to the given policy.\n",
    "            # This is the core of executing a deterministic policy.\n",
    "            action = policy[state]\n",
    "\n",
    "            # Take the action in the environment to get the next state, reward, and termination status.\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Accumulate the reward for the current episode.\n",
    "            total_reward += reward\n",
    "\n",
    "            # Increment the step counter.\n",
    "            step_count += 1\n",
    "\n",
    "            # The episode ends if the state is terminal (terminated or truncated).\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Move to the next state for the next iteration.\n",
    "            state = next_state\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        episode_lengths.append(step_count)\n",
    "    return episode_returns, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359818ef-5844-4300-8e03-f989005f5251",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f138e90efe57b4f18d45fa1f9c7da8e3",
     "grade": false,
     "grade_id": "cell-774fba8dd36ffbe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Utilize a função `execute_policy` para avaliar a política obtida pelo Policy Iteration no Frozen Lake **determinístico** (`policy_iteration_deterministic`) em um ambiente Frozen Lake escorregadio por 10 episódios. Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_1_returns` e a duração dos episódios na variável `agent_1_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20dc536-c94c-43fd-9d87-204ea87b04bb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4ae3ec1ce95e41ffe14db005b9ae544",
     "grade": false,
     "grade_id": "cell-5e1db6859a2651d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# Execute the deterministic policy in the slippery environment for 10 episodes.\n",
    "# The results (rewards and lengths) are stored in the specified variables.\n",
    "#\n",
    "agent_1_returns, agent_1_lengths = execute_policy(env, policy_iteration_deterministic, n_episodes=10)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b773d-73f5-4a22-bda0-5d21a1f264d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e0754bfa1c08878b4c14904ee53a0f",
     "grade": false,
     "grade_id": "cell-bde46ff3fce95520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Repita o procedimento da tarefa anterior, desta vez utilizando a política obtida pelo Policy Iteration no Frozen Lake **escorregadio** (`policy_iteration_slippery`). Armazene as recompensas acumuladas ao longo dos episódios na variável `agent_2_returns` e a duração dos episódios na variável `agent_2_lengths`. Observe o comportamento do agente durante a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020b4b0-47d0-4c60-a4f1-7e6b1411ab47",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c50df759f226ac2558ea4d515f343a7",
     "grade": true,
     "grade_id": "cell-d7202ccd7e779b1a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# Execute the slippery-trained policy in the slippery environment for 10 episodes.\n",
    "# This evaluates how the policy optimized for stochasticity performs.\n",
    "agent_2_returns, agent_2_lengths = execute_policy(env, policy_iteration_slippery, n_episodes=10)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f24dc-2e61-4c83-ad77-50638b8c0e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba9c4b3ee4aec555f44237ab98d95d25",
     "grade": false,
     "grade_id": "cell-eaad1d7c23d3bf5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analise a seguinte comparação entre as recompensas e a duração obtidas por cada uma dessas duas execuções no Frozen Lake escorregadio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1b38f-a55f-4a6a-b334-31380f871153",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbed36db4b67ef8e511dbcec2a153c7f",
     "grade": false,
     "grade_id": "cell-5eec276dbb402e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compare_policy(\n",
    "    rewards_run1, lengths_run1,\n",
    "    rewards_run2, lengths_run2,\n",
    "    label_run1=\"Agent 1\", label_run2=\"Agent 2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two policy runs using mean return and mean episode length.\n",
    "\n",
    "    Args:\n",
    "        rewards_run1 (list): Episode rewards for run 1.\n",
    "        lengths_run1 (list): Episode lengths for run 1.\n",
    "        rewards_run2 (list): Episode rewards for run 2.\n",
    "        lengths_run2 (list): Episode lengths for run 2.\n",
    "        label_run1 (str): Label for run 1.\n",
    "        label_run2 (str): Label for run 2.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_rewards = [np.mean(rewards_run1), np.mean(rewards_run2)]\n",
    "    std_rewards  = [np.std(rewards_run1), np.std(rewards_run2)]\n",
    "\n",
    "    mean_lengths = [np.mean(lengths_run1), np.mean(lengths_run2)]\n",
    "    std_lengths  = [np.std(lengths_run1), np.std(lengths_run2)]\n",
    "\n",
    "    labels = [label_run1, label_run2]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Mean Rewards\n",
    "    axes[0].bar(x, mean_rewards, yerr=std_rewards, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_ylabel(\"Mean Total Reward\")\n",
    "    axes[0].set_title(\"Mean Episode Return ± Std\")\n",
    "    axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # Mean Episode Lengths\n",
    "    axes[1].bar(x, mean_lengths, yerr=std_lengths, capsize=5, width=width, color=['skyblue', 'salmon'])\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_ylabel(\"Mean Episode Length\")\n",
    "    axes[1].set_title(\"Mean Episode Length ± Std\")\n",
    "    axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_policy(agent_1_returns, agent_1_lengths, agent_2_returns, agent_2_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa0ba7-455a-41c4-97d4-8c17fa8b0e1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd865599e6154e8061df6dcbe6aa7eca",
     "grade": false,
     "grade_id": "cell-f8680351f981a5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "15. Explique quais fatores levaram às diferenças observadas entre as políticas obtidas no ambiente determinístico e no ambiente escorregadio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dd49c-eb7c-41a1-a3d1-ca7ef0818922",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4304d231930b7c1e752d1520d20c830",
     "grade": true,
     "grade_id": "cell-21cbc643c62202d7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. Natureza do Ambiente: Determinismo vs. Estocasticidade\n",
    "O fator principal é a propriedade is_slippery do ambiente.\n",
    "\n",
    "Ambiente Determinístico (is_slippery=False): Neste cenário, a ação escolhida pelo agente leva ao resultado esperado com 100% de certeza. Se o agente decide ir para a \"direita\", ele se move para a direita. A política ótima, portanto, traça o caminho mais curto e direto para o objetivo, evitando apenas os buracos de forma precisa. Não há necessidade de se preocupar com movimentos não intencionais.\n",
    "\n",
    "Ambiente Escorregadio (is_slippery=True): Aqui, o ambiente é estocástico. Uma ação escolhida tem apenas uma chance de levar ao resultado pretendido, com as demais possibilidades sendo movimentos perpendiculares. Isso introduz um risco significativo: uma ação aparentemente segura pode levar o agente a um buraco.\n",
    "\n",
    "2. Estratégia de Mitigação de Risco\n",
    "Devido à incerteza, a política ótima no ambiente escorregadio muda de uma estratégia de \"caminho mais curto\" para uma de mitigação de risco.\n",
    "\n",
    "Política Determinística: A política gerada é \"gananciosa\" e otimista. Ela assume que o controle do agente sobre o ambiente é absoluto e, por isso, se aproxima das bordas e dos buracos sem hesitação para encurtar o caminho.\n",
    "\n",
    "Política Estocástica: A política gerada é visivelmente mais conservadora. Ela aprende que ações executadas perto de buracos têm uma alta probabilidade de falha, mesmo que a ação \"correta\" aponte para longe do perigo. Por exemplo, no estado 5 (segunda linha, segunda coluna), a política determinística pode seguramente ir para a direita. No entanto, na versão escorregadia, essa ação pode resultar em um movimento para baixo, levando ao buraco no estado 9. A política ótima, então, aprende a evitar essas zonas de risco, preferindo caminhos mais longos, porém mais seguros, que mantêm o agente longe das \"bordas\" dos buracos.\n",
    "\n",
    "3. Impacto na Função de Valor\n",
    "Os algoritmos de Policy e Value Iteration buscam maximizar o retorno esperado.\n",
    "\n",
    "No ambiente determinístico, o valor de um estado é alto se ele estiver em um caminho curto para a recompensa.\n",
    "\n",
    "No ambiente estocástico, o valor de um estado (V(s)) passa a ser descontado não apenas pelo fator gamma, mas também pela probabilidade de falha. Estados adjacentes a buracos terão valores inerentemente mais baixos, pois o retorno esperado a partir deles é menor devido à chance de cair e receber recompensa zero. O algoritmo então favorece ações que levam a estados com valores esperados mais altos e mais seguros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4571b7-ff1b-4c70-9d85-e0de016a6bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5292df5ef29eee183f943505f3ec2e4",
     "grade": false,
     "grade_id": "cell-426ab2d2c4253fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "16. Quais estratégias poderiam ser adotadas para tornar o comportamento do agente menos conservador quando treinado no ambiente escorregadio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce5371-a76a-415b-b282-4d4ab80ff056",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbcb94e28a49b89a5fb7b66e4c166a94",
     "grade": true,
     "grade_id": "cell-70d62e2dd7e44e0e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "A política \"conservadora\" gerada para o ambiente escorregadio é, de fato, a política ótima para o objetivo de maximizar o retorno esperado naquele ambiente específico. O comportamento de evitar bordas de buracos não é uma falha, mas sim a conclusão lógica do algoritmo ao levar em conta a alta probabilidade de erro (escorregar).\n",
    "\n",
    "Para induzir um comportamento menos conservador (ou seja, que aceite mais riscos em troca de caminhos potencialmente mais curtos), não estamos buscando um algoritmo \"melhor\", mas sim alterando a definição do problema ou o objetivo do agente. Um desenvolvedor experiente deve abordar essas mudanças com cautela, pois elas implicam em um desvio da otimalidade original e podem introduzir instabilidade.\n",
    "\n",
    "A seguir, apresento algumas estratégias viáveis, juntamente com uma análise crítica de suas implicações.\n",
    "\n",
    "Estratégias para Reduzir o Comportamento Conservador\n",
    "1. Modificação da Função de Recompensa (Reward Shaping)\n",
    "Conceito: A principal razão para o conservadorismo é que o custo de cair em um buraco (fim do episódio, recompensa zero) é muito maior do que o benefício de economizar alguns passos. Podemos incentivar caminhos mais curtos introduzindo uma pequena penalidade a cada passo.\n",
    "\n",
    "Implementação Prática: Em vez de uma recompensa de 0 para cada transição, poderíamos usar uma recompensa de -0.01, por exemplo. Isso criaria uma pressão para que o agente chegue ao objetivo o mais rápido possível para minimizar a penalidade acumulada, forçando-o a considerar caminhos mais curtos e, consequentemente, mais arriscados.\n",
    "\n",
    "Análise Crítica (Perspectiva Conservadora): Esta é a abordagem mais direta, mas é fundamental entender que estamos mudando o problema que o agente está resolvendo. A nova política será ótima para a nova função de recompensa, não para a original. Existe o risco de \"super-ajuste\" (over-tuning): se a penalidade por passo for muito alta, o agente pode aprender a preferir cair em um buraco próximo ao início para encerrar o episódio rapidamente, em vez de acumular uma grande penalidade em um caminho longo. Essa técnica deve ser usada com validação rigorosa para garantir que não surjam comportamentos exploratórios indesejados.\n",
    "\n",
    "2. Ajuste do Fator de Desconto (Gamma - γ)\n",
    "Conceito: O parâmetro γ determina a importância de recompensas futuras. Um γ próximo de 1 (como o 0.99 usado) torna o agente \"míope\", valorizando muito a recompensa final de +1. Um γ menor o tornaria mais \"impaciente\", focando em ganhos de curto prazo.\n",
    "\n",
    "Implementação Prática: Reduzir o valor de gamma (e.g., para 0.9 ou 0.8). Com um γ menor, a diferença de valor entre um caminho longo e seguro e um caminho curto e arriscado diminui. O custo de passos adicionais se torna mais significativo em relação à recompensa final descontada, o que pode levar o agente a aceitar mais riscos.\n",
    "\n",
    "Análise Crítica (Perspectiva Conservadora): Assim como o reward shaping, alterar γ modifica o horizonte de otimização do agente. Embora seja um hiperparâmetro legítimo para ajuste, seus efeitos podem ser contra-intuitivos. Um γ muito baixo pode fazer com que a recompensa final se torne tão insignificante que o agente não consiga aprender um caminho coerente. Esta é uma estratégia menos direta que a modificação da recompensa e exige uma análise empírica cuidadosa para validar se o comportamento resultante é de fato superior e não apenas um artefato de um agente excessivamente focado no presente.\n",
    "\n",
    "3. Assunção de um Modelo Otimista (Model Mismatch)\n",
    "Conceito: A política é conservadora porque o modelo do ambiente (as probabilidades de transição em env.unwrapped.P) informa ao agente que o mundo é perigoso. Poderíamos treinar o agente em um modelo que assume que o mundo é mais seguro do que realmente é.\n",
    "\n",
    "Implementação Prática: Esta estratégia foi, na verdade, executada na tarefa 13 do notebook. A política policy_iteration_deterministic foi treinada em um ambiente sem a propriedade is_slippery e depois executada no ambiente escorregadio. O resultado, como mostram os gráficos, é um desempenho muito ruim, pois o agente não está preparado para a estocasticidade do ambiente real.\n",
    "\n",
    "Análise Crítica (Perspectiva Conservadora): Esta abordagem é incorreta e perigosa para qualquer aplicação no mundo real. Treinar com um modelo impreciso ou excessivamente otimista leva a políticas que não são robustas a falhas. Embora seja um exercício acadêmico útil para ilustrar a importância de um modelo preciso, não é uma estratégia viável para reduzir o conservadorismo de forma eficaz e segura. A política resultante não é menos conservadora, é simplesmente uma política ruim para o ambiente em questão.\n",
    "\n",
    "Conclusão\n",
    "Do ponto de vista de um desenvolvedor focado em segurança e otimalidade, a política conservadora é a solução correta para o problema como formulado. O desejo por um comportamento \"menos conservador\" geralmente indica que a função de recompensa original não captura perfeitamente o verdadeiro objetivo do projetista. Portanto, a estratégia mais defensável e transparente é a Modificação da Função de Recompensa (Reward Shaping), desde que seja feita com a plena consciência de que isso redefine o problema e que os resultados devem ser rigorosamente validados para evitar a introdução de comportamentos perigosos ou indesejados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
