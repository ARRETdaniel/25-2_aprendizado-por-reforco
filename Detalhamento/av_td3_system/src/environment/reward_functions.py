"""
Reward calculation for autonomous vehicle navigation.

Implements a multi-component reward function with safety penalties,
progress incentives, and comfort objectives.
"""

import numpy as np
from typing import Dict
import logging


class RewardCalculator:
    """
    Computes multi-component reward for driving policy training.

    Reward components (configurable weights):
    1. Efficiency: Reward for target speed tracking
    2. Lane Keeping: Reward for staying in lane
    3. Comfort: Penalty for high jerk
    4. Safety: Large penalty for collisions/off-road
    """

    def __init__(self, config: Dict):
        """
        Initialize reward calculator with configuration.

        Args:
            config: Dictionary with reward parameters from training_config.yaml
                Expected keys:
                - reward.weights: {efficiency, lane_keeping, comfort, safety, progress}
                - reward.efficiency: {target_speed, speed_tolerance, overspeed_penalty_scale}
                - reward.lane_keeping: {lateral_tolerance, heading_tolerance}
                - reward.comfort: {jerk_threshold}
                - reward.safety: {collision_penalty, offroad_penalty, wrong_way_penalty}
                - reward.progress: {waypoint_bonus, distance_scale}
        """
        self.logger = logging.getLogger(__name__)

        # Extract weights
        self.weights = config.get("weights", {
            "efficiency": 1.0,
            "lane_keeping": 2.0,
            "comfort": 0.5,
            "safety": -100.0,
            "progress": 5.0,  # NEW: High weight for goal-directed progress
        })

        # Efficiency parameters
        self.target_speed = config.get("efficiency", {}).get("target_speed", 8.33)  # m/s (default: 30 km/h)
        self.speed_tolerance = config.get("efficiency", {}).get("speed_tolerance", 1.39)  # m/s (default: 5 km/h)
        self.overspeed_penalty_scale = config.get("efficiency", {}).get(
            "overspeed_penalty_scale", 2.0
        )

        # Lane keeping parameters
        self.lateral_tolerance = config.get("lane_keeping", {}).get(
            "lateral_tolerance", 0.5
        )
        self.heading_tolerance = config.get("lane_keeping", {}).get(
            "heading_tolerance", 0.1
        )

        # Comfort parameters
        self.jerk_threshold = config.get("comfort", {}).get("jerk_threshold", 3.0)

        # Safety parameters
        # FIXED: Reduced collision penalty from -1000 to -100 (High Priority Fix #4)
        # Rationale: TD3's clipped double-Q amplifies negative memories.
        # -1000 creates "collisions are unrecoverable" belief, -100 is still strong
        # but allows agent to learn from mistakes. Matches successful implementations
        # (Ben Elallid et al. 2023, P√©rez-Gil et al. 2022).
        self.collision_penalty = config.get("safety", {}).get("collision_penalty", -100.0)
        self.offroad_penalty = config.get("safety", {}).get("offroad_penalty", -500.0)
        self.wrong_way_penalty = config.get("safety", {}).get(
            "wrong_way_penalty", -200.0
        )

        # Progress parameters (NEW: Goal-directed navigation rewards)
        self.waypoint_bonus = config.get("progress", {}).get("waypoint_bonus", 10.0)
        # FIXED: Increased distance scale from 0.1 to 1.0 (High Priority Fix #3)
        # Rationale: Moving 1m now gives +1.0 progress (weighted: +5.0 total),
        # which can offset efficiency penalty during acceleration. Previous 0.1 scale
        # gave only +0.5 weighted reward, insufficient to overcome -1.0 efficiency penalty.
        self.distance_scale = config.get("progress", {}).get("distance_scale", 1.0)
        self.goal_reached_bonus = config.get("progress", {}).get("goal_reached_bonus", 100.0)

        # State tracking for jerk calculation
        self.prev_acceleration = 0.0
        self.prev_acceleration_lateral = 0.0

        # State tracking for progress calculation
        self.prev_distance_to_goal = None  # Will be set on first step

        # PBRS (Potential-Based Reward Shaping) parameter (Medium Priority Fix #6)
        # Discount factor for potential function (should match TD3's gamma)
        self.gamma = config.get("gamma", 0.99)

    def calculate(
        self,
        velocity: float,
        lateral_deviation: float,
        heading_error: float,
        acceleration: float,
        acceleration_lateral: float,
        collision_detected: bool,
        offroad_detected: bool,
        wrong_way: bool = False,
        distance_to_goal: float = 0.0,
        waypoint_reached: bool = False,
        goal_reached: bool = False,
        lane_half_width: float = None,
        dt: float = 0.05,
        # NEW: Dense safety metrics (Priority 1 & 3 fixes)
        distance_to_nearest_obstacle: float = None,
        time_to_collision: float = None,
        collision_impulse: float = None,
    ) -> Dict:
        """
        Calculate multi-component reward with dense PBRS safety guidance.

        PRIORITY 1, 2, 3 FIXES INTEGRATED:
        ===================================
        - Dense PBRS proximity guidance via distance_to_nearest_obstacle
        - Time-to-collision penalties for imminent collisions
        - Graduated collision penalties via collision_impulse
        - Rebalanced penalty magnitudes (loaded from config)

        Args:
            velocity: Current vehicle velocity (m/s)
            lateral_deviation: Distance from lane center (m, positive=right)
            heading_error: Heading error w.r.t. lane direction (radians)
            acceleration: Current longitudinal acceleration (m/s¬≤)
            acceleration_lateral: Current lateral acceleration (m/s¬≤)
            collision_detected: Whether collision occurred this step
            offroad_detected: Whether vehicle went off-road
            wrong_way: Whether vehicle is driving in wrong direction
            distance_to_goal: Distance remaining to destination (m)
            waypoint_reached: Whether agent reached a waypoint this step
            goal_reached: Whether agent reached final goal this step
            lane_half_width: Half of current lane width from CARLA (m)
            dt: Time step since last measurement for jerk computation (seconds)
            distance_to_nearest_obstacle: Distance to nearest obstacle (m) - NEW
            time_to_collision: Estimated TTC in seconds - NEW
            collision_impulse: Collision force magnitude in Newtons - NEW

        Returns:
            Dictionary with:
            - total: Total reward
            - efficiency: Efficiency component
            - lane_keeping: Lane keeping component
            - comfort: Comfort component
            - safety: Safety component (with dense PBRS guidance)
            - progress: Progress component
            - breakdown: Detailed breakdown of all components
        """
        reward_dict = {}

        # 1. EFFICIENCY REWARD: Forward velocity component
        efficiency = self._calculate_efficiency_reward(velocity, heading_error)
        reward_dict["efficiency"] = efficiency

        # 2. LANE KEEPING REWARD: Minimize lateral deviation and heading error
        lane_keeping = self._calculate_lane_keeping_reward(
            lateral_deviation, heading_error, velocity, lane_half_width
        )
        reward_dict["lane_keeping"] = lane_keeping

        # 3. COMFORT PENALTY: Minimize jerk
        comfort = self._calculate_comfort_reward(
            acceleration, acceleration_lateral, velocity, dt
        )
        reward_dict["comfort"] = comfort

        # Update previous accelerations for next step
        self.prev_acceleration = acceleration
        self.prev_acceleration_lateral = acceleration_lateral

        # 4. SAFETY PENALTY: Dense PBRS guidance + graduated penalties
        safety = self._calculate_safety_reward(
            collision_detected,
            offroad_detected,
            wrong_way,
            velocity,
            distance_to_goal,
            distance_to_nearest_obstacle,  # NEW
            time_to_collision,  # NEW
            collision_impulse,  # NEW
        )
        reward_dict["safety"] = safety

        # 5. PROGRESS REWARD: Reward forward progress toward goal
        progress = self._calculate_progress_reward(
            distance_to_goal, waypoint_reached, goal_reached
        )
        reward_dict["progress"] = progress

        # Calculate total weighted reward
        total_reward = (
            self.weights["efficiency"] * efficiency
            + self.weights["lane_keeping"] * lane_keeping
            + self.weights["comfort"] * comfort
            + self.weights["safety"] * safety
            + self.weights["progress"] * progress
        )

        reward_dict["total"] = total_reward
        reward_dict["breakdown"] = {
            "efficiency": (
                self.weights["efficiency"],
                efficiency,
                self.weights["efficiency"] * efficiency,
            ),
            "lane_keeping": (
                self.weights["lane_keeping"],
                lane_keeping,
                self.weights["lane_keeping"] * lane_keeping,
            ),
            "comfort": (
                self.weights["comfort"],
                comfort,
                self.weights["comfort"] * comfort,
            ),
            "safety": (
                self.weights["safety"],
                safety,
                self.weights["safety"] * safety,
            ),
            "progress": (
                self.weights["progress"],
                progress,
                self.weights["progress"] * progress,
            ),
        }

        #  DIAGNOSTIC LOGGING: Reward Component Balance
        # Added 2025-01-20 for training failure investigation
        # Purpose: Identify which component is dominating and causing -50k mean reward
        self.logger.debug(
            f"[REWARD] Components - "
            f"Efficiency: {efficiency:.3f}√ó{self.weights['efficiency']:.1f}={self.weights['efficiency']*efficiency:.2f}, "
            f"Lane: {lane_keeping:.3f}√ó{self.weights['lane_keeping']:.1f}={self.weights['lane_keeping']*lane_keeping:.2f}, "
            f"Comfort: {comfort:.3f}√ó{self.weights['comfort']:.1f}={self.weights['comfort']*comfort:.2f}, "
            f"Safety: {safety:.3f}√ó{self.weights['safety']:.1f}={self.weights['safety']*safety:.2f}, "
            f"Progress: {progress:.3f}√ó{self.weights['progress']:.1f}={self.weights['progress']*progress:.2f}"
        )

        self.logger.debug(f"[REWARD] TOTAL: {total_reward:.2f}")

        # Log warning if any component is dominating (>80% of total absolute magnitude)
        component_magnitudes = {
            "efficiency": abs(self.weights['efficiency'] * efficiency),
            "lane_keeping": abs(self.weights['lane_keeping'] * lane_keeping),
            "comfort": abs(self.weights['comfort'] * comfort),
            "safety": abs(self.weights['safety'] * safety),
            "progress": abs(self.weights['progress'] * progress),
        }
        total_magnitude = sum(component_magnitudes.values())

        if total_magnitude > 0:
            for component, magnitude in component_magnitudes.items():
                ratio = magnitude / total_magnitude
                if ratio > 0.8:
                    self.logger.warning(
                        f"[REWARD] Component '{component}' is dominating: "
                        f"{ratio*100:.1f}% of total magnitude (threshold: 80%)"
                    )

        return reward_dict

    def _calculate_efficiency_reward(self, velocity: float, heading_error: float) -> float:
        """
        Calculate efficiency reward using forward velocity component.

        SIMPLIFIED VERSION (Priority 1 from documentation-backed analysis):
        - Follows KISS principle from ArXiv Reward Engineering Survey (2408.10215v1)
        - "Simple rewards often outperform complex designs"
        - Removes reverse penalty (breaks continuity at v=0)
        - Removes target speed tracking (unnecessary complexity)
        - Pure linear scaling is TD3-compatible and sufficient

        Based on:
        - P√©rez-Gil et al. (2022): Forward velocity component R = v * cos(œÜ)
        - TD3 requirements (Fujimoto et al. 2018): Continuous differentiable
        - OpenAI Spinning Up: No reward normalization in official TD3
        - CARLA 0.9.16 API: get_velocity() returns Vector3D in m/s

        Mathematical properties:
        - v=0 m/s ‚Üí efficiency=0 (neutral, prevents local optimum)
        - v=1 m/s, œÜ=0¬∞ ‚Üí efficiency=+0.12 (immediate positive feedback)
        - v=8.33 m/s, œÜ=0¬∞ ‚Üí efficiency=+1.0 (optimal)
        - œÜ=90¬∞ ‚Üí efficiency=0 (perpendicular, neutral)
        - œÜ=180¬∞ ‚Üí efficiency=-1.0 (backward, natural penalty from cos)
        - Continuous and differentiable EVERYWHERE (no discontinuities)

        Why simplified:
        - Reverse penalty (* 2.0) created discontinuity at v_forward=0
        - Target speed tracking added 3 conditional branches
        - Linear scaling already incentivizes reaching target speed naturally
        - TD3 exploration (Gaussian noise) benefits from smooth landscape

        Args:
            velocity: Current velocity magnitude (m/s)
            heading_error: Heading error w.r.t. desired direction (radians)

        Returns:
            Efficiency reward in [-1.0, 1.0], continuous and differentiable

        References:
        - Analysis: day_three-reward_efficiency_analysis.md (Assessment: 8.5/10)
        - TD3: https://spinningup.openai.com/en/latest/algorithms/td3.html
        - CARLA: https://carla.readthedocs.io/en/latest/python_api/#carlavehicle
        """
        # Forward velocity component: v * cos(œÜ)
        # Projects velocity onto desired heading direction
        # CARLA API validated: get_velocity() returns Vector3D in m/s (global frame)
        forward_velocity = velocity * np.cos(heading_error)

        # Normalize by target speed to get reward in [-1, 1] range
        # Linear scaling naturally incentivizes reaching target speed:
        # v=4.165 m/s (50%) ‚Üí efficiency=0.5
        # v=8.33 m/s (100%) ‚Üí efficiency=1.0 (maximum reward)
        efficiency = forward_velocity / self.target_speed

        # Clip to [-1, 1] range for safety (though math should keep it in range)
        return float(np.clip(efficiency, -1.0, 1.0))

    def _calculate_lane_keeping_reward(
        self, lateral_deviation: float, heading_error: float, velocity: float,
        lane_half_width: float = None
    ) -> float:
        """
        Calculate lane keeping reward with CARLA-based lane width normalization.

        CRITICAL FIX #2: Reduced velocity gate from 1.0 m/s to 0.1 m/s and added
        continuous velocity scaling to provide learning gradient during acceleration.

        ENHANCEMENT (Priority 3): Dynamic lane width normalization using CARLA API.
        Uses actual road geometry from waypoint.lane_width instead of fixed config.

        Key changes:
        - OLD: Hard cutoff at 1.0 m/s (no reward below 3.6 km/h)
        - NEW: Gate at 0.1 m/s (0.36 km/h, truly stationary) + velocity scaling
        - Gradual reward scaling from 0 to full as velocity increases 0‚Üí3 m/s
        - Enables agent to learn "stay centered while accelerating"
        - ENHANCED: Lateral normalization now uses CARLA lane_width (e.g., 1.25m urban
          vs 0.5m config) to reduce false positive lane invasions

        Rationale:
        - 1.0 m/s is slow pedestrian walk, not "stopped"
        - CARLA physics: 0‚Üí1 m/s takes ~10 ticks, all receiving zero gradient
        - TD3 needs continuous Q-value gradients for policy learning
        - P√©rez-Gil et al. use NO velocity gating (continuous everywhere)
        - Lane width: Town01 ‚âà 2.5m (half=1.25m) vs config 0.5m ‚Üí 2.5x more permissive
        - Reduces false positives at 0.5m < |d| < 1.25m range

        Args:
            lateral_deviation: Perpendicular distance from lane center (m)
            heading_error: Heading error w.r.t. lane direction (radians)
            velocity: Current velocity (m/s) - for gating and scaling
            lane_half_width: Half of current lane width from CARLA (m).
                           If None, uses config lateral_tolerance.

        Returns:
            Lane keeping reward, velocity-scaled, in [-1.0, 1.0]
        """
        # FIXED: Lower velocity threshold from 1.0 to 0.1 m/s
        # Only gate when truly stationary (0.1 m/s = 0.36 km/h)
        if velocity < 0.1:
            return 0.0

        # FIXED: Add velocity scaling for continuous gradient
        # Linearly scale from 0 (at v=0.1) to 1.0 (at v=3.0)
        # This provides learning signal during acceleration phase
        velocity_scale = min((velocity - 0.1) / 2.9, 1.0)  # (v-0.1)/(3.0-0.1) = (v-0.1)/2.9

        # ENHANCEMENT: Use CARLA lane width if available, else fallback to config
        # This enables multi-map generalization and reduces false positives
        effective_tolerance = (
            lane_half_width if lane_half_width is not None
            else self.lateral_tolerance
        )

        # Lateral deviation component (normalized by CARLA lane width or config tolerance)
        lat_error = min(abs(lateral_deviation) / effective_tolerance, 1.0)
        lat_reward = 1.0 - lat_error * 0.7  # 70% weight on lateral error

        # Heading error component (normalized by tolerance)
        head_error = min(abs(heading_error) / self.heading_tolerance, 1.0)
        head_reward = 1.0 - head_error * 0.3  # 30% weight on heading error

        # Combined reward (average of components, shifted to [-0.5, 0.5])
        lane_keeping = (lat_reward + head_reward) / 2.0 - 0.5

        # Apply velocity scaling (gradual increase as vehicle accelerates)
        # At v=0.5 m/s: scale‚âà0.14 ‚Üí some learning signal
        # At v=1.0 m/s: scale‚âà0.31 ‚Üí moderate signal
        # At v=3.0 m/s: scale=1.0 ‚Üí full signal
        return float(np.clip(lane_keeping * velocity_scale, -1.0, 1.0))

    def _calculate_comfort_reward(
        self, acceleration: float, acceleration_lateral: float, velocity: float, dt: float
    ) -> float:
        """
        Calculate comfort reward (penalize high jerk) with physically correct computation.

        COMPREHENSIVE FIX - Addresses all critical issues from analysis:

         FIX #1: Added dt division for correct jerk units (m/s¬≥)
           - OLD: jerk = |accel - prev_accel| ‚Üí Units: m/s¬≤ (acceleration difference)
           - NEW: jerk = (accel - prev_accel) / dt ‚Üí Units: m/s¬≥ (actual jerk)
           - Physics: Jerk = da/dt (third derivative of position)

         FIX #2: Removed abs() for TD3 differentiability
           - OLD: abs(x) is non-differentiable at x=0 (sharp corner)
           - NEW: Uses x¬≤ which is smooth and differentiable everywhere
           - TD3 Requirement: "Smooth Q-functions to prevent exploitation of sharp peaks"
           - Reference: OpenAI Spinning Up TD3 docs, arxiv:2408.10215v1

         FIX #3: Improved velocity scaling with sqrt for smoother transition
           - OLD: Linear scaling can under-penalize low-speed jerks
           - NEW: sqrt scaling provides more gradual transition
           - At v=0.5 m/s: linear scale=0.138, sqrt scale=0.372 (2.7x stronger signal)

         FIX #4: Bounded negative penalties with quadratic scaling
           - OLD: Unbounded penalty (-excess_jerk / threshold) could be very large
           - NEW: Quadratic penalty with 2x threshold cap prevents Q-value explosion
           - Rationale: Same principle as collision penalty reduction (High Priority Fix #4)
           - TD3's clipped double-Q amplifies negative memories

         FIX #5: Updated threshold to correct units
           - Configuration updated: jerk_threshold now in m/s¬≥ (was dimensionless)
           - Typical values: 2-3 m/s¬≥ comfortable, 5-8 m/s¬≥ max tolerable

        Mathematical Properties:
        - Continuous and differentiable everywhere (no discontinuities)
        - Bounded output range: [-1.0, 0.3]
        - Smooth gradient landscape for TD3 policy learning
        - Physically correct units and thresholds

        Future Enhancement (Medium Priority):
        - Add angular jerk component: jerk_angular = dœâ/dt (steering smoothness)
        - CARLA API: Actor.get_angular_velocity() ‚Üí deg/s (convert to rad/s)
        - Would require tracking self.prev_angular_velocity

        Args:
            acceleration: Current longitudinal acceleration (m/s¬≤)
            acceleration_lateral: Current lateral acceleration (m/s¬≤)
            velocity: Current velocity (m/s) - for gating and scaling
            dt: Time step since last measurement (seconds) - for jerk computation

        Returns:
            Comfort reward, velocity-scaled, in [-1.0, 0.3]

        References:
        - TD3: https://spinningup.openai.com/en/latest/algorithms/td3.html
        - CARLA: https://carla.readthedocs.io/en/latest/python_api/#carlavehicle
        - Reward Engineering: arxiv:2408.10215v1
        - Physics: Jerk = da/dt (third derivative of position)
        """
        # Velocity gating: prevent penalties at near-zero velocity
        # Threshold: 0.1 m/s = 0.36 km/h (truly stationary)
        if velocity < 0.1:
            return 0.0

        # FIX #1: Correct jerk computation with dt division
        # Jerk = da/dt (rate of change of acceleration)
        # Units: (m/s¬≤ - m/s¬≤) / s = m/s¬≥ (correct!)
        jerk_long = (acceleration - self.prev_acceleration) / dt
        jerk_lat = (acceleration_lateral - self.prev_acceleration_lateral) / dt

        # FIX #2: Use squared values for TD3 differentiability
        # x¬≤ is smooth everywhere (no sharp corners at x=0)
        jerk_long_sq = jerk_long ** 2
        jerk_lat_sq = jerk_lat ** 2

        # Combined jerk magnitude (Euclidean norm)
        # sqrt(x¬≤ + y¬≤) is smooth and differentiable everywhere except origin
        # At origin (both jerks = 0), sqrt is still differentiable
        total_jerk = np.sqrt(jerk_long_sq + jerk_lat_sq)

        # FIX #3: Improved velocity scaling with sqrt for smoother transition
        # OLD: Linear scaling (velocity - 0.1) / 2.9
        # NEW: Square root scaling for more gradual increase
        # Comparison at v=0.5 m/s: linear=0.138, sqrt=0.372 (2.7x stronger)
        # Comparison at v=1.0 m/s: linear=0.310, sqrt=0.557 (1.8x stronger)
        velocity_scale = min(np.sqrt((velocity - 0.1) / 2.9), 1.0) if velocity > 0.1 else 0.0

        # FIX #4: Bounded comfort reward with quadratic penalty
        # Normalize jerk to [0, 2] range (cap at 2x threshold)
        normalized_jerk = min(total_jerk / self.jerk_threshold, 2.0)

        if normalized_jerk <= 1.0:
            # Below threshold: positive reward for smoothness (linear decrease)
            # normalized_jerk=0 ‚Üí comfort=0.3 (smooth!)
            # normalized_jerk=1 ‚Üí comfort=0.0 (at threshold)
            comfort = (1.0 - normalized_jerk) * 0.3
        else:
            # Above threshold: quadratic penalty (smooth, bounded)
            # normalized_jerk=1 ‚Üí comfort=0.0 (continuous transition)
            # normalized_jerk=2 ‚Üí comfort=-0.3 (max penalty, capped)
            # Quadratic scaling: (1.0)¬≤=1.0, (1.5)¬≤=2.25, (2.0)¬≤=4.0
            # But we scale by 0.3/(2-1)¬≤ = 0.3 to normalize max penalty
            excess_normalized = normalized_jerk - 1.0  # Range: [0, 1]
            comfort = -0.3 * (excess_normalized ** 2)  # Smooth quadratic penalty

        # Update state tracking for next step
        self.prev_acceleration = acceleration
        self.prev_acceleration_lateral = acceleration_lateral

        # Apply velocity scaling and safety clip
        # Velocity scaling reduces penalty at low speeds (when jerk is less noticeable)
        return float(np.clip(comfort * velocity_scale, -1.0, 0.3))

    def _calculate_safety_reward(
        self,
        collision_detected: bool,
        offroad_detected: bool,
        wrong_way: bool,
        velocity: float,
        distance_to_goal: float,
        # NEW PARAMETERS for dense PBRS guidance (Priority 1 Fix)
        distance_to_nearest_obstacle: float = None,
        time_to_collision: float = None,
        collision_impulse: float = None,
    ) -> float:
        """
        Calculate safety reward with dense PBRS guidance and graduated penalties.

        PRIORITY 1 FIX: Dense Safety Guidance (PBRS)
        ============================================
        Implements Potential-Based Reward Shaping (PBRS) for continuous safety signals:
        - Œ¶(s) = -1.0 / max(distance_to_obstacle, 0.5)
        - Provides gradient BEFORE collisions occur
        - Enables proactive collision avoidance learning

        Reference: Analysis document Issue #1 (Sparse Safety Rewards - CRITICAL)
        PBRS Theorem (Ng et al. 1999): F(s,s') = Œ≥Œ¶(s') - Œ¶(s) preserves optimal policy

        PRIORITY 2 FIX: Magnitude Rebalancing
        ======================================
        Reduced penalty magnitudes from -50.0 to -5.0 for balanced multi-objective learning.

        PRIORITY 3 FIX: Graduated Penalties
        ===================================
        Uses collision impulse magnitude for severity-based penalties instead of fixed values.

        Args:
            collision_detected: Whether collision occurred (boolean)
            offroad_detected: Whether vehicle went off-road (boolean)
            wrong_way: Whether vehicle is driving wrong direction (boolean)
            velocity: Current velocity (m/s) - for TTC calculation and stopping penalty
            distance_to_goal: Distance to destination (m) - for progressive stopping penalty
            distance_to_nearest_obstacle: Distance to nearest obstacle in meters (NEW)
            time_to_collision: Estimated TTC in seconds (NEW)
            collision_impulse: Collision force magnitude in Newtons (NEW)

        Returns:
            Safety reward (0 if safe, negative with continuous gradient)
        """
        safety = 0.0

        # ========================================================================
        # PRIORITY 1: DENSE PROXIMITY GUIDANCE (PBRS) - CRITICAL FIX
        # ========================================================================
        # Provides continuous reward shaping that encourages maintaining safe distances
        # BEFORE catastrophic events occur. This is the PRIMARY fix for training failure.

        if distance_to_nearest_obstacle is not None:
            # Obstacle proximity potential: Œ¶(s) = -k / max(d, d_min)
            # Creates continuous gradient as obstacle approaches

            if distance_to_nearest_obstacle < 10.0:  # Only penalize within 10m range
                # Inverse distance potential for nearby obstacles
                # Mathematical form: potential = -k / max(distance, d_min)
                # where k=1.0 (scaling factor), d_min=0.5m (safety buffer)
                #
                # Gradient strength at different distances:
                # - 10.0m: -0.10 (gentle nudge, "stay aware")
                # - 5.0m:  -0.20 (moderate signal, "maintain distance")
                # - 3.0m:  -0.33 (strong signal, "prepare to slow")
                # - 1.0m:  -1.00 (urgent signal, "brake immediately")
                # - 0.5m:  -2.00 (maximum penalty, "collision imminent")

                proximity_penalty = -1.0 / max(distance_to_nearest_obstacle, 0.5)
                safety += proximity_penalty

                # Diagnostic logging for PBRS component
                self.logger.debug(
                    f"[SAFETY-PBRS] Obstacle @ {distance_to_nearest_obstacle:.2f}m "
                    f"‚Üí proximity_penalty={proximity_penalty:.3f}"
                )

            # ====================================================================
            # TIME-TO-COLLISION (TTC) PENALTY - Secondary Safety Signal
            # ====================================================================
            # Additional penalty for imminent collisions (approaching obstacle)
            # TTC < 3.0 seconds: Driver reaction time threshold (NHTSA standard)

            if time_to_collision is not None and time_to_collision < 3.0:
                # Inverse TTC penalty: shorter time = stronger penalty
                # Range: -5.0 (at 0.1s) to -0.17 (at 3.0s)
                #
                # Gradient strength:
                # - 3.0s: -0.17 (early warning, "start decelerating")
                # - 2.0s: -0.25 (moderate urgency, "brake soon")
                # - 1.0s: -0.50 (high urgency, "brake now")
                # - 0.5s: -1.00 (emergency, "hard brake")
                # - 0.1s: -5.00 (max penalty, "collision unavoidable")

                ttc_penalty = -0.5 / max(time_to_collision, 0.1)
                safety += ttc_penalty

                self.logger.debug(
                    f"[SAFETY-TTC] TTC={time_to_collision:.2f}s "
                    f"‚Üí ttc_penalty={ttc_penalty:.3f}"
                )

        # ========================================================================
        # PRIORITY 2 & 3: GRADUATED COLLISION PENALTY (Reduced + Impulse-Based)
        # ========================================================================
        # Uses collision impulse magnitude for severity-based penalties
        # Magnitude reduced from -100 to -10 for balanced learning (Priority 2 fix)

        if collision_detected:
            if collision_impulse is not None and collision_impulse > 0:
                # Graduated penalty based on impact severity
                # Formula: penalty = -min(10.0, impulse / 100.0)
                #
                # Collision severity mapping (approximate force values):
                # - Soft tap (10N):        -0.10 (minor contact, recoverable)
                # - Light bump (100N):     -1.00 (moderate, learn to avoid)
                # - Moderate crash (500N): -5.00 (significant, bad outcome)
                # - Severe crash (1000N+): -10.0 (maximum penalty, capped)
                #
                # Rationale: Soft collisions during exploration should not
                # catastrophically penalize agent. TD3's min(Q1,Q2) already
                # provides pessimism; graduated penalties allow learning.

                collision_penalty = -min(10.0, collision_impulse / 100.0)
                safety += collision_penalty

                self.logger.warning(
                    f"[SAFETY-COLLISION] Impulse={collision_impulse:.1f}N "
                    f"‚Üí graduated_penalty={collision_penalty:.2f}"
                )
            else:
                # Fallback: Default collision penalty (no impulse data available)
                # Reduced from -100 to -10 (Priority 2 fix)
                collision_penalty = -10.0
                safety += collision_penalty

                self.logger.warning(
                    f"[SAFETY-COLLISION] No impulse data, default penalty={collision_penalty:.1f}"
                )

        # ========================================================================
        # OFFROAD AND WRONG-WAY PENALTIES (Reduced Magnitude)
        # ========================================================================
        # Penalty magnitudes reduced for balance with progress rewards (Priority 2)

        if offroad_detected:
            # Reduced from -100 to -10 for balance
            offroad_penalty = -10.0
            safety += offroad_penalty
            self.logger.warning(f"[SAFETY-OFFROAD] penalty={offroad_penalty:.1f}")

        if wrong_way:
            # Reduced from -50 to -5 for balance
            wrong_way_penalty = -5.0
            safety += wrong_way_penalty
            self.logger.warning(f"[SAFETY-WRONG-WAY] penalty={wrong_way_penalty:.1f}")

        # ========================================================================
        # PROGRESSIVE STOPPING PENALTY (Already Implemented in Previous Fix)
        # ========================================================================
        # Discourages unnecessary stopping except near goal

        if not collision_detected and not offroad_detected:
            if velocity < 0.5:  # Essentially stopped (< 1.8 km/h)
                # Base penalty: small constant disincentive for stopping
                stopping_penalty = -0.1

                # Additional penalty if far from goal (progressive)
                if distance_to_goal > 10.0:
                    stopping_penalty += -0.4  # Total: -0.5 when far from goal
                elif distance_to_goal > 5.0:
                    stopping_penalty += -0.2  # Total: -0.3 when moderately far

                safety += stopping_penalty

                if stopping_penalty < -0.15:  # Only log significant stopping penalties
                    self.logger.debug(
                        f"[SAFETY-STOPPING] velocity={velocity:.2f} m/s, "
                        f"distance_to_goal={distance_to_goal:.1f}m "
                        f"‚Üí penalty={stopping_penalty:.2f}"
                    )

        # Diagnostic summary logging
        self.logger.debug(f"[SAFETY] Total safety reward: {safety:.3f}")

        return float(safety)

    def _calculate_progress_reward(
        self,
        distance_to_goal: float,
        waypoint_reached: bool,
        goal_reached: bool,
    ) -> float:
        """
        Calculate progress reward with PBRS (Potential-Based Reward Shaping).

        MEDIUM PRIORITY FIX #6: Added PBRS component for theoretically sound
        dense reward signal.

        Implements dense reward shaping based on:
        1. Distance reduction to goal (increased 10x via distance_scale)
        2. PBRS: F(s,s') = Œ≥Œ¶(s') - Œ¶(s) where Œ¶(s) = -distance_to_goal
        3. Waypoint milestone bonuses
        4. Goal reached bonus

        PBRS Theorem (Ng et al. 1999):
        "Potential-based shaping functions ensure that policies learned with
        shaped rewards remain effective in the original MDP, maintaining
        near-optimal policies."

        Mathematical guarantee: Adding F(s,s') = Œ≥Œ¶(s') - Œ¶(s) does NOT change
        the optimal policy, but provides denser learning signal.

        This addresses the sparse reward problem cited in arXiv:2408.10215:
        "Sparse and delayed nature of rewards in many real-world scenarios can hinder learning progress"

        Args:
            distance_to_goal: Current distance to final goal waypoint (meters)
            waypoint_reached: Whether agent passed a waypoint this step
            goal_reached: Whether agent reached the final destination

        Returns:
            Progress reward (positive for forward progress, includes PBRS term)
        """
        # Safety check: If distance_to_goal is None, log warning and use default
        if distance_to_goal is None:
            self.logger.warning(
                "[PROGRESS] distance_to_goal is None - waypoint manager may not be initialized properly. "
                "Using default distance of 0.0 for this step."
            )
            distance_to_goal = 0.0

        prev_dist_str = f"{self.prev_distance_to_goal:.2f}" if self.prev_distance_to_goal is not None else "None"
        self.logger.debug(
            f"[PROGRESS] Input: distance_to_goal={distance_to_goal:.2f}m, "
            f"waypoint_reached={waypoint_reached}, goal_reached={goal_reached}, "
            f"prev_distance={prev_dist_str}m"
        )

        progress = 0.0

        # Component 1: Distance-based reward (dense, continuous)
        # FIXED: Now uses distance_scale=1.0 (High Priority Fix #3)
        # Reward = (prev_distance - current_distance) * scale
        # Positive when moving toward goal, negative when moving away
        if self.prev_distance_to_goal is not None:
            distance_delta = self.prev_distance_to_goal - distance_to_goal
            distance_reward = distance_delta * self.distance_scale
            progress += distance_reward

            #  DIAGNOSTIC: Log distance delta and reward contribution
            self.logger.debug(
                f"[PROGRESS] Distance Delta: {distance_delta:.3f}m "
                f"({'forward' if distance_delta > 0 else 'backward'}), "
                f"Reward: {distance_reward:.2f} (scale={self.distance_scale})"
            )

            # Component 1b: PBRS (Medium Priority Fix #6)
            # Potential function: Œ¶(s) = -distance_to_goal
            # Shaping: F(s,s') = Œ≥Œ¶(s') - Œ¶(s)
            #                  = Œ≥(-distance_to_goal') - (-distance_to_goal_prev)
            #                  = -Œ≥*distance_to_goal' + distance_to_goal_prev
            potential_current = -distance_to_goal
            potential_prev = -self.prev_distance_to_goal
            pbrs_reward = self.gamma * potential_current - potential_prev
            pbrs_weighted = pbrs_reward * 0.5

            # Add PBRS with moderate weight (0.5x) to complement distance reward
            # Total progress signal = direct distance + PBRS (both encourage forward movement)
            progress += pbrs_weighted

            #  DIAGNOSTIC: Log PBRS components
            self.logger.debug(
                f"[PROGRESS] PBRS: Œ¶(s')={potential_current:.3f}, Œ¶(s)={potential_prev:.3f}, "
                f"F(s,s')={pbrs_reward:.3f}, weighted={pbrs_weighted:.3f} (Œ≥={self.gamma}, weight=0.5)"
            )

        else:
            # First step of episode - no previous distance for comparison
            self.logger.debug(
                f"[PROGRESS] First step: initializing prev_distance_to_goal={distance_to_goal:.2f}m"
            )

        # Update tracking
        self.prev_distance_to_goal = distance_to_goal

        # Component 2: Waypoint milestone bonus (sparse but frequent)
        # Encourage agent to reach intermediate waypoints
        if waypoint_reached:
            progress += self.waypoint_bonus
            self.logger.info(
                f"[PROGRESS]  Waypoint reached! Bonus: +{self.waypoint_bonus:.1f}, "
                f"total_progress={progress:.2f}"
            )

        # Component 3: Goal reached bonus (sparse but terminal)
        # Large reward for completing the route
        if goal_reached:
            progress += self.goal_reached_bonus
            self.logger.info(
                f"[PROGRESS]  Goal reached! Bonus: +{self.goal_reached_bonus:.1f}, "
                f"total_progress={progress:.2f}"
            )

        # Clip to reasonable range and log final result
        clipped_progress = float(np.clip(progress, -10.0, 110.0))

        # üîç DIAGNOSTIC: Log final progress reward and clipping status
        if progress != clipped_progress:
            self.logger.warning(
                f"[PROGRESS]  CLIPPED: raw={progress:.2f} ‚Üí clipped={clipped_progress:.2f}"
            )

        # Build debug string safely to avoid format errors
        distance_rew_str = f"{distance_reward:.2f}" if self.prev_distance_to_goal is not None and 'distance_reward' in locals() else "0.00"
        pbrs_str = f"{pbrs_weighted:.2f}" if self.prev_distance_to_goal is not None and 'pbrs_weighted' in locals() else "0.00"
        waypoint_str = f"{self.waypoint_bonus:.1f}" if waypoint_reached else "0.0"
        goal_str = f"{self.goal_reached_bonus:.1f}" if goal_reached else "0.0"

        self.logger.debug(
            f"[PROGRESS] Final: progress={clipped_progress:.2f} "
            f"(distance: {distance_rew_str}, "
            f"PBRS: {pbrs_str}, "
            f"waypoint: {waypoint_str}, "
            f"goal: {goal_str})"
        )

        return clipped_progress

    def reset(self):
        """Reset internal state for new episode."""
        self.prev_acceleration = 0.0
        self.prev_acceleration_lateral = 0.0
        self.prev_distance_to_goal = None  # Reset progress tracking
