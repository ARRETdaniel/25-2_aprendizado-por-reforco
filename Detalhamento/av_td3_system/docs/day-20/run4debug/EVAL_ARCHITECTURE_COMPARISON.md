# EVAL Architecture Comparison: Current vs Proposed

**Date**: 2025-11-20  
**Purpose**: Visual comparison of evaluation architectures

---

## ğŸ—ï¸ Current Architecture (BROKEN)

### System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CARLA SERVER (Port 2000)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              SINGLETON WORLD (Town01)                  â”‚ â”‚
â”‚  â”‚                                                        â”‚ â”‚
â”‚  â”‚  TRAINING PHASE:                                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚
â”‚  â”‚  â”‚ Ego Vehicle    â”‚  â”‚ TrafficManager  â”‚              â”‚ â”‚
â”‚  â”‚  â”‚ ID: 123        â”‚  â”‚ Port: 8000      â”‚              â”‚ â”‚
â”‚  â”‚  â”‚ Status: ACTIVE â”‚  â”‚ 20 NPCs         â”‚              â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â”‚                                                        â”‚ â”‚
â”‚  â”‚  EVAL PHASE (t = 5000):                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚
â”‚  â”‚  â”‚ EVAL Vehicle   â”‚  â”‚ TrafficManager  â”‚              â”‚ â”‚
â”‚  â”‚  â”‚ ID: 456        â”‚  â”‚ Port: 8050      â”‚ â† CONFLICT!  â”‚ â”‚
â”‚  â”‚  â”‚ Status: ACTIVE â”‚  â”‚ 20 NPCs         â”‚              â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â”‚                                                        â”‚ â”‚
â”‚  â”‚  eval_env.close() called:                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚
â”‚  â”‚  â”‚ EVAL Vehicle   â”‚  â”‚ TrafficManager  â”‚              â”‚ â”‚
â”‚  â”‚  â”‚ ID: 456        â”‚  â”‚ Port: 8050      â”‚ â† PERSISTS!  â”‚ â”‚
â”‚  â”‚  â”‚ Status: âŒDEAD â”‚  â”‚ (orphaned)      â”‚              â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â”‚                                                        â”‚ â”‚
â”‚  â”‚  BACK TO TRAINING:                                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚ â”‚
â”‚  â”‚  â”‚ Ego Vehicle    â”‚ â† Reference may be STALE!         â”‚ â”‚
â”‚  â”‚  â”‚ ID: 123 (?)    â”‚   (Could point to recycled actor) â”‚ â”‚
â”‚  â”‚  â”‚ Status: ???    â”‚                                   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: ğŸ’¥ vehicle.apply_control() returns CORRUPTED values
        (steering=-973852377088, gear=5649815)
```

### Code Flow

```python
# Training initialization
env = CARLANavigationEnv(tm_port=8000)          # Creates client, spawns vehicle ID=123
vehicle_ref = env.vehicle                        # Reference to ID=123

# Training loop (t=0 to 4999)
for t in range(5000):
    action = agent.select_action(obs)
    vehicle_ref.apply_control(action)            # âœ… Works (ID=123 alive)

# EVAL phase (t=5000)
eval_env = CARLANavigationEnv(tm_port=8050)     # âŒ Spawns EVAL vehicle ID=456
                                                 # âŒ Creates TM on port 8050
eval_env.reset()                                 # âŒ May destroy NPCs from training TM
# ... run 10 eval episodes ...
eval_env.close()                                 # âŒ Destroys EVAL vehicle ID=456
                                                 # âŒ TM on port 8050 persists (orphaned)

# Back to training (t=5001)
action = agent.select_action(obs)
vehicle_ref.apply_control(action)                # ğŸ’¥ CRASH! ID=123 may be invalid
                                                 # Returns corrupted VehicleControl
```

### Why It Fails

1. **Shared World**: `eval_env` connects to the SAME CARLA world as `env`
2. **Actor Destruction**: `eval_env.close()` destroys actors in the shared world
3. **Stale References**: `env.vehicle` (ID=123) may become invalid
4. **TM Conflict**: Two TMs (port 8000 and 8050) manage NPCs in the same world
5. **Orphaned TM**: TM on port 8050 persists after eval, keeps managing dead NPCs

---

## âœ… Proposed Architecture (FIXED)

### System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CARLA SERVER (Port 2000)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              SINGLETON WORLD (Town01)                  â”‚ â”‚
â”‚  â”‚                                                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚
â”‚  â”‚  â”‚ Ego Vehicle    â”‚  â”‚ TrafficManager  â”‚              â”‚ â”‚
â”‚  â”‚  â”‚ ID: 123        â”‚  â”‚ Port: 8000      â”‚              â”‚ â”‚
â”‚  â”‚  â”‚ Status: ACTIVE â”‚  â”‚ 20 NPCs         â”‚              â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â”‚        â†‘                     â†‘                         â”‚ â”‚
â”‚  â”‚        â”‚                     â”‚                         â”‚ â”‚
â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚ â”‚
â”‚  â”‚               SHARED BY:                               â”‚ â”‚
â”‚  â”‚        - EXPLORATION phase                             â”‚ â”‚
â”‚  â”‚        - LEARNING phase                                â”‚ â”‚
â”‚  â”‚        - EVAL phase â† NEW!                             â”‚ â”‚
â”‚  â”‚                                                        â”‚ â”‚
â”‚  â”‚  NO separate EVAL environment!                         â”‚ â”‚
â”‚  â”‚  ALL phases use the SAME actor (ID=123)                â”‚ â”‚
â”‚  â”‚  ALL phases use the SAME TrafficManager (port 8000)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: âœ… No actor destruction
        âœ… No stale references
        âœ… No TM conflicts
        âœ… Simple, safe architecture
```

### Code Flow

```python
# Training initialization
env = CARLANavigationEnv(tm_port=8000)          # Creates client, spawns vehicle ID=123
vehicle_ref = env.vehicle                        # Reference to ID=123

# Training loop with EVAL phase
for t in range(max_timesteps):
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # EVAL PHASE (t % eval_freq == 0)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if t > 0 and t % eval_freq == 0:
        in_eval_phase = True
        
        # Run evaluation using SAME environment
        eval_metrics = evaluate(env, agent)     # âœ… Uses env (not eval_env!)
        
        # evaluate() implementation:
        for eval_ep in range(num_eval_episodes):
            obs, _ = env.reset()                # âœ… Reset SAME environment
            while not done:
                action = agent.select_action(obs, deterministic=True)  # âœ… No noise
                obs, reward, done, _, info = env.step(action)
                # No training, just collect metrics
        
        # Reset after eval for fresh training episode
        obs, _ = env.reset()                    # âœ… Fresh start
        in_eval_phase = False
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # EXPLORATION PHASE (t < start_timesteps)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if t < start_timesteps:
        action = env.action_space.sample()      # âœ… Random action
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LEARNING PHASE (t >= start_timesteps)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif not in_eval_phase:
        action = agent.select_action(obs, deterministic=False)
        noise = np.random.normal(...)           # âœ… Exploration noise
        action = (action + noise).clip(...)
    
    # Execute action (ALL phases use same vehicle_ref)
    next_obs, reward, done, _, info = env.step(action)  # âœ… Always valid
    
    # Store transition (LEARNING phase only)
    if not in_eval_phase and t >= start_timesteps:
        replay_buffer.add(obs, action, next_obs, reward, done)
    
    # Train agent (LEARNING phase only)
    if not in_eval_phase and t >= start_timesteps:
        agent.train(replay_buffer, batch_size)

# Clean shutdown
env.close()  # âœ… Only closed at the very end
```

### Why It Works

1. **Single Environment**: `env` instance persists throughout entire training
2. **No Actor Destruction**: Vehicle ID=123 stays alive (only reset, not destroyed)
3. **Valid References**: `env.vehicle` always points to live actor
4. **Single TM**: Port 8000 manages all NPCs consistently
5. **Phase Transitions**: Seamless switching between EXPLORATION/LEARNING/EVAL

---

## ğŸ“Š Side-by-Side Comparison

| Aspect | Current (Separate EVAL Env) | Proposed (Phase-Based) |
|--------|----------------------------|------------------------|
| **Environment Instances** | 2 (train + eval) | 1 (unified) |
| **Vehicle Instances** | 2 (train vehicle + eval vehicle) | 1 (same vehicle reset) |
| **TrafficManager Ports** | 2 (8000 + 8050) | 1 (8000) |
| **Actor Lifecycle** | âŒ CONFLICT (eval destroys actors) | âœ… SAFE (only resets) |
| **Vehicle Reference** | âŒ STALE after eval | âœ… ALWAYS VALID |
| **Code Complexity** | ğŸŸ¡ HIGH (manage 2 envs) | ğŸŸ¢ LOW (single env) |
| **CARLA Compliance** | âŒ Violates singleton world | âœ… Respects architecture |
| **TD3 Paper Compliance** | ğŸŸ¡ Similar (separate env) | ğŸŸ¢ Same (deterministic policy) |
| **Deterministic Eval** | âœ… Yes (different seed) | âœ… Yes (no noise) |
| **Risk Level** | ğŸ”´ HIGH (vehicle corruption) | ğŸŸ¢ LOW (proven pattern) |

---

## ğŸ”„ Phase Transition Diagram

### Proposed Architecture Flow

```
Timestep:  0     1000   1001   2000   2001   ...   5000   5001
           â†“      â†“      â†“      â†“      â†“      â†“      â†“      â†“
Phase:    EXPL   EVAL   LEARN  EVAL   LEARN  ... EVAL   LEARN
           â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Action:   RND    DET    POL+N  DET    POL+N  ... DET    POL+N
           â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Train:     âœ—     âœ—      âœ“      âœ—      âœ“      ... âœ—      âœ“
           â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Env:     SAME   SAME   SAME   SAME   SAME   ... SAME   SAME
           â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€...â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
                    ALL USE: env (ID=123)

Legend:
  EXPL = EXPLORATION (random actions)
  EVAL = EVALUATION (deterministic policy, no training)
  LEARN = LEARNING (policy + noise, training enabled)
  RND = Random action
  DET = Deterministic policy (no noise)
  POL+N = Policy + exploration noise
```

### Current Architecture Flow (BROKEN)

```
Timestep:  0     1000            1001   2000            2001
           â†“      â†“               â†“      â†“               â†“
Phase:    EXPL   EVAL            LEARN  EVAL            LEARN
           â”‚      â”‚               â”‚      â”‚               â”‚
Env:      env    eval_env        env    eval_env        env
           â”‚      â”‚               â”‚      â”‚               â”‚
Vehicle:  123    456 â† NEW!      123    456 â† NEW!      123
           â”‚      â”‚               â”‚      â”‚               â”‚
Status:  ALIVE   ALIVE           ???    ALIVE           ???
                  â†“ close()               â†“ close()
                 DEAD                    DEAD
                  â†“                       â†“
                env.vehicle (123)       env.vehicle (123)
                MAY BE INVALID!         MAY BE INVALID!
                      â†“                       â†“
                    ğŸ’¥ CRASH               ğŸ’¥ CRASH
```

---

## ğŸ¯ Implementation Diff Preview

### Current Code (BROKEN)

```python
def evaluate(self):
    # âŒ Creates NEW environment with DIFFERENT TM port
    eval_env = CARLANavigationEnv(
        self.carla_config_path,
        self.agent_config_path,
        self.training_config_path,
        tm_port=self.eval_tm_port  # Port 8050
    )
    
    for episode in range(self.num_eval_episodes):
        obs_dict, _ = eval_env.reset()  # Uses eval_env
        # ... run episode ...
        next_obs_dict, reward, done, _, info = eval_env.step(action)
    
    # âŒ Destroys EVAL actors (corrupts training env)
    eval_env.close()
    
    return eval_metrics
```

### Proposed Code (FIXED)

```python
def evaluate(self):
    """Evaluate using TRAINING environment (no separate instance)."""
    print(f"[EVAL] Entering evaluation phase...")
    
    # âœ… Save current episode count (EVAL doesn't count as training episodes)
    episode_num_before = self.episode_num
    
    for episode in range(self.num_eval_episodes):
        obs_dict, _ = self.env.reset()  # âœ… Uses TRAINING env (self.env)
        
        while not done:
            # âœ… Deterministic action (no exploration noise)
            action = self.agent.select_action(obs_dict, deterministic=True)
            next_obs_dict, reward, done, _, info = self.env.step(action)
            # ... collect metrics ...
    
    # âœ… Restore episode count
    self.episode_num = episode_num_before
    
    # âœ… Reset after EVAL for fresh training episode
    obs_dict, _ = self.env.reset()
    
    print(f"[EVAL] Exiting evaluation phase")
    
    # âœ… Return metrics AND fresh observation
    return eval_metrics, obs_dict
```

---

## ğŸ“ Migration Checklist

### Files to Modify

- [ ] `scripts/train_td3.py`:
  - [ ] Remove `self.eval_tm_port = 8050` from `__init__`
  - [ ] Add `self.in_eval_phase = False` flag
  - [ ] Rewrite `evaluate()` method (use `self.env`)
  - [ ] Update training loop to handle EVAL phase

### Variables to Remove

```python
# DELETE THESE:
self.eval_tm_port = 8050
```

### New Variables to Add

```python
# ADD THESE:
self.in_eval_phase = False  # Track evaluation mode
```

### Methods to Modify

```python
# BEFORE: def evaluate(self):
#   Creates eval_env, closes it after
#
# AFTER: def evaluate(self):
#   Uses self.env, returns (metrics, obs_dict)
```

---

## âœ… Expected Outcomes After Implementation

1. **NO vehicle state corruption**:
   - `vehicle.apply_control()` always receives valid actor
   - No corrupted steering/brake/gear values

2. **NO CARLA timeout errors**:
   - No actor lifecycle conflicts
   - No TM registry inconsistencies

3. **Simplified codebase**:
   - Single environment instance
   - Single TrafficManager port
   - Cleaner training loop logic

4. **Same evaluation quality**:
   - Still runs `num_eval_episodes` deterministic episodes
   - Still collects mean/std metrics
   - Still logs to TensorBoard

5. **Proven reliability**:
   - Uses same pattern as EXPLORATION/LEARNING phases
   - No environment switching complexity
   - Respects CARLA's singleton world design

---

**Status**: âœ… **ANALYSIS COMPLETE - READY TO IMPLEMENT**  
**Next**: Modify `scripts/train_td3.py` according to proposed changes  
**Validation**: Run 100-step micro-test, then 5K validation
