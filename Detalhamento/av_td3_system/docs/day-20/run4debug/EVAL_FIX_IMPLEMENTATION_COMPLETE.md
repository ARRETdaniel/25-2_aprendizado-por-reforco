# EVAL Phase Fix - Implementation Complete

**Date**: November 20, 2025
**Status**: ‚úÖ **IMPLEMENTATION COMPLETE**
**File Modified**: `scripts/train_td3.py`
**Reference Documents**:
- EVAL_PHASE_SOLUTION_ANALYSIS.md
- EVAL_ARCHITECTURE_COMPARISON.md
- EVAL_SOLUTION_SUMMARY.md

---

## üéØ Implementation Summary

Successfully implemented the **Unified Phase-Based Architecture** to fix CARLA vehicle state corruption issue during evaluation phase. All changes follow CARLA 0.9.16 documentation, TD3 paper best practices, and OpenAI Gym standards.

---

## ‚úÖ Changes Implemented

### 1. Removed Separate TrafficManager Port Configuration ‚úÖ

**Location**: `__init__()` method (lines ~148-156)

**Before**:
```python
self.training_tm_port = 8000  # Training uses default TM port
self.eval_tm_port = 8050      # Evaluation uses separate TM port
```

**After**:
```python
# CARLA Singleton World Architecture (per documentation):
# Client.get_world() returns SINGLE shared world instance.
# All environments MUST use the SAME TrafficManager to avoid actor lifecycle conflicts.
self.tm_port = 8000  # Single TM port for all phases (EXPLORATION, LEARNING, EVAL)
```

**Justification**:
- CARLA docs: `Client.get_world()` returns singleton world instance
- Separate TM ports create registry conflicts in shared world
- Reference: EVAL_PHASE_SOLUTION_ANALYSIS.md Section 3.1

---

### 2. Added EVAL Phase Tracking Flag ‚úÖ

**Location**: `__init__()` method (lines ~308-311)

**Code**:
```python
# EVAL phase tracking (EVAL_PHASE_SOLUTION_ANALYSIS.md)
# Track when we're in evaluation mode to avoid adding eval experiences to replay buffer
self.in_eval_phase = False
```

**Purpose**:
- Track evaluation mode state
- Prevent EVAL experiences from being added to replay buffer (deterministic policy)
- Enable phase-specific behavior (no training during EVAL)

**Reference**: TD3 paper - evaluation uses deterministic policy without training

---

### 3. Completely Rewrote evaluate() Method ‚úÖ

**Location**: `evaluate()` method (lines ~1288-1423)

**Key Changes**:

#### 3.1 Uses TRAINING Environment (No Separate Instance)
```python
# OLD (BROKEN):
eval_env = CARLANavigationEnv(..., tm_port=self.eval_tm_port)
# ... run episodes ...
eval_env.close()  # ‚ùå Destroys actors in shared world!

# NEW (FIXED):
obs_dict, reset_info = self.env.reset()  # ‚úÖ Uses self.env
# ... run episodes ...
# NO env.close() - environment stays alive!
```

**Benefit**: Eliminates actor lifecycle conflicts

---

#### 3.2 Added Vehicle State Validation
```python
# BEFORE evaluation
vehicle_id_before_eval = self.env.vehicle.id
vehicle_alive_before = self.env.vehicle.is_alive
print(f"[EVAL]   ID: {vehicle_id_before_eval}")
print(f"[EVAL]   is_alive: {vehicle_alive_before}")

# ... run evaluation episodes ...

# AFTER evaluation
vehicle_id_after_eval = self.env.vehicle.id
vehicle_alive_after = self.env.vehicle.is_alive
print(f"[EVAL]   ID: {vehicle_id_after_eval}")
print(f"[EVAL]   is_alive: {vehicle_alive_after}")
```

**Purpose**:
- Detect actor corruption early
- Validate CARLA actor lifecycle
- Log vehicle ID changes (expected after reset)

**Reference**: CARLA Actor API - `is_alive` property

---

#### 3.3 Episode Count Preservation
```python
# Save current episode count (EVAL episodes don't count as training)
episode_num_before_eval = self.episode_num

# ... run evaluation episodes ...

# Restore episode count
self.episode_num = episode_num_before_eval
```

**Justification**:
- EVAL episodes are for metrics, not training
- Maintains accurate training episode counter
- Reference: EVAL_PHASE_SOLUTION_ANALYSIS.md Section 8.1 (Critical Implementation Notes)

---

#### 3.4 Fresh Observation Reset After EVAL
```python
# CRITICAL: Reset environment after EVAL for fresh training episode
print(f"[EVAL] Resetting environment for fresh training episode...")
fresh_obs_dict, _ = self.env.reset()

# Return both metrics AND fresh observation
return eval_metrics, fresh_obs_dict
```

**Purpose**:
- Prevent training from continuing with last EVAL episode's state
- Ensure clean boundary between EVAL and LEARNING phases
- Reference: EVAL_PHASE_SOLUTION_ANALYSIS.md Section 8.4 (Episode Reset)

---

#### 3.5 Deterministic Policy (No Exploration Noise)
```python
# TD3 paper: evaluate with deterministic policy
action = self.agent.select_action(
    obs_dict,
    deterministic=True  # Key difference from LEARNING phase
)
```

**Justification**:
- TD3 paper (Fujimoto et al.): Evaluation uses deterministic actions
- Stable-Baselines3 TD3: `deterministic=True` during eval
- Reference: TD3/main.py line 88-98 (eval_policy function)

---

### 4. Updated Training Loop for EVAL Phase ‚úÖ

**Location**: `train()` method (lines ~1227-1283)

**Before**:
```python
if t % self.eval_freq == 0:
    eval_metrics = self.evaluate()  # Returns dict only
    # ... log metrics ...
    # ‚ùå obs_dict NOT updated! Training continues with stale state
```

**After**:
```python
if t % self.eval_freq == 0:
    # Set EVAL phase flag
    self.in_eval_phase = True

    # Run evaluation (returns metrics + fresh obs_dict)
    eval_metrics, obs_dict = self.evaluate()

    # Clear EVAL phase flag
    self.in_eval_phase = False

    # ... log metrics ...

    # CRITICAL: Reset episode tracking for fresh training episode
    done = False
    self.episode_reward = 0
    self.episode_timesteps = 0
    self.episode_collision_count = 0
    self.episode_lane_invasion_count = 0
    self.episode_reward_components = {...}
```

**Benefits**:
- Tracks EVAL phase state
- Handles fresh observation from evaluate()
- Resets episode tracking for clean training continuation
- Reference: EVAL_ARCHITECTURE_COMPARISON.md (Phase Transition Diagram)

---

## üìä Architecture Comparison

| Aspect | Before (Separate Env) | After (Unified Phases) |
|--------|----------------------|----------------------|
| **Environment Instances** | 2 (train + eval) | 1 (unified) |
| **TrafficManager Ports** | 2 (8000 + 8050) | 1 (8000) |
| **Actor Lifecycle** | ‚ùå CONFLICT | ‚úÖ SAFE |
| **Vehicle References** | ‚ùå STALE | ‚úÖ VALID |
| **Code Complexity** | üü° HIGH | üü¢ LOW |
| **CARLA Compliance** | ‚ùå Violates singleton | ‚úÖ Respects architecture |
| **TD3 Compliance** | üü° Similar approach | ‚úÖ Deterministic policy |

---

## üî¨ Validation Plan

### Test #1: 100-Step Micro-Validation (NOT YET RUN)

**Command**:
```bash
cd /workspace/av_td3_system
python scripts/train_td3.py \
  --scenario 0 \
  --max-timesteps 100 \
  --eval-freq 50 \
  --num-eval-episodes 2 \
  --seed 42 \
  --debug
```

**Expected Behavior**:
- ‚úÖ Steps 0-49: EXPLORATION (random forward-biased actions)
- ‚úÖ Step 50: EVAL phase triggers
  - Vehicle state logged BEFORE eval
  - 2 deterministic eval episodes
  - Vehicle state logged AFTER eval
  - Fresh observation returned
- ‚úÖ Steps 51-99: LEARNING (policy + noise)
- ‚úÖ Step 100: EVAL phase triggers again
- ‚úÖ **NO vehicle corruption** (steering=-973852377088)
- ‚úÖ **NO CARLA timeout** errors
- ‚úÖ **Vehicle ID may change** after resets (EXPECTED)

**Success Criteria**:
```
[EVAL] Vehicle state before evaluation:
[EVAL]   ID: 123
[EVAL]   is_alive: True
[EVAL] Resetting environment for fresh training episode...
[EVAL] Vehicle state after evaluation:
[EVAL]   ID: 456  # ‚Üê ID change is EXPECTED (new spawn)
[EVAL]   is_alive: True
[EVAL] Note: Vehicle ID changed (123 -> 456)
[EVAL]       This is EXPECTED after env.reset() (respawns actors)
```

---

### Test #2: 5K Validation Test (NOT YET RUN)

**Command**:
```bash
python scripts/train_td3.py \
  --scenario 0 \
  --max-timesteps 5000 \
  --eval-freq 1000 \
  --num-eval-episodes 10 \
  --seed 42
```

**Expected Behavior**:
- 5 EVAL phases at steps: 1000, 2000, 3000, 4000, 5000
- All EVAL phases complete successfully
- Training continues after each EVAL
- TensorBoard metrics show:
  - `eval/mean_reward` at timesteps 1000, 2000, 3000, 4000, 5000
  - `train/episode_reward` continues between EVAL phases
  - No gaps or discontinuities

---

### Test #3: TensorBoard Metrics Verification (NOT YET RUN)

**Check in TensorBoard**:
```bash
tensorboard --logdir data/logs/
```

**Verify**:
- ‚úÖ `eval/mean_reward` appears at correct timesteps
- ‚úÖ `eval/success_rate` appears at correct timesteps
- ‚úÖ `train/episode_reward` continues smoothly
- ‚úÖ No sudden drops or spikes after EVAL phases
- ‚úÖ Gradient metrics remain stable across EVAL transitions

---

## üéì Lessons Learned

### 1. CARLA Architecture Understanding
**Key Insight**: CARLA is NOT Gym/MuJoCo!
- Gym: `gym.make()` creates independent simulation instances
- CARLA: `CARLANavigationEnv()` connects to shared CARLA server
- `Client.get_world()` returns **SINGLETON** world instance
- Cannot create independent environments like Gym

**Reference**: CARLA Python API - `carla.Client.get_world()`

---

### 2. TD3 Paper vs CARLA Reality
**TD3 Reference Implementation**:
```python
# TD3/main.py (works for Gym)
eval_env = gym.make(env_name)  # Creates INDEPENDENT env
```

**CARLA Adaptation**:
```python
# Must use SAME environment (shared world)
obs_dict, _ = self.env.reset()  # Reuses training env
```

**Why**: TD3's separate eval env approach works for Gym (independent sims) but FAILS for CARLA (shared world).

**Reference**: TD3 paper (Fujimoto et al. 2018), CARLA 0.9.16 docs

---

### 3. Actor Lifecycle is Critical
**CARLA Actors**:
- Server-managed, not client-managed
- `actor.destroy()` blocks until server completes
- References become STALE when actors destroyed
- ALWAYS validate with `is_alive` property

**Failure Mode**: `eval_env.close()` destroys actors ‚Üí training vehicle reference becomes stale ‚Üí `apply_control()` returns corrupted values.

**Reference**: CARLA Actor API - `carla.Actor.is_alive`

---

### 4. TrafficManager Persistence
**Behavior**:
- TMs outlive environment instances
- Creating multiple TMs leads to registry conflicts
- **Solution**: Use single TM port (8000) for all phases

**Reference**: CARLA TrafficManager API - `shut_down()` method

---

### 5. Simplicity Wins
**Phase-Based Architecture** is simpler and safer:
- ‚úÖ Single environment instance
- ‚úÖ Single TrafficManager
- ‚úÖ No actor lifecycle complexity
- ‚úÖ Proven pattern (EXPLORATION ‚Üí LEARNING ‚Üí EVAL)

**Reference**: EVAL_ARCHITECTURE_COMPARISON.md

---

## üìö References

### CARLA 0.9.16 Documentation
- Python API: https://carla.readthedocs.io/en/latest/python_api/
- `carla.Client`: https://carla.readthedocs.io/en/latest/python_api/#carla.Client
- `carla.World`: https://carla.readthedocs.io/en/latest/python_api/#carla.World
- `carla.Actor`: https://carla.readthedocs.io/en/latest/python_api/#carla.Actor
- `carla.TrafficManager`: https://carla.readthedocs.io/en/latest/python_api/#carla.TrafficManager

### TD3 Paper & Implementation
- Paper: Fujimoto et al., "Addressing Function Approximation Error in Actor-Critic Methods" (ICML 2018)
- Official code: `TD3/main.py` (eval_policy function)
- Stable-Baselines3 TD3: https://stable-baselines3.readthedocs.io/en/master/modules/td3.html

### OpenAI Gym
- Gym API: https://gymnasium.farama.org/
- Environment lifecycle: https://gymnasium.farama.org/api/env/

### Project Documentation
- EVAL_PHASE_SOLUTION_ANALYSIS.md (comprehensive analysis)
- EVAL_ARCHITECTURE_COMPARISON.md (visual diagrams)
- EVAL_SOLUTION_SUMMARY.md (executive summary)
- ANALYSIS_TODO_LIST.md (Issue #2: CARLA Vehicle State Corruption)

---

## ‚úÖ Status

**Implementation**: ‚úÖ **COMPLETE**
**Testing**: ‚è≠Ô∏è **PENDING** (awaiting user to run validation tests)
**Next Steps**:
1. Run 100-step micro-validation test
2. Run 5K full validation test
3. Verify TensorBoard metrics continuity
4. Document results in EVAL_FIX_VALIDATION_REPORT.md

---

**Implementation completed following**:
- ‚úÖ CARLA 0.9.16 official documentation
- ‚úÖ TD3 paper best practices (deterministic evaluation)
- ‚úÖ Stable-Baselines3 TD3 architecture
- ‚úÖ OpenAI Gym standards
- ‚úÖ PyTorch best practices (gradient flow)
- ‚úÖ CNN feature extraction standards

**Code quality**:
- ‚úÖ Clear comments with references
- ‚úÖ Comprehensive logging
- ‚úÖ Vehicle state validation
- ‚úÖ Episode tracking preservation
- ‚úÖ Fresh observation reset after EVAL

---

**End of Implementation Summary**
