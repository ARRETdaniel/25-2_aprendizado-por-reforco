# GPU Memory Optimization Guide

## Problem Statement

When running CARLA (5.4GB) + PyTorch TD3 agent on a 6GB GPU (RTX 2060), we encounter CUDA Out of Memory errors:

```
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB.
GPU 0 has a total capacity of 5.61 GiB of which 6.94 MiB is free.
```

This guide documents the **official PyTorch CUDA memory optimization** techniques based on:
https://pytorch.org/docs/stable/notes/cuda.html#environment-variables

---

## Root Cause Analysis

### Memory Breakdown
| Component | Memory Usage | Percentage |
|-----------|--------------|------------|
| CARLA Server | 5.4GB | 90% |
| Xorg (Display) | 85MB | 1.4% |
| PyTorch Model | 50MB | 0.8% |
| Training Batch (256) | 400MB | 6.7% |
| **TOTAL REQUIRED** | **5.94GB** | **99%** |
| Available | 6.00GB | 100% |

### Why Small Allocations Fail
Even when total memory appears available, **memory fragmentation** causes small allocations to fail:

```
Requested: 20MB
Available: 6.94MB (but in small fragments!)
Result: OOM Error
```

PyTorch normally allocates memory in **fixed 2MB segments**. With dynamic batch sizes and varying tensor shapes, this creates hundreds of small "memory slivers" that cannot be efficiently reused.

---

## Solution 1: PyTorch Memory Allocator Optimization ‚≠ê RECOMMENDED

### A. Expandable Segments (Primary Fix)

**What it does:**
- Allows memory segments to grow dynamically instead of fixed 2MB chunks
- Reduces fragmentation by 60-80%
- Recovers 200-500MB of usable memory

**How to use:**
```bash
# Set environment variable before running Python
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Or in Python (before importing torch)
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import torch
```

**For Docker:**
```bash
docker run --gpus all \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  td3-av-system:latest \
  python scripts/train_td3.py
```

### B. Additional Optimizations (Combine for Best Results)

```bash
# Combined configuration
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:128,garbage_collection_threshold:0.8"
```

**Parameters explained:**

| Parameter | Value | Effect |
|-----------|-------|--------|
| `expandable_segments` | `True` | Dynamic segment growth (reduces fragmentation by 60-80%) |
| `max_split_size_mb` | `128` | Don't split blocks larger than 128MB (reduces fragmentation) |
| `garbage_collection_threshold` | `0.8` | Reclaim memory when usage exceeds 80% (proactive cleanup) |

### C. Backend Selection (Advanced)

```bash
# Default (recommended for most cases)
PYTORCH_CUDA_ALLOC_CONF=backend:native

# CUDA 11.4+ built-in async allocator (requires CUDA 11.4+)
PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync
```

We use CUDA 12.1, so `cudaMallocAsync` is available, but `native` with `expandable_segments` is usually better for our workload.

---

## Solution 2: Reduce Batch Size

**Change:**
```yaml
# config/td3_config.yaml
batch_size: 256  # Original

# config/td3_config_lowmem.yaml
batch_size: 64   # Reduced
```

**Impact:**
- Saves ~300MB GPU memory
- Slightly slower training (4x fewer samples per update)
- May affect training stability (smaller gradient estimates)

**When to use:**
- When Solution 1 alone is insufficient
- When memory is still tight after applying expandable_segments

---

## Solution 3: Memory Monitoring and Debugging

Add memory monitoring to understand where memory is used:

```python
import torch

def log_gpu_memory(prefix=""):
    """Log GPU memory usage for debugging OOM issues."""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3  # GB
        print(f"üß† {prefix} GPU Memory:")
        print(f"   Allocated: {allocated:.2f}GB")
        print(f"   Reserved: {reserved:.2f}GB")
        print(f"   Peak: {max_allocated:.2f}GB")

# Use in code
log_gpu_memory("Before model init")
model = TD3Agent(...)
log_gpu_memory("After model init")

# Clear cache manually
torch.cuda.empty_cache()
log_gpu_memory("After cache clear")
```

---

## Solution 4: CARLA Quality Reduction (Fallback)

If all else fails, reduce CARLA's memory usage:

```bash
# Run CARLA with Low quality preset
docker run --gpus all -p 2000-2002:2000-2002 \
  carlasim/carla:0.9.16 \
  ./CarlaUE4.sh -quality-level=Low

# Expected savings: ~500-700MB (5.4GB ‚Üí 4.7GB)
```

**Trade-off:**
- Reduces visual fidelity
- Affects texture quality and rendering
- Still suitable for training (physics unaffected)

---

## Implementation Checklist

### For Testing (100 steps, visual display)
- [x] Add `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to `run_visual_test_docker.sh`
- [x] Add memory monitoring to `test_visual_navigation.py`
- [x] Use `td3_config_lowmem.yaml` (batch_size=64)
- [ ] Restart CARLA to clear any memory fragmentation
- [ ] Run visual test: `bash scripts/run_visual_test_docker.sh`
- [ ] Check memory logs to verify optimization worked

### For Training (1M steps, headless)
- [x] Create `td3_config_lowmem.yaml` with batch_size=64
- [ ] Update Docker Compose to include `PYTORCH_CUDA_ALLOC_CONF`
- [ ] Add memory monitoring to training script
- [ ] Run initial training test (1000 steps) to verify memory stability
- [ ] If stable ‚Üí proceed with full training
- [ ] If OOM persists ‚Üí apply CARLA quality reduction

---

## Expected Results

### Before Optimization
```
CARLA: 5.4GB (90%)
PyTorch: 20MB allocation fails
Result: CUDA Out of Memory
```

### After Optimization (expandable_segments + batch_size=64)
```
CARLA: 5.4GB (90%)
PyTorch Models: 50MB (0.8%)
Training Batch (64): 150MB (2.5%)
Overhead: 100MB (1.7%)
Total: 5.7GB (95% of 6GB) ‚úÖ FITS!
```

### Memory Timeline
```
Initial:              0.5GB (PyTorch base)
After CARLA env:     +0.0GB (CARLA on GPU, not PyTorch)
After TD3 agent:     +0.05GB (Model weights)
After first batch:   +0.15GB (Batch + gradients)
Peak (training):      5.7GB (CARLA + PyTorch)
```

---

## Verification Commands

### Check CUDA Memory
```bash
# Before starting
nvidia-smi

# During training (in another terminal)
watch -n 1 nvidia-smi
```

### Check PyTorch Allocator Config
```python
import torch
import os

# Check environment variable
print(f"PYTORCH_CUDA_ALLOC_CONF: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF', 'Not set')}")

# Check current memory
if torch.cuda.is_available():
    print(f"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB")
    print(f"Reserved: {torch.cuda.memory_reserved()/1e9:.2f}GB")

    # Get detailed stats
    stats = torch.cuda.memory_stats()
    print(f"Peak allocated: {stats['allocated_bytes.all.peak']/1e9:.2f}GB")
```

---

## Troubleshooting

### Still Getting OOM After Applying expandable_segments?

1. **Restart CARLA**
   ```bash
   docker-compose down
   docker-compose up -d carla
   ```
   This clears any memory fragmentation from previous runs.

2. **Reduce batch size further**
   ```yaml
   batch_size: 32  # Try even smaller
   ```

3. **Use CARLA Low quality**
   ```bash
   # Edit docker-compose.yml
   command: ./CarlaUE4.sh -RenderOffScreen -quality-level=Low
   ```

4. **Check for other GPU processes**
   ```bash
   nvidia-smi
   # Kill any unnecessary processes using GPU
   ```

5. **Use CPU for agent (last resort)**
   ```python
   agent = TD3Agent(..., device='cpu')
   # CARLA stays on GPU, agent on CPU
   # Very slow but guaranteed to work
   ```

---

## References

- **PyTorch CUDA Memory Management:**
  https://pytorch.org/docs/stable/notes/cuda.html#environment-variables

- **PyTorch Memory Allocator Documentation:**
  https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management

- **CARLA Performance Tuning:**
  https://carla.readthedocs.io/en/latest/adv_rendering_options/

- **TD3 Paper (Original Implementation):**
  Fujimoto et al. 2018 - https://arxiv.org/abs/1802.09477

---

## Summary

**Quick Start (6GB GPU):**
```bash
# 1. Set environment variable
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:128,garbage_collection_threshold:0.8"

# 2. Use low-memory config
python scripts/train_td3.py --config config/td3_config_lowmem.yaml

# 3. Monitor memory
watch -n 1 nvidia-smi
```

**Expected Result:**
- ‚úÖ Training runs without OOM
- ‚úÖ Memory usage stays below 6GB
- ‚úÖ No performance degradation (same speed as batch_size=256)

**If still OOM:**
- Reduce batch_size to 32
- Use CARLA Low quality preset
- Consider using HPC cluster for full training (A100 40GB)

---

*Document created: 2025-01-XX*
*Last updated: 2025-01-XX*
*Author: Daniel Terra Gomes*
