# Systematic Fix Plan: Reward Validation Issues

**Project:** AV TD3 System - End-to-End Visual Autonomous Navigation  
**Created:** November 24, 2025  
**Last Updated:** November 24, 2025  
**Status:** üîÑ IN PROGRESS (Phase 4 - Validation Testing)

---

## Overview

This document tracks the systematic investigation and fixing of reward function issues identified during manual validation testing with `validate_rewards_manual.py`.

**Context:** After implementing the baseline controller and off-road detection fix, we are now systematically validating and fixing reward function issues to ensure TD3 training stability.

**Methodology:** Follow official documentation (TD3 paper, CARLA API, Gymnasium) for each issue, with structured phases:
1. **Documentation Research** - Prove if issue matters
2. **Investigation** - Root cause analysis
3. **Implementation** - Fix with proper documentation
4. **Validation** - Manual testing
5. **Documentation** - Create investigation reports

---

## Issue Tracking

### ‚úÖ Issue 3.1: Progress Reward Discontinuity [FIXED]

**Status:** ‚úÖ IMPLEMENTATION COMPLETE - Ready for Phase 4 validation  
**Priority:** HIGH (blocks TD3 training)  
**Discovery Date:** November 24, 2025  
**Fix Date:** November 24, 2025

**Description:**
Progress reward oscillates to 0.0 for ~0.5 seconds (~10 steps at 20 FPS) during normal forward driving, then jumps back to ~10.0. Creates discontinuity harmful to TD3 learning.

**Observable Behavior:**
- Reward pattern: 10.0 ‚Üí 0.0 ‚Üí 10.0 (repeats during driving)
- Duration: ~0.5 seconds (‚âà10 steps)
- Context: Vehicle moving correctly, no actual progress stoppage

**Root Cause:**
`WaypointManager._find_nearest_segment()` returns `None` when:
1. Vehicle >20m from any route segment (off-road exploration)
2. Waypoint search window misses vehicle (¬±2 behind, +10 ahead range issue)
3. First few steps before `current_waypoint_idx` stabilizes

Previous safety check (Nov 23) returned `0.0` when `distance_to_goal` was `None`, creating the exact discontinuity it tried to prevent.

**Impact Analysis:**
- TD3 paper Section 3.1: "accumulation of error" in temporal difference learning
- Variance: œÉ¬≤ = 25 (for 10‚Üí0‚Üí10 oscillation)
- Accumulated variance over horizon=100: ‚âà 2,475 (CATASTROPHIC)
- Result: Unstable Q-values, divergent behavior, training failure

**Solution Implemented:**
Temporal smoothing filter (Option A from investigation):
- Use `prev_distance_to_goal` when current is `None` (maintains continuity)
- Track None occurrences with `none_count` diagnostic counter
- Log error if None persists >50 steps (detects waypoint manager bugs)
- Add `none_count` reset in `reset()` method (per-episode tracking)

**Files Modified:**
- `src/environment/reward_functions.py` (lines ~983-1060, ~1157)

**Benefits:**
- ‚úÖ Eliminates discontinuity (œÉ¬≤ = 25 ‚Üí œÉ¬≤ ‚âà 0)
- ‚úÖ Maintains TD3 learning stability (no variance accumulation)
- ‚úÖ Detects persistent waypoint manager failures
- ‚úÖ Backwards compatible with valid distance values

**Tradeoffs:**
- ‚ö†Ô∏è Masks underlying waypoint manager search window bug
- ‚ö†Ô∏è Vehicle could drift off-road without penalty for <50 steps
  - Mitigation: Off-road detection (`OffroadDetector`) already penalizes in safety reward

**Documentation:**
- ‚úÖ `PHASE_1_DOCUMENTATION.md` - TD3 paper variance proof
- ‚úÖ `PHASE_2_INVESTIGATION.md` - Root cause analysis
- ‚úÖ `PHASE_3_IMPLEMENTATION.md` - Fix details and code changes
- üîÑ `PHASE_4_VALIDATION_CHECKLIST.md` - Testing plan (ready to execute)

**Validation Plan:**
- Scenario 1: Normal forward driving (no 0.0 spikes)
- Scenario 2: Sharp turns (smoothing maintains continuity)
- Scenario 3: Off-road exploration (diagnostic logging works)
- Scenario 4: Recovery after off-road (proper reset)
- Scenario 5: Episode start (first step handling)
- Scenario 6: Persistent failure detection (>50 steps)

**Next Steps:**
1. üîÑ Execute Phase 4 validation testing
2. ‚èπÔ∏è Document results in `PHASE_4_VALIDATION.md`
3. ‚èπÔ∏è Create Git commit following conventional commit format
4. ‚èπÔ∏è Mark Issue 3.1 as RESOLVED

---

### ‚èπÔ∏è Issue 1: Safety Penalty Persistence [PENDING]

**Status:** ‚èπÔ∏è NOT STARTED  
**Priority:** MEDIUM  
**Discovery Date:** November 24, 2025

**Description:**
Safety penalty (lane invasion, off-road) persists after vehicle recovers to correct lane/road position.

**Sub-Issues:**
- 1.1: Lane invasion penalty not clearing after recovery
- 1.2: No penalty when stopped on invaded lane
- 1.3: Moving on center line gives +0 safety reward (should be positive)
- 1.4: Safety reward stays negative after recovery
- 1.5: No positive safety reward while moving correctly
- 1.6: Safety reward calculation inconsistent with lane keeping goal

**Investigation Plan:**
1. Phase 1: Research Gymnasium reward shaping best practices
2. Phase 2: Investigate `_calculate_safety_reward()` logic
3. Phase 3: Implement hysteresis/recovery mechanism
4. Phase 4: Validate with manual testing
5. Phase 5: Document fix

**Blocked By:** Issue 3.1 (priority - discontinuity more critical for TD3)

---

### ‚èπÔ∏è Issue 2: Comfort Reward Calculation [PENDING]

**Status:** ‚èπÔ∏è NOT STARTED  
**Priority:** LOW  
**Discovery Date:** November 24, 2025 (suspected, not confirmed)

**Description:**
Comfort reward (jerk minimization) may have calculation issues or threshold problems.

**Investigation Plan:**
1. Phase 1: Research jerk calculation best practices
2. Phase 2: Validate `_calculate_comfort_reward()` implementation
3. Phase 3: Tune thresholds based on CARLA vehicle dynamics
4. Phase 4: Validate with manual testing
5. Phase 5: Document findings

**Blocked By:** Issue 3.1 (priority), Issue 1 (related to overall reward)

---

## Lessons Learned

### From Issue 3.1 (Progress Reward Discontinuity)

1. **Safety Checks Can Introduce Discontinuity:**
   - Nov 23 safety check (return 0.0 when None) created the exact problem it tried to prevent
   - **Takeaway:** Always consider temporal continuity when adding safety checks in reward functions

2. **Temporal Smoothing vs. Masking Bugs:**
   - Temporal smoothing (using previous value) maintains continuity but can mask underlying bugs
   - **Tradeoff:** Accept smoothing for training stability, but add diagnostics to detect masked failures

3. **Search Window Assumptions Break:**
   - Waypoint manager assumes `current_waypoint_idx` changes slowly (¬±2 behind, +10 ahead)
   - During exploration or fast movements, this assumption breaks
   - **Takeaway:** Adaptive search or global fallback needed for robustness

4. **TD3 Paper Predictions Validated:**
   - Phase 1 documentation predicted discontinuity would harm training (œÉ¬≤ = 25 ‚Üí accumulated variance ‚âà 2,475)
   - Investigation confirmed the source and validated theoretical analysis
   - **Takeaway:** Theoretical analysis (TD3 paper) correctly predicts practical issues

5. **Systematic Approach Works:**
   - 5-phase structured investigation (Documentation ‚Üí Investigation ‚Üí Implementation ‚Üí Validation ‚Üí Documentation)
   - Each phase builds on previous, with clear artifacts
   - **Takeaway:** Follow this pattern for all future reward fixes

---

## Future Work

### Short-term (After Issue 3.1 Validation)

1. **Fix Safety Penalty Persistence** (Issue 1)
   - Implement hysteresis mechanism
   - Add recovery detection logic
   - Tune penalty/reward thresholds

2. **Validate Comfort Reward** (Issue 2)
   - Check jerk calculation correctness
   - Tune comfort thresholds
   - Verify against CARLA vehicle dynamics

### Long-term (Separate Issues)

1. **Optimize Waypoint Manager Search Window:**
   - Expand search range: ¬±2‚Üí¬±5 behind, +10‚Üí+20 ahead
   - Add adaptive search based on vehicle speed
   - Implement global fallback if local search fails
   - **Reference:** Issue 3.1 root cause analysis

2. **Tune Off-Road Threshold:**
   - Current: 20m threshold for `_find_nearest_segment()`
   - May be too strict for Town01 road widths
   - **Action:** Analyze Town01 road geometry and adjust

3. **Add Reward Debugging Tools:**
   - Real-time reward visualization (matplotlib plots)
   - Per-component reward logging dashboard
   - Automated reward continuity tests (detect discontinuities)

4. **Consider Reward Normalization:**
   - TD3 may benefit from normalized rewards (e.g., [-1, +1] range)
   - Research best practices in TD3 implementations
   - Test impact on training stability

---

## References

### Documentation Created
- `PHASE_1_DOCUMENTATION.md` - TD3 variance analysis, Gymnasium docs, CARLA API
- `PHASE_2_INVESTIGATION.md` - Root cause analysis for Issue 3.1
- `PHASE_3_IMPLEMENTATION.md` - Temporal smoothing fix implementation
- `PHASE_4_VALIDATION_CHECKLIST.md` - Manual testing plan for Issue 3.1

### Related Files
- `BUG_ROUTE_DISTANCE_INCREASES.md` - Projection method fix (Nov 23)
- `src/environment/reward_functions.py` - Reward calculation implementation
- `src/environment/waypoint_manager.py` - Waypoint projection and search
- `src/environment/sensors.py` - Off-road detection (`OffroadDetector` class)
- `validate_rewards_manual.py` - Manual testing script (keyboard control)

### External References
- **TD3 Paper:** Fujimoto et al., "Addressing Function Approximation Error in Actor-Critic Methods" (2018)
- **CARLA Waypoint API:** https://carla.readthedocs.io/en/latest/core_map/
- **Gymnasium Documentation:** https://gymnasium.farama.org/
- **Vector Projection:** https://en.wikipedia.org/wiki/Vector_projection

---

## Progress Summary

### Completed
- ‚úÖ Issue 3.1 Phase 1: Documentation Research (TD3 variance proof)
- ‚úÖ Issue 3.1 Phase 2: Investigation (root cause identified)
- ‚úÖ Issue 3.1 Phase 3: Implementation (temporal smoothing fix)
- ‚úÖ Created comprehensive documentation (Phases 1-3)
- ‚úÖ Created validation checklist (Phase 4 plan)

### In Progress
- üîÑ Issue 3.1 Phase 4: Validation testing (ready to execute)

### Pending
- ‚èπÔ∏è Issue 3.1 Phase 5: Final documentation and Git commit
- ‚èπÔ∏è Issue 1: Safety penalty persistence investigation
- ‚èπÔ∏è Issue 2: Comfort reward validation

### Blocked
- None (Issue 3.1 cleared for validation)

---

**Last Updated:** November 24, 2025  
**Next Review:** After Issue 3.1 Phase 4 validation completes  
**Owner:** Systematic Investigation Process (following TD3 paper and official docs)
