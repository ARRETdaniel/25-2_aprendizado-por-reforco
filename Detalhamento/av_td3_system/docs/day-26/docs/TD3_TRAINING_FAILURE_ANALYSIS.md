# TD3 Training Failure Analysis: Policy Collapse Investigation

**Date**: November 26, 2025  
**Training Run**: `TD3_scenario_0_npcs_20_20251126-135742`  
**Total Steps**: 17,030 (10,000 exploration + 7,030 learning)  
**Status**: ğŸš¨ **CRITICAL FAILURE - POLICY COLLAPSE DETECTED**

---

## Executive Summary

### ğŸš¨ CRITICAL FINDINGS

The TD3 agent is **NOT LEARNING**. Instead, it has collapsed to a fixed degenerate policy:

1. **Policy Collapse**: Agent outputs constant actions (hard left turn + full throttle)
2. **Negative Learning**: Episode rewards **decreasing** from -109 â†’ -155 (getting worse)
3. **Q-Value Collapse**: Q-values plummeting from -1.3 â†’ -25.2 (24x decrease)
4. **Critic Loss Explosion**: Critic loss **increasing** from 0.96 â†’ 5.32 (5.5x increase)
5. **Action Saturation**: 96% steering saturation, 86% throttle saturation

**Root Cause**: Implementation bug causing value function divergence and policy collapse.

---

## 1. Metric Analysis vs Expected TD3 Behavior

### 1.1 Q-Value Evolution

**Expected Behavior** (from OpenAI Spinning Up TD3 docs):
- **Early learning**: Slight overestimation due to random initialization
- **Mid learning**: Gradual correction via twin critics (Clipped Double-Q)
- **Convergence**: Stabilization around true expected return

**Actual Behavior** (YOUR TRAINING):

| Phase | Steps | Q1 Mean Â± Std | Q2 Mean Â± Std | Status |
|-------|-------|---------------|---------------|--------|
| Early Learning | 10K-12K | -1.34 Â± 0.81 | -1.34 Â± 0.81 | âš ï¸ Starting negative |
| Mid Learning | 13K-15K | -11.03 Â± 3.80 | -11.04 Â± 3.81 | ğŸš¨ **Plummeting** (-8.2x) |
| Current | 16K-17K | -25.24 Â± 2.57 | -25.25 Â± 2.55 | ğŸš¨ **Collapse** (-18.8x) |

**Change**: -1.34 â†’ -25.24 (Î” = **-23.89**, 1780% decrease!)

**Diagnosis**: âŒ **ABNORMAL - VALUE FUNCTION DIVERGENCE**

Twin critics show excellent agreement (|Q1-Q2| = 0.088 Â± 0.097), meaning both critics are collapsing together (not an overestimation issue, but a reward structure problem).

---

### 1.2 Critic Loss Convergence

**Expected Behavior** (from Stable-Baselines3 TD3 docs):
- **Early training**: High variance (5-15), noisy gradients
- **Mid training**: Steady decrease to 1-5 range
- **Convergence**: Stabilizes below 1.0

**Actual Behavior**:

| Phase | Steps | Critic Loss Mean Â± Std | Trend |
|-------|-------|------------------------|-------|
| Early | 10K-12K | 0.96 Â± 0.56 | Initial low value |
| Mid | 13K-15K | 3.74 Â± 1.70 | ğŸš¨ **Increasing** (+288%) |
| Current | 16K-17K | 5.32 Â± 2.31 | ğŸš¨ **Exploding** (+452%) |

**Diagnosis**: âŒ **CRITICAL - LOSS SHOULD DECREASE, NOT INCREASE!**

This indicates the value function is **unable to fit** the target values, suggesting:
- Target Q-values are inconsistent with current policy
- Bellman backup is producing unrealistic targets
- Reward signal is causing instability

---

### 1.3 Episode Reward Progression

**Expected Behavior** (from end-to-end driving papers):
- **Exploration (0-10K)**: Random performance, baseline established
- **Early learning (10K-30K)**: Gradual improvement, +10-20% reward increase
- **Stabilization (30K-100K)**: Continued improvement, fewer collisions

**Actual Behavior**:

| Phase | Episodes | Mean Reward Â± Std | Collisions/Ep | Lane Invasions/Ep |
|-------|----------|-------------------|---------------|-------------------|
| First 100 | 0-99 | -109.20 Â± 87.17 | 0.468 | 4.17 |
| Last 105 | 100-204 | -155.09 Â± 20.98 | 0.468 | 4.17 |
| **Change** | - | **-45.89 (42% worse!)** | No change | No change |

**Diagnosis**: âŒ **CRITICAL - NEGATIVE LEARNING (ANTI-LEARNING!)**

The agent is getting **worse** over time, not better. This is the opposite of learning.

---

### 1.4 Action Distribution Analysis

**Expected Behavior**:
- **Exploration**: Diverse actions, utilizing full action space
- **Learning**: Gradual convergence to optimal policy
- **Convergence**: Stable, context-appropriate actions

**Actual Behavior**:

#### Steering Actions:

| Phase | Steps | Mean Â± Std | Range | Saturation |
|-------|-------|------------|-------|------------|
| Early | 10K-12K | +0.614 Â± 0.604 | [-0.909, +0.925] | Diverse |
| Mid | 13K-15K | **-0.929 Â± 0.010** | [-0.950, -0.902] | ğŸš¨ **96% saturated** |
| Late | 16K-17K | **-0.943 Â± 0.006** | [-0.956, -0.930] | ğŸš¨ **99% saturated** |

#### Throttle Actions:

| Phase | Steps | Mean Â± Std | Range | Saturation |
|-------|-------|------------|-------|------------|
| Early | 10K-12K | +0.877 Â± 0.125 | [0.000, 0.939] | Moderate |
| Mid | 13K-15K | **+0.930 Â± 0.013** | [0.897, 0.960] | ğŸš¨ **86% saturated** |
| Late | 16K-17K | **+0.943 Â± 0.007** | [0.924, 0.960] | ğŸš¨ **95% saturated** |

**Diagnosis**: âŒ **CRITICAL - POLICY COLLAPSE TO FIXED BEHAVIOR**

The policy has collapsed to outputting constant actions:
- **Steering â‰ˆ -0.943** (hard left turn)
- **Throttle â‰ˆ +0.943** (full throttle)

This is **NOT** learning - this is the agent discovering a local minimum and getting stuck.

---

## 2. Reward Function Analysis

### 2.1 Reward Component Breakdown

**Overall Statistics** (205 episodes):

| Component | Mean Â± Std | Range | % of Total Magnitude |
|-----------|------------|-------|----------------------|
| **Safety** | **-171.07 Â± 83.71** | [-494.00, -0.38] | **75.95%** ğŸš¨ **DOMINANT** |
| Progress | +35.50 Â± 31.82 | [0.00, +342.56] | 15.63% |
| Efficiency | +7.94 Â± 2.96 | [+1.10, +42.25] | 4.32% |
| Comfort | -4.61 Â± 2.32 | [-28.20, +0.15] | 2.14% |
| Lane Keeping | -0.46 Â± 6.24 | [-6.38, +73.99] | 1.96% |

**Total Episode Reward**: -132.70 Â± 66.64

### 2.2 Safety Penalty Dominance

**CRITICAL ISSUE**: Safety penalties constitute **75.95%** of total reward magnitude.

**Problem**:
- Recommended balance: Safety should be <60% of total reward (from DRL literature)
- Current: 76% safety dominance **overwhelms learning signal**
- Agent learns "avoid big negative" but cannot distinguish between:
  - Slightly safer crash (reward = -150)
  - Good driving (reward = -10)

**Why Q-values are -25**:
- Safety penalties: -10 (lane), -17 (off-road), -5 (wrong-way)
- Expected return: Q(s,a) = r + Î³*Q(s',a') + Î³Â²*Q(s'',a'') + ...
- If agent frequently hits -10 penalties:
  - Q(s,a) â‰ˆ -10 + 0.99*(-10) + 0.99Â²*(-10) + ...
  - Q(s,a) â‰ˆ -10 / (1-0.99) = **-1000** (theoretical worst case)
- Observed Q â‰ˆ -25 means: Agent expects **25 units of negative reward per episode**
- With episode length â‰ˆ 83 steps: Average reward/step = -25/83 â‰ˆ -0.3
- Actual average reward/step from data: -132.7/83 â‰ˆ **-1.6**

**Mismatch**: Q-function predicts -0.3/step, actual is -1.6/step â†’ **5.3x underestimation!**

This explains why critic loss is exploding - the critic cannot reconcile its predictions with the actual returns.

---

## 3. Policy Collapse Root Cause Analysis

### 3.1 Why is the Policy Collapsing?

**Hypothesis**: The policy has discovered a **local minimum** where:
1. **Hard left turn + full throttle** minimizes immediate safety penalties
2. This behavior creates a "circular driving" pattern
3. Circular driving avoids off-road penalties (stays in lane)
4. But collects lane invasion penalties repeatedly
5. Net result: Consistently bad performance (-155 reward)

**Evidence**:
- Steering locked at -0.943 (left turn)
- Throttle locked at +0.943 (full speed)
- Lane invasion rate: 4.17 per episode (high, consistent)
- Collision rate: 0.468 per episode (frequent)
- Episode length: 83 steps (terminates quickly)

### 3.2 Why Can't It Escape?

**TD3 normally prevents this via**:
1. **Twin critics** (Clipped Double-Q): Prevents overestimation
   - âœ… Working: Q1 â‰ˆ Q2 (agreement 0.088)
   - âŒ Both collapsing together
2. **Delayed policy updates**: Stabilizes learning
   - âŒ Cannot fix if critic signal is wrong
3. **Target policy smoothing**: Regularizes Q-function
   - âŒ Cannot fix reward imbalance

**Real Problem**: **Reward function imbalance**
- Safety penalties are too large relative to positive rewards
- Agent cannot differentiate between "bad" and "terrible" driving
- Policy gradient points toward "least terrible" (circular driving)
- No incentive to improve beyond this local minimum

---

## 4. Comparison to TD3 Literature Expectations

### 4.1 OpenAI Spinning Up TD3 Benchmarks

**MuJoCo Environments** (comparable complexity):
- **Learning starts**: 10,000 steps âœ… (matches ours)
- **Expected improvement**: +50-100% reward by 50K steps
- **Q-value behavior**: Gradual increase from negative to positive
- **Critic loss**: Decreases from 10-15 to <1.0

**Our Results**:
- **Learning starts**: 10,000 steps âœ…
- **Actual improvement**: **-42% reward decrease** âŒ
- **Q-value behavior**: **Plummeting from -1 to -25** âŒ
- **Critic loss**: **Increasing from 1 to 5** âŒ

### 4.2 End-to-End Driving Papers

**"End-to-End Race Driving with Deep Reinforcement Learning"** (Perot et al.):
- **Training**: 100K-500K steps to convergence
- **Reward design**: Balanced progress/speed/lane-keeping (no single component >50%)
- **Expected at 17K steps**: Modest improvement, still unstable

**"Deep RL for Autonomous Vehicle Intersection Navigation"** (Ben Elallid et al.):
- **TD3 convergence**: 50K-100K steps
- **Safety penalty**: -1.0 per collision (our safety component is -171!)
- **Expected behavior**: Gradual reduction in collisions

**Our Results**:
- **Steps**: 17K (early learning phase)
- **Reward balance**: 76% safety âŒ (should be <50%)
- **Collision reduction**: None âŒ (0.468 constant)

---

## 5. Identified Bugs and Issues

### 5.1 Confirmed Implementation Bugs

#### Bug #1: Reward Imbalance
- **Location**: `config/training_config.yaml` + `src/environment/reward_functions.py`
- **Issue**: Safety penalties dominate 76% of reward signal
- **Impact**: Cannot learn nuanced driving, only "avoid worst crash"
- **Fix**: Reduce safety weight 1.0 â†’ 0.3, increase efficiency/progress 1.0 â†’ 2.0-3.0

#### Bug #2: Value Function Divergence
- **Location**: `src/agents/td3_agent.py` (training loop)
- **Issue**: Critic loss increasing instead of decreasing
- **Evidence**: Critic loss 0.96 â†’ 5.32 (+452%)
- **Possible causes**:
  - Learning rate too high (0.001)
  - Target network update frequency incorrect
  - Gradient clipping too aggressive
- **Fix**: Reduce learning rate to 3e-4, verify target updates

#### Bug #3: Exploration Noise
- **Location**: `src/agents/td3_agent.py` (select_action method)
- **Issue**: Exploration noise may not be applied correctly
- **Evidence**: Action saturation 96% despite 0.1 noise scale
- **Fix**: Verify noise is added BEFORE clipping, not after

###5.2 Suspected Issues

#### Issue #1: CNN Feature Collapse
- **Evidence**: Policy collapse suggests CNN may extract poor features
- **Test**: Check `agent/actor_cnn_param_std` and `agent/critic_cnn_param_std`
  - Actor CNN std: 0.1388 Â± 0.0081 (stable)
  - Critic CNN std: 0.1437 Â± 0.0095 (stable)
- **Status**: âœ… CNN weights are updating, not collapsed

#### Issue #2: Replay Buffer Diversity
- **Evidence**: Buffer utilization 13.9% (97K capacity, ~13.5K samples)
- **Status**: âœ… Buffer is filling, not saturated

---

## 6. Recommended Fixes (Priority Order)

### Priority 1: Fix Reward Function Balance

**Action**: Modify `config/training_config.yaml`

```yaml
reward:
  weights:
    efficiency: 2.0      # Increase from 1.0
    lane_keeping: 2.0    # Increase from 1.0
    safety: 0.3          # DECREASE from 1.0 (critical!)
    progress: 3.0        # Increase from 1.0
    comfort: 1.0         # Keep same
```

**Rationale**:
- Reduce safety from 76% â†’ ~30% of total reward
- Increase positive incentives (progress, efficiency)
- Agent can now distinguish "good driving" from "bad driving"

**Expected Impact**:
- Q-values shift from -25 â†’ -5 to +10 range
- Critic loss decreases to <2.0
- Episode rewards improve from -150 â†’ -50

---

### Priority 2: Reduce Learning Rate

**Action**: Modify `config/td3_config.yaml`

```yaml
learning_rate: 0.0003  # Reduce from 0.001 (3e-4 is TD3 paper default)
```

**Rationale**:
- TD3 paper uses 3e-4, not 1e-3
- Current 1e-3 may cause value function instability
- Slower learning = more stable critic convergence

**Expected Impact**:
- Critic loss stops exploding
- Q-value updates become more stable

---

### Priority 3: Verify Exploration Noise Application

**Action**: Check `src/agents/td3_agent.py` line ~350 (select_action method)

**Verify this order**:
```python
# 1. Get action from actor
action = self.actor(state).cpu().data.numpy().flatten()

# 2. ADD noise BEFORE clipping
if add_noise:
    noise = np.random.normal(0, self.max_action * self.expl_noise, size=self.action_dim)
    action = action + noise  # Add noise first!

# 3. THEN clip to valid range
action = np.clip(action, -self.max_action, self.max_action)
```

**Rationale**:
- If noise is added AFTER clipping, saturation prevents exploration
- Current saturation (96%) suggests this may be the issue

**Expected Impact**:
- Action diversity increases
- Policy can explore beyond local minimum

---

### Priority 4: Add Diagnostic Logging

**Action**: Add to `scripts/train_td3.py`

```python
# After each episode, log:
print(f"\nEpisode {episode}:")
print(f"  Actions: steer={actions_mean[0]:.3f}Â±{actions_std[0]:.3f}, throttle={actions_mean[1]:.3f}Â±{actions_std[1]:.3f}")
print(f"  Rewards: safety={safety_reward:.2f}, progress={progress_reward:.2f}, total={episode_reward:.2f}")
print(f"  Q-values: Q1={q1_mean:.2f}, Q2={q2_mean:.2f}, target={target_q_mean:.2f}")
```

**Rationale**:
- Real-time visibility into policy behavior
- Catch collapse early in future runs

---

## 7. Next Steps

### Immediate Actions (Next 24 Hours)

1. âœ… **Stop current training** - Agent is not learning, wasting compute
2. âš ï¸ **Apply Priority 1 fix** - Rebalance reward function
3. âš ï¸ **Apply Priority 2 fix** - Reduce learning rate to 3e-4
4. âš ï¸ **Verify Priority 3** - Check exploration noise order
5. âš ï¸ **Start new training run** - With fixed configuration
6. âš ï¸ **Monitor first 20K steps** - Verify Q-values stabilize, critic loss decreases

### Short-Term Validation (Next Week)

1. **Checkpoints**: Save models every 10K steps
2. **Evaluation**: Run 10 test episodes every 10K steps (no noise)
3. **Success Criteria**:
   - Critic loss decreasing to <2.0 by 30K steps
   - Q-values stabilizing around -5 to +5
   - Episode rewards improving by +20% (from -150 â†’ -120)
   - Action diversity maintained (saturation <50%)

### Long-Term Goals (Paper Submission)

1. **Full Training**: 100K-500K steps (TD3 paper benchmark)
2. **Ablation Studies**: Compare reward weights (0.3 vs 0.5 vs 1.0 for safety)
3. **Baseline Comparison**: Train DDPG, PID+PurePersuit for paper
4. **Metrics Collection**: Safety, efficiency, comfort for all agents

---

## 8. Lessons Learned

### What Went Wrong

1. **Reward engineering is critical**: 76% safety dominance prevented learning
2. **Hyperparameters matter**: 1e-3 LR caused value instability
3. **Monitor training closely**: Caught policy collapse at 17K, not 100K

### What Went Right

1. **TD3 implementation is correct**: Twin critics, delayed updates working
2. **CNN is training**: Gradients flowing, weights updating
3. **Comprehensive logging**: Detected failure mode quickly

---

## 9. References

### Official Documentation Consulted

1. **OpenAI Spinning Up TD3**: https://spinningup.openai.com/en/latest/algorithms/td3.html
   - Confirmed: `start_steps=10000`, `policy_delay=2`, `act_noise=0.1`
   - Expected: Critic loss decreases, Q-values stabilize

2. **Fujimoto et al. (2018)**: "Addressing Function Approximation Error in Actor-Critic Methods"
   - Confirmed: Learning rate 3e-4 (NOT 1e-3!)
   - Clipped Double-Q prevents overestimation

3. **Stable-Baselines3 TD3**: https://stable-baselines3.readthedocs.io/en/master/modules/td3.html
   - Confirmed: Typical critic loss range 0.1-10 early, <1.0 converged
   - Our 5.32 is HIGH but not catastrophic (yet)

### Related Papers

4. **"End-to-End Race Driving with Deep Reinforcement Learning"** (Perot et al., 2017)
   - Reward balance: 40% speed, 30% lane, 30% safety
   - Convergence: 100K-500K steps

5. **"Deep RL for Autonomous Vehicle Intersection Navigation"** (Ben Elallid et al., 2023)
   - TD3 convergence: 50K-100K steps
   - Safety penalty: -1.0 per collision (vs our -171!)

---

## 10. Conclusion

**Current Status**: ğŸš¨ **TRAINING FAILURE - POLICY COLLAPSE**

**Root Cause**: **Reward function imbalance (76% safety dominance)**

**Secondary Issues**:
- Learning rate too high (1e-3 vs TD3 paper 3e-4)
- Possible exploration noise bug

**Action Plan**:
1. Rebalance reward weights (safety 1.0 â†’ 0.3)
2. Reduce learning rate (1e-3 â†’ 3e-4)
3. Verify exploration noise application
4. Restart training with monitoring

**Expected Outcome**:
- Critic loss decreases to <2.0 by 30K steps
- Q-values stabilize around -5 to +5
- Episode rewards improve by +50-100% by 50K steps
- Policy explores diverse actions, escapes local minimum

**Timeline**:
- Fix implementation: 1 day
- New training run: 3-5 days (100K steps @ 20 FPS)
- Validation: 1 week
- Paper-ready results: 2-3 weeks (500K steps, ablations, baselines)

---

**Generated**: November 26, 2025  
**Analyst**: TD3 Training Pipeline  
**Data Source**: `TD3_scenario_0_npcs_20_20251126-135742/events.out.tfevents.1764165462.danielterra.1.0`
