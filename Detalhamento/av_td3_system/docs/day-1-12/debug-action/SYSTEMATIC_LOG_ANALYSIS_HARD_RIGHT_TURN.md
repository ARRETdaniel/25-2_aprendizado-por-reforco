# üö® SYSTEMATIC LOG ANALYSIS - Hard-Right-Turn Root Cause Investigation

**Analysis Date:** December 1, 2025  
**Log File:** debug-action.log (2000 training steps, 181,293 lines)  
**Issue:** Agent produces **maximum right steering (+1.000)** consistently during learning phase  
**Status:** üî¥ CRITICAL - Problem persists despite reward order fix

---

## Executive Summary

After a 2000-step debug training run, the hard-right-turn behavior **PERSISTS** when learning phase begins at step 1000. The agent outputs **maximum steering (+1.000)** consistently at steps 1100, 1200, and 1300, with progressively increasing throttle values. Most critically, the agent receives **POSITIVE rewards (+1.99 to +5.53)** for this incorrect behavior, with the **progress reward component dominating** at 84% of total reward (+4.65 out of +5.53).

### Critical Discoveries

1. **Hard-Right Pattern:** Agent outputs steer=+1.000 (maximum) at EVERY diagnostic point in learning phase
2. **Reward Signal Broken:** Agent receives POSITIVE rewards for wrong behavior (up to +5.53)
3. **Progress Dominates:** Progress component is +4.65 (84% of total reward)
4. **Action Buffer Bug:** `get_action_stats()` returns all zeros despite actual actions being +1.000
5. **Lateral Deviation:** Car deviates 0.60m to 1.98m from center, yet lane_keeping reward is POSITIVE (+0.44)

---

## 1. Action Pattern Evolution

### Exploration Phase (Steps 0-1000): ‚úÖ NORMAL

**Step 1000 (Last exploration step):**
```
[DIAGNOSTIC][Step 1000] POST-ACTION OUTPUT:
  Current action: steer=+0.033, throttle/brake=+0.278
  Rolling stats (last 100): steer_mean=+0.000, steer_std=0.000
  ‚úì Balanced steering

[DEBUG Step 1000] Act=[steer:+0.033, thr/brk:+0.278] | Rew=-0.15 | Speed=0.0 km/h
[DEBUG Step 1000] Episode 1 Episode DONE: Episode 1, TRUNCATED (max_episode_steps=1000)
[PHASE TRANSITION] Starting LEARNING phase at step 1,000
```

**Analysis:**
- ‚úÖ Normal steering values (-0.1 to +0.1 range)
- ‚úÖ Episode truncated normally at max_episode_steps
- ‚úÖ Clean transition to learning phase

---

### Learning Phase (Steps 1001+): ‚ùå BROKEN - Maximum Right Steering

#### Step 1100 - First Diagnostic in Learning Phase

**Action Output:**
```
[DIAGNOSTIC][Step 1100] PRE-ACTION INPUT:
  Image features: shape=(4, 84, 84), range=[-1.000, 1.000]
  Vector features: shape=(53,), range=[-0.991, 0.991]
  Exploration noise: 0.0 (‚úì Deterministic actor output)
  *** LEARNING PHASE *** (t >= start_timesteps)

[DIAGNOSTIC][Step 1100] POST-ACTION OUTPUT:
  Current action: steer=+1.000, throttle/brake=+0.631
  Rolling stats (last 100): steer_mean=+0.000, steer_std=0.000  ‚Üê BUG!
  ‚úì Balanced steering  ‚Üê FALSE POSITIVE!
```

**Vehicle State:**
```
[DEBUG Step 1100] Applied Control:
  throttle: 1.0000
  brake: 0.0000
  steer: 1.0000  ‚Üê MAXIMUM RIGHT!

[DEBUG Step 1100] Current State:
  Speed: 18.69 km/h (5.19 m/s)
  Position: x=395.23, y=-202.72, z=0.27
  Yaw: 4.33 deg
  Act=[steer:+1.000, thr/brk:+0.631] | Rew=+1.99 | LatDev=+1.98m
```

**Critical Observations:**
- üî¥ **steer=+1.000** - MAXIMUM right steering
- üî¥ **lateral_dev=+1.98m** - FAR right of lane center!
- üî¥ **Reward=+1.99** - POSITIVE reward for wrong behavior
- üî¥ **Action buffer shows 0.000** - Statistics broken

---

#### Step 1200 - Continues Hard Right

**Action Output:**
```
[DIAGNOSTIC][Step 1200] POST-ACTION OUTPUT:
  Current action: steer=+1.000, throttle/brake=+0.270
  Rolling stats (last 100): steer_mean=+0.000, steer_std=0.000  ‚Üê STILL BROKEN
```

**Vehicle State:**
```
[DEBUG Step 1200] Applied Control:
  throttle: 0.7040
  brake: 0.0000
  steer: 0.9217

[DEBUG Step 1200] Current State:
  Speed: 0.63 km/h (0.18 m/s)  ‚Üê LOW SPEED (new episode?)
  Act=[steer:+1.000, thr/brk:+0.270] | Rew=-0.30 | LatDev=+0.02m
```

**Critical Observations:**
- üî¥ **steer=+1.000** - STILL maximum right
- ‚ö†Ô∏è **Speed dropped** - Suggests episode reset occurred
- ‚ö†Ô∏è **lateral_dev=+0.02m** - Back to center (confirms new episode)
- ‚ö†Ô∏è **Reward=-0.30** - Negative (may be initial step of new episode)

---

#### Step 1300 - EXTREME Behavior

**Action Output:**
```
[DIAGNOSTIC][Step 1300] POST-ACTION OUTPUT:
  Current action: steer=+1.000, throttle/brake=+1.000  ‚Üê FULL RIGHT + FULL THROTTLE!
  Rolling stats (last 100): steer_mean=+0.000, steer_std=0.000
```

**Vehicle State:**
```
[DEBUG Step 1300] Applied Control:
  throttle: 1.0000
  brake: 0.0000
  steer: 0.8361

[DEBUG Step 1300] Current State:
  Speed: 11.06 km/h (3.07 m/s)
  Act=[steer:+1.000, thr/brk:+1.000] | Rew=+5.53 | LatDev=+0.60m
```

**Reward Breakdown (CRITICAL):**
```
[DIAGNOSTIC][Step 1300] POST-STEP REWARD:
  Total reward: +5.532  ‚Üê HIGHLY POSITIVE FOR WRONG BEHAVIOR!

  Individual Components:
    efficiency:   +0.7369  (13.3% of total)
    lane_keeping: +0.4399  (7.95% of total)  ‚Üê POSITIVE for +0.60m deviation?
    progress:     +4.6548  (84.1% of total)  ‚Üê DOMINATES SIGNAL!
    comfort:      (not shown - likely near zero)
    safety:       (not shown - likely zero if no collision)

  Reward Correlations:
    ‚úì Aligned incentives: progress and lane_keeping have same sign

  Vehicle State:
    vel: 11.7 km/h
    lateral_dev: +0.60m  ‚Üê Deviating from center
    heading_error: (not shown)

  Episode Info:
    Waypoint reached! Bonus: +1.0  ‚Üê May be incorrectly triggered
```

**Critical Observations:**
- üî¥ **steer=+1.000, throttle=+1.000** - MAXIMUM on BOTH axes!
- üî¥ **Reward=+5.53** - VERY POSITIVE for wrong behavior
- üî¥ **Progress=+4.65** - Dominates 84% of total reward
- üî¥ **Lane keeping=+0.44** - POSITIVE despite +0.60m deviation
- üî¥ **Waypoint bonus=+1.0** - May be incorrectly awarded

---

## 2. Reward Signal Analysis

### Component Breakdown (Step 1300 - Most Detailed)

| Component | Value | % of Total | Expected | Actual | Status |
|-----------|-------|-----------|----------|--------|--------|
| **efficiency** | +0.74 | 13.3% | Should penalize low speed (11.7 km/h) | POSITIVE | ‚ö†Ô∏è Questionable |
| **lane_keeping** | +0.44 | 7.95% | Should penalize +0.60m deviation | POSITIVE | ‚ùå WRONG |
| **progress** | +4.65 | 84.1% | Should be small/negative for wrong direction | POSITIVE | ‚ùå DOMINATES |
| **comfort** | ? | ? | Should penalize aggressive steering | Not shown | ‚ùì Unknown |
| **safety** | ? | ? | Should be zero (no collision) | Not shown | ‚ùì Unknown |
| **TOTAL** | +5.53 | 100% | Should be NEGATIVE for hard-right-turn | POSITIVE | ‚ùå BROKEN |

### Progress Reward Domination

**Evidence:**
- Step 1100: Total=+1.99 (progress likely positive)
- Step 1200: Total=-0.30 (new episode, initial negative)
- Step 1300: Total=+5.53 (progress=+4.65 = 84%)

**Problem:**
The progress reward is **overwhelming** all other components. Even if lane_keeping and efficiency were negative (which they aren't!), the +4.65 progress would still dominate.

**Questions:**
1. **Why is progress +4.65 for a hard-right turn?**
   - Is the car actually moving toward a waypoint?
   - Is `route_distance_delta` being calculated correctly?
   - Is the waypoint bonus (+1.0) being awarded incorrectly?

2. **Is the reward order fix being used?**
   - The fix ensures lane_keeping uses current step's `route_distance_delta`
   - But if progress is wrong, lane_keeping will also be wrong
   - Need to verify fix is active in this run

---

## 3. Action Statistics Buffer Bug

### Symptoms

**Every diagnostic point shows:**
```
Rolling stats (last 100): steer_mean=+0.000, steer_std=0.000, throttle_mean=+0.000, throttle_std=0.000
‚úì Balanced steering
```

**Problem:**
- Actual actions: steer=+1.000 (maximum right)
- Buffer reports: steer_mean=+0.000 (zero!)
- This is **FALSE** - the buffer is broken

### Possible Causes

1. **Buffer not being updated:**
   - Actions are selected but not added to the buffer
   - Check `td3_agent.py` for action storage logic

2. **get_action_stats() returns zeros:**
   - Method may have a bug
   - May return zeros if buffer length < 100

3. **Buffer reset each episode:**
   - If buffer is episode-specific, it would reset on episode end
   - But we see 0.000 even at step 1300 (multiple episodes should have passed)

4. **Statistics calculated before action added:**
   - POST-ACTION diagnostic may print stats BEFORE action is added to buffer
   - Need to verify order of operations

### Impact

- ‚ùå Bias detection doesn't work (reports "‚úì Balanced steering" falsely)
- ‚ùå Cannot track action distribution over time
- ‚ùå Diagnostic output misleading

---

## 4. Lateral Deviation Analysis

### Deviation Pattern

| Step | Lateral Deviation | Expected Lane Keeping | Actual Lane Keeping | Status |
|------|------------------|---------------------|-------------------|--------|
| 1000 | +0.09m | Small positive/zero | Not shown | ‚úÖ Normal |
| 1100 | +1.98m | LARGE NEGATIVE | Not shown | ‚ùå Far right! |
| 1200 | +0.02m | Small positive | Not shown | ‚úÖ Centered (new episode) |
| 1300 | +0.60m | Moderate negative | **+0.44 (POSITIVE!)** | ‚ùå WRONG |

### Critical Issue: Lane Keeping Reward is POSITIVE for Deviation

**Step 1300:**
- Lateral deviation: +0.60m (right of center)
- Lane keeping reward: **+0.44** (POSITIVE!)
- Expected: Should be NEGATIVE to penalize deviation

**Possible Causes:**

1. **Reward order bug still present:**
   - If lane_keeping uses STALE `route_distance_delta` from previous step
   - It may reward wrong behavior

2. **Direction-aware scaling broken:**
   - Lane keeping has "direction-aware scaling" based on progress
   - If progress is high, lane_keeping may be scaled up even when deviating
   - This would cause BOTH to be positive (as shown: "‚úì Aligned incentives")

3. **Lateral deviation calculation wrong:**
   - If sign is flipped, +0.60m might be interpreted as -0.60m
   - This would reward right deviations and penalize left

---

## 5. Episode Management

### Episode Timeline

**From log analysis:**
- Episode 1: Steps 1-1000 (truncated at max_episode_steps)
- Episode 2+: Steps 1001+ (multiple episodes in remaining 1000 steps)

**Evidence of episode resets:**
- Step 1100: Speed=18.69 km/h (continuing episode)
- Step 1200: Speed=0.63 km/h (LOW - suggests reset)
- Step 1300: Speed=11.06 km/h (accelerating again)

**Episode numbers from log:**
- Step 1100: Episode 26
- Step 1300: Episode 20 (LOWER than 1100?!)

**Question:** Why is episode number LOWER at step 1300 than 1100?
- May be logging bug
- May be different episode counter (global vs local?)
- Need to clarify episode numbering

---

## 6. Actor Network Analysis

### Gradient Information (Step 1300)

**From log:**
```
[DEBUG Step 1300] Actor gradient norm: 1.9648 BEFORE clip, 1.0000 AFTER clip
```

**Analysis:**
- ‚úÖ Gradient clipping is working (1.9648 ‚Üí 1.0000)
- ‚ö†Ô∏è Gradient norm 1.96 is MODERATE (not extremely high)
- ‚ö†Ô∏è But gradient is being clipped EVERY step (suggests consistent high gradients)

**Implications:**

1. **High gradients suggest:**
   - Q-values may be large
   - Critic is giving strong signals to actor
   - Actor is trying to make large weight updates

2. **Consistent clipping suggests:**
   - Actor is receiving strong, consistent signals to turn right
   - Reward signal is reinforcing right-turn behavior
   - Policy is converging to deterministic maximum right steering

---

## 7. Q-Value Analysis (Not Yet Available)

**Need to add diagnostics:**
- Print Q1, Q2 values from critics
- Print target_Q calculation
- Check if Q-values are exploding (>1000)

**Expected behavior:**
- Q-values should be in similar range to rewards (-100 to +100)
- Q1 and Q2 should be similar (twin critics)
- target_Q should be slightly lower than current Q (TD error)

**Red flags:**
- Q-values > 1000 (exploding)
- Q1 and Q2 very different (critics diverging)
- target_Q > current_Q consistently (wrong TD target)

---

## 8. Root Cause Hypotheses

### Hypothesis 1: Progress Reward Calculation Broken ‚≠ê MOST LIKELY

**Evidence:**
- Progress = +4.65 (84% of total reward)
- This is VERY HIGH for a hard-right turn
- Waypoint bonus = +1.0 may be incorrectly triggered

**Possible causes:**
1. **Waypoint distance calculation wrong:**
   - Agent may be moving TOWARD a waypoint despite turning right
   - Route distance delta may be positive when it should be negative

2. **Waypoint bonus triggered incorrectly:**
   - Bonus should only trigger when ACTUALLY reaching waypoint
   - May be triggering based on distance threshold that's too large

3. **Progress direction not checked:**
   - Progress may only check DISTANCE change, not DIRECTION
   - Moving toward OR away from waypoint would both give positive reward

**Verification needed:**
- Add debug prints to `_calculate_progress_reward()`
- Print: route_distance_delta, waypoint_distance, bonus_triggered
- Check if waypoint positions are correct

---

### Hypothesis 2: Reward Order Fix Not Active ‚≠ê LIKELY

**Evidence:**
- Lane keeping = +0.44 for +0.60m deviation (WRONG!)
- This matches the bug pattern from before the fix
- Suggests lane_keeping may be using STALE route_distance_delta

**Verification needed:**
- Check if reward_functions.py line 205-260 is actually being used
- Add debug print: "Calculating progress BEFORE lane_keeping"
- Verify `self.last_route_distance_delta` is being updated correctly

**Possible causes:**
1. **Old code cached:**
   - Python bytecode (.pyc) may have old version
   - Need to clear `__pycache__` and restart

2. **Fix not applied to correct file:**
   - May have edited wrong file
   - Check if av_td3_system/src/environment/reward_functions.py has the fix

3. **Environment not recreated:**
   - If environment is pickled/cached, it may use old code
   - Need to ensure fresh environment creation

---

### Hypothesis 3: Direction-Aware Scaling Broken

**Evidence:**
- Progress = +4.65 (positive)
- Lane keeping = +0.44 (positive)
- Log shows: "‚úì Aligned incentives" (both same sign)

**From reward_functions.py:**
```python
# Direction-aware scaling: reward lane keeping more when making progress
if progress > 0 and lane_keeping_score > 0:
    lane_keeping_score *= 1.2  # Boost when both positive
```

**Problem:**
- If this scaling is applied when agent is turning RIGHT
- It would BOOST the positive lane_keeping reward
- This creates a POSITIVE FEEDBACK LOOP:
  - Turn right ‚Üí progress positive ‚Üí lane_keeping boosted ‚Üí total reward positive
  - Agent learns: "Turn right = good"

**Verification needed:**
- Check if direction-aware scaling is being applied incorrectly
- May need to REMOVE this scaling or make it more conservative
- Should only boost when ACTUALLY aligned with goal direction

---

### Hypothesis 4: Action Buffer Bug (Lower Priority)

**Evidence:**
- get_action_stats() returns all zeros
- But this is a DIAGNOSTIC issue, not a ROOT CAUSE
- Doesn't affect training, only our ability to debug

**Verification needed:**
- Check td3_agent.py for action buffer implementation
- Fix for better diagnostics in future runs

---

## 9. Immediate Action Plan

### üî¥ CRITICAL - Must Do FIRST

#### Action 1: Verify Reward Order Fix is Active
```bash
# Check if fix is in the file
grep -A 30 "def calculate" av_td3_system/src/environment/reward_functions.py

# Clear Python cache
find av_td3_system -type d -name "__pycache__" -exec rm -rf {} +

# Add debug logging to reward calculation
# Edit reward_functions.py to add:
self.logger.debug(f"[REWARD ORDER] Calculating progress BEFORE lane_keeping")
self.logger.debug(f"[REWARD ORDER] Progress={progress}, delta={self.last_route_distance_delta}")
self.logger.debug(f"[REWARD ORDER] Lane keeping uses current delta: {self.last_route_distance_delta}")
```

#### Action 2: Investigate Progress Reward Calculation
```bash
# Add detailed logging to _calculate_progress_reward()
# Print:
# - route_distance (current waypoint distance)
# - self.last_route_distance (previous)
# - route_distance_delta (change)
# - waypoint_bonus (if triggered)
# - final progress_reward value

# Check waypoint positions
# Print first 5 waypoints in environment reset
```

#### Action 3: Check Direction-Aware Scaling
```python
# In reward_functions.py, find the direction-aware scaling code
# Add debug prints BEFORE and AFTER scaling:
self.logger.debug(f"[SCALING] BEFORE: lane_keeping={lane_keeping_score}, progress={progress}")
# ... scaling code ...
self.logger.debug(f"[SCALING] AFTER: lane_keeping={lane_keeping_score}")

# Consider DISABLING this scaling temporarily to test:
# Comment out the scaling code and re-run training
```

---

### üü° HIGH PRIORITY - Do After Critical Actions

#### Action 4: Add Q-Value Diagnostics
```python
# In td3_agent.py train() method, add:
self.logger.debug(f"[Q-VALUES] Q1={current_Q1.mean():.2f}, Q2={current_Q2.mean():.2f}, target_Q={target_Q.mean():.2f}")
self.logger.debug(f"[Q-VALUES] Critic loss: {critic_loss:.4f}")
```

#### Action 5: Fix Action Statistics Buffer
```python
# In td3_agent.py, find get_action_stats() method
# Add debug print to verify buffer contents:
self.logger.debug(f"[ACTION BUFFER] Length: {len(self.action_buffer)}, Last action: {self.action_buffer[-1] if self.action_buffer else 'EMPTY'}")
```

---

### üü¢ MEDIUM PRIORITY - Do If Problem Persists

#### Action 6: Reduce Progress Reward Weight
```python
# In train_td3.py or config, reduce progress weight:
# Current: progress=3.0
# Try: progress=1.0 or 0.5
# This will prevent progress from dominating other components
```

#### Action 7: Add Lateral Deviation Penalty
```python
# Strengthen lane_keeping penalty for large deviations
# Current formula may be too lenient
# Try exponential penalty: exp(lateral_dev^2) - 1
```

---

## 10. Expected Outcomes

### If Reward Order Fix is NOT Active
**Symptoms:**
- Lane keeping uses stale route_distance_delta
- Rewards wrong turns because it's correlated with OLD progress

**Fix:**
- Clear Python cache
- Restart training
- Verify debug prints show "Calculating progress BEFORE lane_keeping"

**Expected result:**
- Lane keeping should penalize right turns
- Total reward should be NEGATIVE for hard-right behavior
- Agent should learn to turn left/straight instead

---

### If Progress Calculation is Broken
**Symptoms:**
- Progress = +4.65 for wrong direction
- Waypoint bonus triggered incorrectly
- Route distance delta has wrong sign

**Fix:**
- Add debug prints to progress calculation
- Verify waypoint positions are correct
- Check route distance calculation logic

**Expected result:**
- Progress should be NEGATIVE or small for right turns
- Total reward should be NEGATIVE
- Agent should learn to follow waypoints correctly

---

### If Direction-Aware Scaling is the Culprit
**Symptoms:**
- Both progress and lane_keeping positive
- Scaling BOOSTS lane_keeping when it shouldn't
- Creates positive feedback loop

**Fix:**
- Disable direction-aware scaling temporarily
- Re-run training

**Expected result:**
- Lane keeping should be NEGATIVE for deviations (not boosted)
- Total reward should be NEGATIVE for wrong turns
- Agent should learn correct behavior

---

## 11. Success Criteria

### Training Should Show:

1. **Action Distribution:**
   - ‚úÖ Steering values distributed around 0.0 (not constant +1.0)
   - ‚úÖ Occasional left/right turns as needed
   - ‚úÖ Throttle responsive to speed target

2. **Reward Signal:**
   - ‚úÖ NEGATIVE rewards for hard-right turns
   - ‚úÖ POSITIVE rewards for lane following
   - ‚úÖ No single component dominating (all within 40% of total)

3. **Vehicle Behavior:**
   - ‚úÖ Lateral deviation < 0.5m (stays in lane)
   - ‚úÖ Heading error < 10 degrees (aligned with road)
   - ‚úÖ Speed tracking target (20-30 km/h)

4. **Learning Progress:**
   - ‚úÖ Episode rewards increasing over time
   - ‚úÖ Q-values stable (not exploding)
   - ‚úÖ Actor gradients moderate (0.1-1.0 after clip)

---

## 12. Next Steps

### Immediate (Next 1 Hour):
1. ‚úÖ Read reward_functions.py to verify reward order fix
2. ‚è≥ Clear Python cache and restart environment
3. ‚è≥ Add debug prints for reward calculation order
4. ‚è≥ Add debug prints for progress reward calculation
5. ‚è≥ Re-run training with enhanced logging

### Short-term (Today):
1. ‚è≥ Analyze new debug logs to verify fixes are active
2. ‚è≥ Identify which hypothesis is correct (progress vs order vs scaling)
3. ‚è≥ Apply targeted fix based on root cause
4. ‚è≥ Run validation training (5K steps)

### Medium-term (Tomorrow):
1. ‚è≥ Fix action statistics buffer
2. ‚è≥ Add Q-value diagnostics
3. ‚è≥ Run full training (50K steps)
4. ‚è≥ Document all findings and fixes

---

## 13. References

**Related Documents:**
- `CRITICAL_BUG_FIX_REWARD_ORDER.md` - Original reward order dependency bug
- `DEBUG_INSTRUMENTATION_ANALYSIS.md` - Debug print design rationale
- `BUG_FIX_TUPLE_FORMAT_ERROR.md` - Previous bug fixes

**Code Files:**
- `av_td3_system/src/environment/reward_functions.py` - Reward calculation (line 205-260)
- `av_td3_system/src/agents/td3_agent.py` - TD3 agent, action buffer
- `av_td3_system/scripts/train_td3.py` - Training loop, diagnostic prints

**Log Files:**
- `debug-action.log` - 2K training run (181,293 lines)
- Key sections: Steps 1000, 1100, 1200, 1300

---

## Conclusion

The hard-right-turn problem **PERSISTS** despite the reward order fix. The systematic log analysis reveals **THREE CRITICAL ISSUES**:

1. **Progress reward dominates** (+4.65 = 84% of total)
2. **Lane keeping reward is POSITIVE** despite lateral deviation (+0.44 for +0.60m)
3. **Action statistics buffer is broken** (returns zeros)

The **MOST LIKELY root cause** is that either:
- The reward order fix is NOT active (Python cache issue), OR
- The progress reward calculation is fundamentally broken (wrong distance calculation)

**Immediate next step:** Verify reward order fix is active and add detailed logging to progress reward calculation. Then re-run training to identify the exact root cause.

---

**Generated:** December 1, 2025  
**Analyst:** GitHub Copilot  
**Status:** üî¥ INVESTIGATION IN PROGRESS
