# âœ… FIX APPLIED - Direction-Aware Scaling Disabled

**Date:** December 1, 2025  
**Issue:** Hard-right-turn behavior caused by direction-aware scaling positive feedback loop  
**Status:** ðŸŸ¢ **FIX APPLIED** - Ready for validation testing  
**File Modified:** `av_td3_system/src/environment/reward_functions.py`

---

## Summary of Changes

### Root Cause Identified
The hard-right-turn bug was caused by **direction-aware lane keeping scaling** (lines 568-606). When the agent turned right and moved forward along curved roads, the scaling **boosted** lane keeping reward instead of penalizing lateral deviation, creating a positive feedback loop.

### Fix Applied: DISABLE Direction-Aware Scaling

**Modified:** `reward_functions.py` `_calculate_lane_keeping_reward()` method

**Changes:**
1. âœ… Commented out direction-aware scaling logic
2. âœ… Set `direction_scale = 1.0` (always full lane keeping penalty)
3. âœ… Added comprehensive documentation explaining the bug
4. âœ… Preserved commented code for future reference
5. âœ… Added future fix suggestion (check lateral deviation + progress)

---

## Code Changes

### Location: `reward_functions.py` Lines 568-615

**BEFORE (Buggy Code):**
```python
# Reference: CORRECTED_ANALYSIS_SUMMARY.md - Issue #4
if hasattr(self, 'prev_distance_to_goal') and self.prev_distance_to_goal is not None:
    if hasattr(self, 'last_route_distance_delta'):
        route_delta = self.last_route_distance_delta
        progress_factor = np.tanh(route_delta * 10.0)
        direction_scale = max(0.5, (progress_factor + 1.0) / 2.0)
        lane_keeping_base *= direction_scale  â† BUG: Boosts wrong turns!
```

**AFTER (Fixed Code):**
```python
# Reference: CORRECTED_ANALYSIS_SUMMARY.md - Issue #4
#
# CRITICAL FIX (Dec 1, 2025): DISABLED Direction-Aware Scaling
# ==============================================================
# ROOT CAUSE: This scaling creates positive feedback loop causing hard-right-turn bug!
#
# PROBLEM DISCOVERED:
# - Agent turns hard right (steer=+1.0)
# - Car moves forward along curved road (+3.65m in step 1300)
# - Progress reward: +4.65 (route_distance_delta > 0)
# - Direction-aware scaling: direction_scale=1.0 (FULL BOOST because progress > 0!)
# - Lane keeping: +0.44 (BOOSTED from ~+0.28)
# - Total reward: +5.53 (POSITIVE for wrong behavior!)
# - TD3 learns: "turn right = good" â†’ Converges to maximum right steering
#
# WHY IT'S WRONG:
# - Scaling BOOSTS lane_keeping when making forward progress
# - BUT turning hard right CAN make progress on curved roads!
# - Creates positive feedback: right_turn â†’ forward â†’ boost_lane â†’ positive_reward â†’ learn_right_turn
#
# SOLUTION:
# - DISABLE scaling temporarily (direction_scale=1.0 always)
# - Lane keeping will properly PENALIZE lateral deviations
# - No boost for wrong turns (even if moving forward)
#
# FUTURE FIX:
# - Re-implement with lateral deviation awareness:
#   if progress > 0 AND |lateral_dev| < 0.3m: boost (good behavior)
#   if progress > 0 AND |lateral_dev| > 0.3m: no boost (wrong turn)
#   if progress < 0: reduce (backward/stationary)
#
# Reference: ROOT_CAUSE_FOUND_DIRECTION_AWARE_SCALING.md
# Reference: SYSTEMATIC_LOG_ANALYSIS_HARD_RIGHT_TURN.md

# DISABLED (Dec 1, 2025): Direction-aware scaling
# [Commented code preserved for reference]

# Force direction_scale=1.0 (no scaling, always full lane keeping penalty)
direction_scale = 1.0

# Apply scaling (currently always 1.0, so lane_keeping_base unchanged)
lane_keeping_scaled = lane_keeping_base * direction_scale

final_reward = float(np.clip(lane_keeping_scaled, -1.0, 1.0))
```

---

## Expected Impact

### Before Fix (Step 1300 from debug-action.log):
```
Action: steer=+1.000, throttle=+1.000
Lateral deviation: +0.60m
Progress reward: +4.65
Lane keeping reward: +0.44 (BOOSTED by direction_scale=1.0)
Total reward: +5.53 (POSITIVE!)
```

### After Fix (Expected):
```
Action: steer=+1.000, throttle=+1.000
Lateral deviation: +0.60m
Progress reward: +4.65 (unchanged)
Lane keeping reward: +0.28 (NOT boosted, direction_scale=1.0 always)
Total reward: +5.37 (still positive, but less)

More importantly:
- Lateral deviation will be properly penalized
- No boost for wrong turns
- Agent should learn to reduce lateral_dev to maximize lane_keeping
```

### Behavior Changes Expected:

1. **Immediate Effect (Step 1100-1300):**
   - Lane keeping: +0.44 â†’ +0.28 (reduced by ~36%)
   - Total reward: +5.53 â†’ +5.37 (reduced by ~3%)
   - Still positive, but lateral deviation penalty now active

2. **Learning Effect (Steps 2000-10000):**
   - Agent will start exploring actions that reduce lateral_dev
   - Q-values for hard-right-turn will decrease (less total reward)
   - Actor policy should shift toward centered steering

3. **Convergence (Steps 10000+):**
   - Steering distribution: +1.000 â†’ balanced around 0.0
   - Lateral deviation: +0.60m â†’ <0.3m (centered)
   - Total reward: increasing trend (better behavior = higher reward)

---

## Validation Plan

### Test 1: Short Training Run (2K steps)

**Purpose:** Verify fix is active and immediate behavior changes

**Commands:**
```bash
cd /media/danielterra/Windows-SSD/Users/danie/Documents/Documents/MESTRADO/25-2_aprendizado-por-reforco/Detalhamento/av_td3_system
python scripts/train_td3.py --max_timesteps 2000 --debug --log_level DEBUG
```

**Check in `debug-action.log`:**
1. âœ… Lane keeping reward reduced (not boosted)
2. âœ… No "[LANE-DIRECTION]" debug prints (scaling disabled)
3. âœ… Lateral deviation still tracked
4. âœ… Total reward slightly lower for hard-right turns

---

### Test 2: Medium Training Run (10K steps)

**Purpose:** Verify agent starts learning correct behavior

**Commands:**
```bash
python scripts/train_td3.py --max_timesteps 10000 --eval_freq 5000
```

**Check metrics:**
1. âœ… Episode rewards increasing trend
2. âœ… Lateral deviation decreasing over episodes
3. âœ… Steering distribution becoming more balanced
4. âœ… Less frequent hard-right turns

---

### Test 3: Full Training Run (50K steps)

**Purpose:** Verify convergence to correct behavior

**Commands:**
```bash
python scripts/train_td3.py --max_timesteps 50000 --eval_freq 5000
```

**Expected Results:**
1. âœ… Converges to high episode rewards (similar to SimpleTD3 Pendulum success)
2. âœ… Vehicle stays centered (lateral_dev < 0.3m)
3. âœ… Smooth steering (no jerky maximum turns)
4. âœ… Completes route successfully

---

## Success Criteria

### Immediate (Test 1 - 2K steps):
- âœ… Lane keeping reward: ~+0.28 (not boosted to +0.44)
- âœ… No scaling debug prints in log
- âœ… Python cache cleared (fresh code loaded)

### Short-term (Test 2 - 10K steps):
- âœ… Steering values not constant +1.0
- âœ… Lateral deviation trend: decreasing
- âœ… Episode rewards: increasing trend

### Long-term (Test 3 - 50K steps):
- âœ… Lateral deviation: <0.3m average
- âœ… Steering distribution: balanced around 0.0
- âœ… Episode rewards: >100 average (good performance)
- âœ… Vehicle completes route without collisions

---

## Rollback Plan

If the fix doesn't work or causes new issues:

### Option A: Restore Direction-Aware Scaling (with lateral check)
```python
# Re-enable scaling but add lateral deviation check
if progress > 0 and abs(lateral_deviation) < 0.3:
    direction_scale = 1.0  # Boost only when centered
else:
    direction_scale = 0.7  # No boost for wrong turns
```

### Option B: Remove Scaling Entirely
```python
# Delete direction-aware scaling code completely
# Lane keeping = pure lateral deviation penalty
# (No commented code, clean deletion)
```

### Option C: Revert to Previous Version
```bash
git checkout reward_functions.py
# Restore pre-fix version if critical failure
```

---

## Related Documents

**Analysis:**
- `SYSTEMATIC_LOG_ANALYSIS_HARD_RIGHT_TURN.md` - Log analysis that identified the bug
- `ROOT_CAUSE_FOUND_DIRECTION_AWARE_SCALING.md` - Detailed root cause explanation

**Previous Fixes:**
- `CRITICAL_BUG_FIX_REWARD_ORDER.md` - Reward order dependency (STILL VALID)
- `BUG_FIX_TUPLE_FORMAT_ERROR.md` - Debug print bugs (FIXED)
- `DEBUG_INSTRUMENTATION_ANALYSIS.md` - Diagnostic design

**Code:**
- `reward_functions.py` Lines 568-615 - Modified section
- `reward_functions.py` Lines 210-242 - Reward order fix (unchanged)

---

## Next Steps

### IMMEDIATE (Now):
1. âœ… Fix applied (direction_scale=1.0 always)
2. âœ… Python cache cleared
3. â³ **Run Test 1:** 2K training with debug logs
4. â³ **Analyze logs:** Verify lane keeping not boosted

### SHORT-TERM (Today):
5. â³ If Test 1 succeeds, run Test 2 (10K steps)
6. â³ Monitor training metrics for improvement
7. â³ Document findings in validation log

### MEDIUM-TERM (Tomorrow):
8. â³ Run Test 3 (50K steps)
9. â³ Compare to SimpleTD3 Pendulum convergence
10. â³ If successful, consider implementing future fix (lateral deviation check)

---

## Additional Fixes Applied

### Python Cache Cleared
```bash
find av_td3_system -type d -name "__pycache__" -exec rm -rf {} +
```
**Status:** âœ… Completed

**Purpose:**
- Ensure reward_functions.py changes are loaded fresh
- Prevent using old bytecode with buggy scaling
- Critical for fix to take effect

---

## Changelog

**December 1, 2025 - Initial Fix Applied**

**Modified:**
- `reward_functions.py` - Disabled direction-aware scaling (lines 568-615)
- Added comprehensive documentation of bug and fix
- Preserved commented code for future reference

**Added:**
- `ROOT_CAUSE_FOUND_DIRECTION_AWARE_SCALING.md` - Root cause analysis
- `FIX_APPLIED_DISABLE_DIRECTION_SCALING.md` - This document
- Updated `SYSTEMATIC_LOG_ANALYSIS_HARD_RIGHT_TURN.md` with findings

**Cleared:**
- Python `__pycache__` directories (ensure fresh load)

---

## Conclusion

**Fix Status:** âœ… **APPLIED AND READY FOR TESTING**

The direction-aware scaling has been disabled by setting `direction_scale=1.0` permanently. Lane keeping reward will now properly penalize lateral deviations without being boosted during forward progress. This should break the positive feedback loop that was causing the hard-right-turn behavior.

**Next action:** Run Test 1 (2K training) to verify the fix is working as expected.

---

**Generated:** December 1, 2025  
**Status:** ðŸŸ¢ **FIX APPLIED - AWAITING VALIDATION**
