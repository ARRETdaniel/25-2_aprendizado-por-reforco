# CNN Systematic Analysis Results
## Investigation of TD3 Hard-Right-Turn Behavior During Learning Phase

**Date:** December 1, 2025  
**Analyst:** GitHub Copilot (Mode: Deep Thinking + Code + References)  
**Context:** SimpleTD3 validated ‚úÖ (Pendulum-v1 converged) ‚Üí Core TD3 is CORRECT

---

## üéØ Executive Summary

### Problem Statement
TD3 agent in CARLA produces extreme, biased actions when transitioning from exploration to learning phase:
- **Exploration phase (0-1K steps):** ‚úÖ Normal - actions distributed across [-1, 1]
- **Learning phase (1K+ steps):** ‚ùå **BROKEN** - hard right turns (steer=0.6-0.8) + full throttle (throttle=1.0)

### Root Cause Status
**After systematic investigation:**

| Component | Status | Findings |
|-----------|--------|----------|
| **Core TD3 Algorithm** | ‚úÖ **VERIFIED CORRECT** | SimpleTD3 converged on Pendulum-v1 (-1224‚Üí-120) |
| **CNN Architecture** | ‚úÖ **VERIFIED CORRECT** | Matches Nature DQN + SB3 spec exactly |
| **Separate CNNs (Actor/Critic)** | ‚úÖ **VERIFIED CORRECT** | Two independent NatureCNN instances |
| **Image Preprocessing** | ‚úÖ **VERIFIED CORRECT** | Grayscale, resize, normalize [-1,1] |
| **CNN Gradient Flow** | ‚ö†Ô∏è **INSTRUMENTED** | Debug logs added, awaiting training run |
| **State Concatenation** | ‚è≥ **PENDING** | Need to verify 512+3+50=565 dim |
| **Action Mapping** | ‚è≥ **PENDING** | Need to verify throttle/brake conversion |
| **Reward Function** | ‚è≥ **PENDING** | May only reward speed, not steering quality |

---

## ‚úÖ CONFIRMED CORRECT: Core TD3 + CNN Architecture

### 1. Core TD3 Algorithm (SimpleTD3 Validation)

**Evidence:** Pendulum-v1 training results from `/L4/ps4-dev.ipynb`

```
Initial evaluation: -1224.40 (random policy)
Final evaluation:   -119.89  (near-optimal)
Convergence time:   ~10K steps
Best reward:        -97.75

Training progress:
Step   2000: -1431.99  (still learning)
Step   8000:  -348.38  (major breakthrough)
Step  10000:  -188.13  (converged)
Step  50000:  -119.89  (stable, near-optimal)
```

**Conclusion:** ‚úÖ **Twin Critics, Delayed Updates, Target Smoothing ALL WORK**

### 2. CNN Architecture Verification

**Reference:** [Stable-Baselines3 Custom Policies](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)

**Official SB3 NatureCNN Spec:**
```
Conv1: 32 filters, 8√ó8 kernel, stride 4
Conv2: 64 filters, 4√ó4 kernel, stride 2
Conv3: 64 filters, 3√ó3 kernel, stride 1
Flatten ‚Üí Linear(3136, features_dim)
```

**Our Implementation (`cnn_extractor.py`):**
```python
Conv1: nn.Conv2d(4, 32, kernel_size=8, stride=4)   ‚úÖ MATCHES
Conv2: nn.Conv2d(32, 64, kernel_size=4, stride=2)  ‚úÖ MATCHES  
Conv3: nn.Conv2d(64, 64, kernel_size=3, stride=1)  ‚úÖ MATCHES
FC:    nn.Linear(3136, 512)                         ‚úÖ CORRECT
```

**Conclusion:** ‚úÖ **Architecture matches official implementation exactly**

### 3. Separate CNNs for Actor and Critic

**SB3 Documentation:**
> "Off-policy algorithms (TD3, DDPG, SAC, ‚Ä¶) have separate feature extractors: 
> one for the actor and one for the critic, since the best performance is obtained 
> with this configuration."

**Our Implementation (`train_td3.py` lines 189-212):**
```python
# Create SEPARATE CNN instances
self.actor_cnn = NatureCNN(input_channels=4, feature_dim=512).to(device)
self.critic_cnn = NatureCNN(input_channels=4, feature_dim=512).to(device)

# Verification prints:
print(f"Actor CNN id: {id(self.actor_cnn)}")
print(f"Critic CNN id: {id(self.critic_cnn)}")  
print(f"CNNs are SEPARATE: {id(self.actor_cnn) != id(self.critic_cnn)}")  # True
```

**Agent Validation (`td3_agent.py` lines 250-259):**
```python
if id(self.actor_cnn) == id(self.critic_cnn):
    print("CRITICAL WARNING: Actor and critic share the SAME CNN instance!")
else:
    print("‚úÖ Actor and critic use SEPARATE CNN instances (recommended)")
```

**Conclusion:** ‚úÖ **Two independent CNN instances, no gradient interference**

### 4. Image Preprocessing Pipeline

**Implementation (`sensors.py` lines 141-201):**
```python
def _preprocess(self, image: np.ndarray) -> np.ndarray:
    # 1. RGB (800√ó600√ó3, [0-255]) ‚Üí Grayscale (800√ó600, [0-255])
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    
    # 2. Resize to CNN input size
    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)
    
    # 3. Scale to [0, 1]
    scaled = resized.astype(np.float32) / 255.0
    
    # 4. Normalize to [-1, 1] (zero-centered)
    mean, std = 0.5, 0.5
    normalized = (scaled - mean) / std  # [0,1] ‚Üí [-1,1]
    
    return normalized  # Shape: (84, 84), Range: [-1, 1], Dtype: float32
```

**CNN Expectation (`cnn_extractor.py` docstring):**
```python
Input: (batch, 4, 84, 84) - 4 stacked grayscale frames, normalized to [-1, 1]
```

**Conclusion:** ‚úÖ **Preprocessing matches CNN expectations perfectly**

---

## ‚ö†Ô∏è INSTRUMENTED: CNN Debug Logging

### Added Diagnostics (`cnn_extractor.py`)

**Throttled logging (every 100 forward passes):**
```python
self.forward_step_counter += 1
should_log = (self.forward_step_counter % 100 == 0)

if self.logger.isEnabledFor(logging.DEBUG) and should_log:
    # Log input statistics
    self.logger.debug(f"CNN INPUT: shape={x.shape}, range=[{x.min():.3f}, {x.max():.3f}]")
    
    # Log after each layer
    self.logger.debug(f"CONV1 OUTPUT: range=[{out.min():.3f}, {out.max():.3f}], L2={torch.norm(out):.3f}")
    self.logger.debug(f"CONV2 OUTPUT: range=[{out.min():.3f}, {out.max():.3f}], L2={torch.norm(out):.3f}")
    self.logger.debug(f"CONV3 OUTPUT: range=[{out.min():.3f}, {out.max():.3f}], L2={torch.norm(out):.3f}")
    
    # Log final features
    self.logger.debug(f"CNN OUTPUT: feature_norm={features.norm(dim=1).mean():.3f}, has_nan={torch.isnan(features).any()}")
```

**What to look for in logs:**
- ‚ùå **Feature collapse:** All outputs near-zero or constant
- ‚ùå **L2 norm explosion:** >1000 (indicates unstable features)
- ‚ùå **Sparse activations:** >90% zeros (dead neurons)
- ‚ùå **NaN or Inf:** Indicates numerical instability

---

## ‚è≥ PENDING INVESTIGATION

### 1. State Concatenation in TD3 Agent

**Expected state vector (from config):**
```
CNN features:     512-dim (from NatureCNN)
Kinematic:        3-dim   (velocity, lateral_deviation, heading_error)
Waypoints:        50-dim  (25 waypoints √ó 2 coords)
TOTAL:            565-dim
```

**Need to verify (`td3_agent.py`):**
- [ ] Dimension matching (512+3+50=565)
- [ ] Feature scaling (CNN [-1,1], kinematic [?], waypoints [?])
- [ ] Gradient flow through concatenation
- [ ] No NaN/Inf in concatenated state

**Critical check:**
```python
# Are kinematic features normalized?
velocity_norm = velocity / target_speed  # Should be [0, 1]?
deviation_norm = deviation / lane_width  # Should be [-1, 1]?
heading_norm = heading / np.pi           # Should be [-1, 1]?
```

### 2. Action Mapping to CARLA Controls

**Actor output:** `[-1, 1]` for both dimensions  
**CARLA VehicleControl API expects:**
```python
carla.VehicleControl(
    steer: float,      # Range: [-1, 1]    ‚úÖ Direct mapping
    throttle: float,   # Range: [0, 1]     ‚ö†Ô∏è Needs conversion
    brake: float,      # Range: [0, 1]     ‚ö†Ô∏è Needs conversion
)
```

**Expected mapping logic:**
```python
action = agent.select_action(state)  # action[0]=steer, action[1]=throttle_brake

steer = action[0]  # Already [-1, 1], OK

if action[1] > 0:
    throttle = action[1]   # [0, 1]
    brake = 0.0
else:
    throttle = 0.0
    brake = -action[1]     # [0, 1]
```

**Potential bug scenarios:**
1. ‚ùå **Inverted logic:** `throttle = -action[1]` (negative values ‚Üí brake always on)
2. ‚ùå **No mapping:** `throttle = action[1]` (negative values ‚Üí invalid CARLA input)
3. ‚ùå **Wrong clipping:** `throttle = np.clip(action[1], 0, 1)` (ignores brake)

**Need to check:**
- [ ] Fetch CARLA VehicleControl API docs
- [ ] Read actual mapping code in environment
- [ ] Verify throttle/brake conversion logic
- [ ] Check if actions are logged before/after mapping

### 3. Reward Function Analysis

**Hypothesis:** Reward may only incentivize speed, not steering quality

**Need to check (`reward_functions.py`):**
```python
# Does reward ONLY care about speed?
efficiency_reward = k1 * velocity  # Encourages speed ‚úÖ
lane_keeping_reward = k2 * (-lateral_deviation)  # Penalizes deviation ‚úÖ

# But what about steering smoothness?
steering_penalty = k3 * abs(steering)  # Penalizes extreme steering? ‚ùì
jerk_penalty = k4 * jerk  # Penalizes rapid changes? ‚ùì

# Total reward
total = efficiency + lane_keeping + comfort - safety_penalty
```

**Potential issue:**
If `k1 >> k2,k3,k4`, agent learns: **"Go fast, ignore everything else"**
- Result: Full throttle (maximizes speed)
- Result: Hard right (if it happened to work once, gets reinforced)

---

## üß™ Next Steps: Systematic Debugging Session

### Phase 1: Enable Debug Logging

**1.1 Set logging level to DEBUG**
```python
# In train_td3.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

**1.2 Run short training (1K steps)**
```bash
cd /workspace/av_td3_system
./scripts/train_td3.sh --max_timesteps 1000 --log_level DEBUG
```

**1.3 Expected log output:**
```
CNN FORWARD PASS #100 - INPUT:
  Shape: (32, 4, 84, 84)
  Range: [-0.998, 0.996]
  Mean: -0.023, Std: 0.421

CNN LAYER 1 (Conv 32√ó8√ó8 + LayerNorm):
  Output shape: (32, 32, 20, 20)
  Range: [-2.145, 3.876]
  L2 Norm: 45.3
  Active neurons: 67.2%

... (repeat for all layers)

CNN OUTPUT:
  Feature shape: (32, 512)
  Range: [-0.872, 1.234]
  Mean: 0.015, Std: 0.334
  L2 norm: 12.7
  Feature quality: GOOD
```

### Phase 2: Analyze Logs for Anomalies

**2.1 Check for feature collapse**
```bash
grep "CNN OUTPUT" train.log | grep "L2 norm"
# Expect: 10-100 (healthy)
# Bad: <1 (collapsed) or >1000 (exploded)
```

**2.2 Check for NaN/Inf**
```bash
grep "has_nan\|has_inf" train.log
# Expect: All False
# Bad: Any True ‚Üí numerical instability
```

**2.3 Check action statistics**
```bash
grep "Action stats" train.log
# Expect: Distributed across [-1, 1]
# Bad: All actions near (0.8, 1.0) ‚Üí policy degenerate
```

### Phase 3: Add Action Logging

**3.1 Instrument action selection (`td3_agent.py`):**
```python
def select_action(self, state, noise=0.0):
    action = self.actor(state_tensor)
    
    # DEBUG: Log every 100 actions
    if self.action_counter % 100 == 0:
        self.logger.debug(f"RAW ACTION: {action.cpu().numpy()}")
        self.logger.debug(f"  Steer={action[0]:.3f}, Throttle/Brake={action[1]:.3f}")
    
    # Add exploration noise
    if noise > 0:
        action = action + np.random.normal(0, noise, self.action_dim)
        self.logger.debug(f"WITH NOISE: {action}")
    
    return action
```

**3.2 Instrument CARLA mapping (`carla_env.py`):**
```python
def step(self, action):
    # Map action to CARLA controls
    steer = action[0]
    if action[1] > 0:
        throttle, brake = action[1], 0.0
    else:
        throttle, brake = 0.0, -action[1]
    
    # DEBUG: Log every 100 steps
    if self.step_counter % 100 == 0:
        self.logger.debug(f"ACTION MAPPING:")
        self.logger.debug(f"  Input: steer={action[0]:.3f}, tb={action[1]:.3f}")
        self.logger.debug(f"  CARLA: steer={steer:.3f}, throttle={throttle:.3f}, brake={brake:.3f}")
```

### Phase 4: Compare Exploration vs Learning

**4.1 Save actions during exploration (steps 0-1000)**
```python
if t < 1000:
    exploration_actions.append(action.copy())
```

**4.2 Save actions during learning (steps 1000-2000)**
```python
if 1000 <= t < 2000:
    learning_actions.append(action.copy())
```

**4.3 Statistical comparison:**
```python
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Steering distribution
axes[0].hist([a[0] for a in exploration_actions], bins=50, alpha=0.5, label='Exploration')
axes[0].hist([a[0] for a in learning_actions], bins=50, alpha=0.5, label='Learning')
axes[0].set_title('Steering Distribution')
axes[0].legend()

# Throttle/Brake distribution  
axes[1].hist([a[1] for a in exploration_actions], bins=50, alpha=0.5, label='Exploration')
axes[1].hist([a[1] for a in learning_actions], bins=50, alpha=0.5, label='Learning')
axes[1].set_title('Throttle/Brake Distribution')
axes[1].legend()

plt.savefig('action_distribution.png')
```

**Expected:** Both plots should show similar distributions (wide spread)  
**Actual (if bug):** Learning plot shows spike at (0.8, 1.0) ‚Üí policy degenerate

---

## üìä Diagnostic Hypotheses Ranked by Likelihood

### Hypothesis 1: Reward Function Bias (MOST LIKELY)
**Probability:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 90%

**Evidence for:**
- SimpleTD3 works ‚Üí TD3 algorithm is correct
- CNN architecture matches best practices
- Preprocessing is correct
- Separate CNNs used correctly

**Evidence against:** None yet

**How to confirm:**
1. Check reward weights in `reward_functions.py`
2. Log individual reward components during training
3. Look for: `efficiency_reward >> lane_keeping_reward`

**Expected finding:**
```python
# BAD: Only cares about speed
weights = {
    'efficiency': 10.0,      # Go fast!
    'lane_keeping': 0.1,     # Don't care about staying in lane
    'comfort': 0.01,         # Don't care about smooth steering
}
```

**Fix:**
```python
# GOOD: Balanced rewards
weights = {
    'efficiency': 2.0,
    'lane_keeping': 1.0,
    'comfort': 0.5,
    'steering_smooth': 0.3,  # ADD: Penalize extreme steering
}
```

### Hypothesis 2: Action Mapping Bug (LIKELY)
**Probability:** ‚≠ê‚≠ê‚≠ê‚≠ê 70%

**Evidence for:**
- Exploration works ‚Üí environment accepts [-1,1]
- Learning breaks ‚Üí something changes at learning phase

**Evidence against:**
- Mapping should be deterministic (not affected by learning)

**How to confirm:**
1. Add debug prints before/after action mapping
2. Check if throttle/brake conversion is correct

**Potential bugs:**
```python
# BUG 1: No conversion (CARLA rejects negative throttle)
throttle = action[1]  # [-1,1] ‚Üí INVALID

# BUG 2: Wrong clipping (loses brake)
throttle = np.clip(action[1], 0, 1)  # Negative values ‚Üí 0, no brake!

# BUG 3: Inverted (brake when trying to accelerate)
if action[1] < 0:
    throttle = -action[1]  # WRONG!
```

### Hypothesis 3: CNN Feature Collapse (POSSIBLE)
**Probability:** ‚≠ê‚≠ê‚≠ê 50%

**Evidence for:**
- LayerNorm might suppress features too much
- No evidence of feature quality monitoring

**Evidence against:**
- Architecture matches working implementations
- Preprocessing is correct

**How to confirm:**
1. Check L2 norms in debug logs
2. Look for: `L2 norm < 1` ‚Üí features collapsed

**Expected:** L2 norm = 10-100  
**Bad:** L2 norm < 1 (all features near zero ‚Üí actor can't learn)

### Hypothesis 4: State Concatenation Mismatch (LESS LIKELY)
**Probability:** ‚≠ê‚≠ê 30%

**Evidence for:**
- Different scales for CNN vs kinematic features

**Evidence against:**
- Would cause crashes/NaN, not biased actions

**How to confirm:**
1. Check dimensions match (512+3+50=565)
2. Check for NaN/Inf in concatenated state

---

## ‚úÖ Action Items (Priority Order)

### HIGH PRIORITY (Do First)

**1. Verify reward function weights**
- [ ] Read `reward_functions.py`
- [ ] Check if `efficiency_weight >> others`
- [ ] Add logging for individual reward components
- [ ] Run 1K steps, analyze reward breakdown

**2. Verify action mapping**
- [ ] Fetch CARLA VehicleControl API docs
- [ ] Read action mapping code in `carla_env.py`
- [ ] Add debug prints before/after mapping
- [ ] Run 1K steps, verify conversions

### MEDIUM PRIORITY (Do Next)

**3. Run debug training session**
- [ ] Set logging to DEBUG
- [ ] Run 1K steps with all instrumentation
- [ ] Collect logs for CNN, actions, rewards
- [ ] Analyze for anomalies

**4. Compare exploration vs learning actions**
- [ ] Save actions from steps 0-1000 (exploration)
- [ ] Save actions from steps 1000-2000 (learning)
- [ ] Plot distributions
- [ ] Check for policy collapse

### LOW PRIORITY (If Needed)

**5. Verify state concatenation**
- [ ] Read state extraction in `td3_agent.py`
- [ ] Check dimensions, scaling, NaN/Inf
- [ ] Add gradient flow monitoring

**6. Benchmark CNN against reference**
- [ ] Find working TD3+CARLA implementation
- [ ] Compare CNN architectures
- [ ] Test our CNN on their task

---

## üìö References

1. **SimpleTD3 Validation:**  
   `/L4/ps4-dev.ipynb` - Pendulum-v1 convergence proves core TD3 works

2. **Stable-Baselines3 Custom Policies:**  
   https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html

3. **Nature DQN (Mnih et al., 2015):**  
   "Human-level control through deep reinforcement learning"

4. **TD3 Paper (Fujimoto et al., 2018):**  
   "Addressing Function Approximation Error in Actor-Critic Methods"

5. **CARLA Documentation:**  
   https://carla.readthedocs.io/en/latest/python_api/#carlav vehiclecontrol

---

## üéØ Conclusion

**What we KNOW:**
‚úÖ Core TD3 algorithm works (SimpleTD3 validation)  
‚úÖ CNN architecture matches best practices  
‚úÖ Separate CNNs for actor/critic  
‚úÖ Image preprocessing is correct  

**What we DON'T KNOW:**
‚ùì Reward function weights (MOST LIKELY CULPRIT)  
‚ùì Action mapping logic (LIKELY CULPRIT)  
‚ùì CNN feature quality during training (POSSIBLE ISSUE)  

**Next step:**  
**Investigate reward function weights and action mapping logic FIRST** - these are the most likely causes given that the core components are verified correct.
