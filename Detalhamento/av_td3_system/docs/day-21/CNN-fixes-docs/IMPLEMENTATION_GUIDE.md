# üöÄ CNN NORMALIZATION - IMPLEMENTATION GUIDE

**Date**: 2025-11-21  
**Task**: Add LayerNorm to CNN to fix feature explosion  
**Estimated Time**: 30 minutes implementation + 1-2 hours validation  
**Critical**: Must complete before 1M production run

---

## Quick Reference

**Problem**: CNN L2 norm = 7.36 √ó 10¬π¬≤ (10¬π‚Å∞√ó too high)  
**Solution**: Add LayerNorm after each convolutional and FC layer  
**Expected**: CNN L2 norm < 100 (10¬π‚Å∞√ó reduction)

---

## STEP 1: Modify CNN Architecture (30 minutes)

### File: `src/networks/cnn_extractor.py`

**Current Code** (lines ~90-140):
```python
class NatureCNN(nn.Module):
    def __init__(
        self,
        input_channels: int = 4,
        num_frames: int = 4,
        feature_dim: int = 512,
    ):
        super(NatureCNN, self).__init__()

        self.input_channels = input_channels
        self.num_frames = num_frames
        self.feature_dim = feature_dim

        # Convolutional layers
        self.conv1 = nn.Conv2d(
            in_channels=input_channels,
            out_channels=32,
            kernel_size=8,
            stride=4,
            padding=0,
        )

        self.conv2 = nn.Conv2d(
            in_channels=32,
            out_channels=64,
            kernel_size=4,
            stride=2,
            padding=0,
        )

        self.conv3 = nn.Conv2d(
            in_channels=64,
            out_channels=64,
            kernel_size=3,
            stride=1,
            padding=0,
        )

        # Calculate flattened size
        self.flat_size = self._compute_flat_size()

        # Fully connected layer
        self.fc = nn.Linear(self.flat_size, feature_dim)

        # Initialize weights
        self._initialize_weights()
```

**CHANGE TO** (add LayerNorm layers):
```python
class NatureCNN(nn.Module):
    def __init__(
        self,
        input_channels: int = 4,
        num_frames: int = 4,
        feature_dim: int = 512,
    ):
        super(NatureCNN, self).__init__()

        self.input_channels = input_channels
        self.num_frames = num_frames
        self.feature_dim = feature_dim

        # Convolutional layers
        self.conv1 = nn.Conv2d(
            in_channels=input_channels,
            out_channels=32,
            kernel_size=8,
            stride=4,
            padding=0,
        )
        # ‚úÖ ADD: LayerNorm after Conv1
        # Output shape: (B, 32, 20, 20)
        self.ln1 = nn.LayerNorm([32, 20, 20])

        self.conv2 = nn.Conv2d(
            in_channels=32,
            out_channels=64,
            kernel_size=4,
            stride=2,
            padding=0,
        )
        # ‚úÖ ADD: LayerNorm after Conv2
        # Output shape: (B, 64, 9, 9)
        self.ln2 = nn.LayerNorm([64, 9, 9])

        self.conv3 = nn.Conv2d(
            in_channels=64,
            out_channels=64,
            kernel_size=3,
            stride=1,
            padding=0,
        )
        # ‚úÖ ADD: LayerNorm after Conv3
        # Output shape: (B, 64, 7, 7)
        self.ln3 = nn.LayerNorm([64, 7, 7])

        # Calculate flattened size
        self.flat_size = self._compute_flat_size()

        # Fully connected layer
        self.fc = nn.Linear(self.flat_size, feature_dim)
        # ‚úÖ ADD: LayerNorm after FC
        # Output shape: (B, 512)
        self.ln4 = nn.LayerNorm(feature_dim)

        # Initialize weights
        self._initialize_weights()
```

---

**Current Forward Method** (lines ~185-299):
```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    # Input validation
    if x.dim() != 4:
        raise ValueError(f"Expected 4D input (B, C, H, W), got {x.dim()}D")
    
    # Conv1
    x = F.leaky_relu(self.conv1(x), negative_slope=0.01)
    
    # Conv2
    x = F.leaky_relu(self.conv2(x), negative_slope=0.01)
    
    # Conv3
    x = F.leaky_relu(self.conv3(x), negative_slope=0.01)
    
    # Flatten
    x = x.view(x.size(0), -1)
    
    # FC
    x = F.leaky_relu(self.fc(x), negative_slope=0.01)
    
    return x
```

**CHANGE TO** (add normalization before activation):
```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    # Input validation
    if x.dim() != 4:
        raise ValueError(f"Expected 4D input (B, C, H, W), got {x.dim()}D")
    
    # Conv1 ‚Üí LayerNorm ‚Üí LeakyReLU
    x = self.conv1(x)                              # (B, 32, 20, 20)
    x = self.ln1(x)                                # ‚úÖ NORMALIZE
    x = F.leaky_relu(x, negative_slope=0.01)      # (B, 32, 20, 20)
    
    # Conv2 ‚Üí LayerNorm ‚Üí LeakyReLU
    x = self.conv2(x)                              # (B, 64, 9, 9)
    x = self.ln2(x)                                # ‚úÖ NORMALIZE
    x = F.leaky_relu(x, negative_slope=0.01)      # (B, 64, 9, 9)
    
    # Conv3 ‚Üí LayerNorm ‚Üí LeakyReLU
    x = self.conv3(x)                              # (B, 64, 7, 7)
    x = self.ln3(x)                                # ‚úÖ NORMALIZE
    x = F.leaky_relu(x, negative_slope=0.01)      # (B, 64, 7, 7)
    
    # Flatten
    x = x.view(x.size(0), -1)                     # (B, 3136)
    
    # FC ‚Üí LayerNorm ‚Üí LeakyReLU
    x = self.fc(x)                                 # (B, 512)
    x = self.ln4(x)                                # ‚úÖ NORMALIZE
    x = F.leaky_relu(x, negative_slope=0.01)      # (B, 512)
    
    return x
```

---

### Docstring Update (Optional but Recommended)

Add to class docstring:
```python
"""
NatureCNN visual feature extractor with LayerNorm for stable training.

Architecture:
    Input:   (B, 4, 84, 84) - 4 stacked grayscale frames
    Conv1:   (B, 32, 20, 20) ‚Üí LayerNorm ‚Üí LeakyReLU
    Conv2:   (B, 64, 9, 9)   ‚Üí LayerNorm ‚Üí LeakyReLU
    Conv3:   (B, 64, 7, 7)   ‚Üí LayerNorm ‚Üí LeakyReLU
    Flatten: (B, 3136)
    FC:      (B, 512)        ‚Üí LayerNorm ‚Üí LeakyReLU
    Output:  512-dimensional feature vector

Normalization Strategy:
    LayerNorm is used instead of BatchNorm2d for better stability in RL:
    - Independent of batch size (stable with small batches from replay buffer)
    - Same statistics in train/eval mode (deterministic behavior)
    - Per-sample normalization (no batch dependencies)
    
    Reference:
    - Ba et al. (2016): "Layer Normalization" - https://arxiv.org/abs/1607.06450
    - Stabilizes CNN features to L2 norm ~10-100 (vs 10¬π¬≤ without normalization)

References:
    - Mnih et al. (2015): "Human-level control through deep reinforcement learning," Nature
    - Fujimoto et al. (2018): "Addressing Function Approximation Error in Actor-Critic Methods"
    - Ba et al. (2016): "Layer Normalization"
"""
```

---

## STEP 2: Test Implementation (10 minutes)

### Quick Python Test

Create temporary test file `test_cnn_norm.py`:
```python
#!/usr/bin/env python3
import sys
sys.path.insert(0, '/workspace/av_td3_system')

import torch
from src.networks.cnn_extractor import NatureCNN

# Create CNN
cnn = NatureCNN(input_channels=4, feature_dim=512)
print("‚úÖ CNN initialized successfully")

# Test forward pass
batch_size = 32
dummy_input = torch.randn(batch_size, 4, 84, 84)
print(f"Input shape: {dummy_input.shape}")
print(f"Input L2 norm: {torch.norm(dummy_input).item():.2f}")

# Forward pass
output = cnn(dummy_input)
print(f"\nOutput shape: {output.shape}")
print(f"Output L2 norm: {torch.norm(output).item():.2f}")

# Check feature statistics
print(f"\nFeature Statistics:")
print(f"  Mean: {output.mean().item():.4f}")
print(f"  Std:  {output.std().item():.4f}")
print(f"  Min:  {output.min().item():.4f}")
print(f"  Max:  {output.max().item():.4f}")

# Expected results
expected_norm = 100  # Should be around 10-100
actual_norm = torch.norm(output).item()

if actual_norm < expected_norm:
    print(f"\n‚úÖ SUCCESS: L2 norm {actual_norm:.2f} < {expected_norm}")
    print("LayerNorm is working correctly!")
else:
    print(f"\n‚ùå FAIL: L2 norm {actual_norm:.2f} >= {expected_norm}")
    print("Check LayerNorm implementation")

# Check all LayerNorm layers exist
print("\nLayerNorm Layers:")
for name, module in cnn.named_modules():
    if isinstance(module, torch.nn.LayerNorm):
        print(f"  ‚úÖ {name}: {module}")

print("\n‚úÖ Test complete!")
```

Run test:
```bash
cd /workspace/av_td3_system
python test_cnn_norm.py
```

**Expected Output**:
```
‚úÖ CNN initialized successfully
Input shape: torch.Size([32, 4, 84, 84])
Input L2 norm: 327.68

Output shape: torch.Size([32, 512])
Output L2 norm: 45.23  ‚Üê Should be 10-100

Feature Statistics:
  Mean: 1.2345
  Std:  8.4567
  Min:  -15.2341
  Max:  +18.9876

‚úÖ SUCCESS: L2 norm 45.23 < 100
LayerNorm is working correctly!

LayerNorm Layers:
  ‚úÖ ln1: LayerNorm([32, 20, 20])
  ‚úÖ ln2: LayerNorm([64, 9, 9])
  ‚úÖ ln3: LayerNorm([64, 7, 7])
  ‚úÖ ln4: LayerNorm([512])

‚úÖ Test complete!
```

If test passes, clean up:
```bash
rm test_cnn_norm.py
```

---

## STEP 3: Smoke Test in Training (10 minutes)

Run minimal training to verify normalization in actual training loop:

```bash
cd /workspace/av_td3_system
python scripts/train_td3.py \
  --scenario 0 \
  --max-timesteps 100 \
  --eval-freq 1000 \
  --seed 42 \
  --debug
```

**Monitor Logs**:
```bash
# In another terminal
tail -f data/logs/TD3_*/train.log | grep "CNN Feature Stats"
```

**Expected Output**:
```
Step 0    CNN Feature Stats: L2 Norm: 42.34  Mean: 1.23  Std: 8.45
Step 10   CNN Feature Stats: L2 Norm: 38.91  Mean: 0.98  Std: 7.82
Step 20   CNN Feature Stats: L2 Norm: 45.67  Mean: 1.45  Std: 9.12
...
Step 100  CNN Feature Stats: L2 Norm: 41.23  Mean: 1.12  Std: 8.34
```

**Compare with Previous** (WITHOUT normalization):
```
Step 0    CNN Feature Stats: L2 Norm: 14,235,678,912  Mean: 42,345,678  Std: 89,234,567
Step 100  CNN Feature Stats: L2 Norm: 234,567,891,234 Mean: 567,890,123 Std: 1,234,567,890
```

**Success Criteria**:
- ‚úÖ L2 norm < 100 throughout training
- ‚úÖ Mean: -10 to +10
- ‚úÖ Std: < 50
- ‚úÖ No NaN or inf values
- ‚úÖ No crashes

---

## STEP 4: Full 5K Validation (1 hour)

If smoke test passes, run full 5K validation:

```bash
cd /workspace/av_td3_system
python scripts/train_td3.py \
  --scenario 0 \
  --max-timesteps 5000 \
  --eval-freq 5001 \
  --seed 42 \
  --debug
```

**Monitor Progress**:
```bash
# Watch training
tail -f data/logs/TD3_*/train.log

# Monitor TensorBoard in parallel
tensorboard --logdir data/logs --port 6006
```

**Success Criteria** (from SYSTEMATIC_METRICS_VALIDATION.md):

### CNN Features:
```
‚úÖ L2 Norm:  < 100         (was: 7.36 √ó 10¬π¬≤)
‚úÖ Mean:     -10 to +10    (was: 14.3 billion)
‚úÖ Std:      < 50          (was: 325 billion)
‚úÖ Range:    < 200         (was: 865 billion)
```

### Training Metrics:
```
‚úÖ Critic Loss:        < 100, decreasing     (was: mean=987, max=7500)
‚úÖ Actor Loss:         -1000 to -10000       (was: -5.9 √ó 10¬π¬≤)
‚úÖ Q-Values:           -10 to +10            (was: -49 to +103)
‚úÖ TD Error:           < 5, decreasing       (was: 9.7, stable)
‚úÖ Episode Rewards:    Improving             (was: -913 decline)
‚úÖ Gradient Alerts:    0 warnings, 0 critical (was: 0, still good)
```

### TensorBoard Metrics to Check:
1. `debug/cnn_features_l2_norm` ‚Üí Should be flat line around 10-100
2. `train/critic_loss` ‚Üí Should decrease from ~100 to ~10
3. `train/actor_loss` ‚Üí Should be negative thousands (not trillions)
4. `debug/td_error_q1` ‚Üí Should decrease towards 1
5. `train/episode_reward` ‚Üí Should show increasing trend
6. `debug/q1_mean` ‚Üí Should stabilize around policy's expected return
7. `alerts/gradient_explosion_warning` ‚Üí Should remain 0
8. `alerts/gradient_explosion_critical` ‚Üí Should remain 0

---

## STEP 5: Extended 50K Validation (8-12 hours)

**Only proceed if 5K validation passes all criteria**

```bash
cd /workspace/av_td3_system
python scripts/train_td3.py \
  --scenario 0 \
  --max-timesteps 50000 \
  --eval-freq 10000 \
  --checkpoint-freq 10000 \
  --seed 42 \
  --debug
```

**Additional Success Criteria**:
```
‚úÖ Episode rewards show clear learning (e.g., 50 ‚Üí 500)
‚úÖ Critic loss converges (e.g., 100 ‚Üí 10 ‚Üí 1)
‚úÖ TD error < 1 by end of training
‚úÖ Evaluation success rate > 50%
‚úÖ No instability or divergence
```

---

## STEP 6: Final 1M Production Run

**Only proceed after 50K validation passes**

```bash
cd /workspace/av_td3_system
python scripts/train_td3.py \
  --scenario 0 \
  --max-timesteps 1000000 \
  --eval-freq 25000 \
  --checkpoint-freq 50000 \
  --num-eval-episodes 20 \
  --seed 42
```

**Monitor**:
- TensorBoard every 25K steps
- Check checkpoints saved every 50K
- Review evaluation metrics every 25K

---

## Troubleshooting

### Issue 1: Import Error
```
ModuleNotFoundError: No module named 'torch.nn.LayerNorm'
```

**Solution**: LayerNorm is built-in to PyTorch. Check you're using:
```bash
python -c "import torch; print(torch.__version__)"
# Should be >= 1.0.0
```

---

### Issue 2: Shape Mismatch
```
RuntimeError: Given normalized_shape=[32, 20, 20], expected input with shape [*, 32, 20, 20], but got input of size [32, 32, 21, 21]
```

**Solution**: Verify convolutional output shapes:
```python
# Test each layer individually
x = torch.randn(1, 4, 84, 84)
print(f"Input: {x.shape}")

x = conv1(x)
print(f"After Conv1: {x.shape}")  # Should be [1, 32, 20, 20]

x = conv2(x)
print(f"After Conv2: {x.shape}")  # Should be [1, 64, 9, 9]

x = conv3(x)
print(f"After Conv3: {x.shape}")  # Should be [1, 64, 7, 7]
```

**Shape Calculation**:
```
Conv1: (84 - 8) / 4 + 1 = 20  ‚úì
Conv2: (20 - 4) / 2 + 1 = 9   ‚úì
Conv3: (9 - 3) / 1 + 1 = 7    ‚úì
```

---

### Issue 3: L2 Norm Still High
```
Output L2 norm: 5234.56  (Expected: < 100)
```

**Solution**:
1. Check LayerNorm is applied BEFORE activation:
   ```python
   # ‚úÖ CORRECT:
   x = self.conv1(x)
   x = self.ln1(x)
   x = F.leaky_relu(x)
   
   # ‚ùå WRONG:
   x = F.leaky_relu(self.conv1(x))
   x = self.ln1(x)
   ```

2. Verify all 4 LayerNorm layers are present:
   ```python
   for name, module in cnn.named_modules():
       if isinstance(module, torch.nn.LayerNorm):
           print(name, module)
   # Should print: ln1, ln2, ln3, ln4
   ```

---

### Issue 4: Training Slower
```
Training speed: 45 FPS (was: 60 FPS)
```

**Explanation**: LayerNorm adds computational overhead (~25% slowdown)

**Trade-off**:
- ‚ùå 25% slower training
- ‚úÖ 10¬π‚Å∞√ó feature stabilization
- ‚úÖ Prevents training collapse
- ‚úÖ Enables successful learning

**Mitigation**: Use GPU acceleration (if available)

---

## Validation Checklist

### Pre-Implementation
- [ ] Read CNN_IMPLEMENTATION_ANALYSIS.md
- [ ] Read CRITICAL_FIXES_REQUIRED.md
- [ ] Understand why normalization is needed

### Implementation
- [ ] Add `self.ln1 = nn.LayerNorm([32, 20, 20])` in `__init__`
- [ ] Add `self.ln2 = nn.LayerNorm([64, 9, 9])` in `__init__`
- [ ] Add `self.ln3 = nn.LayerNorm([64, 7, 7])` in `__init__`
- [ ] Add `self.ln4 = nn.LayerNorm(512)` in `__init__`
- [ ] Update `forward()` to apply normalization before activation
- [ ] Update docstring with normalization details

### Testing
- [ ] Run standalone CNN test (`test_cnn_norm.py`)
- [ ] Verify L2 norm < 100
- [ ] Run smoke test (100 steps)
- [ ] Verify no crashes or NaN values

### Validation
- [ ] Run 5K validation
- [ ] Check all metrics vs SYSTEMATIC_METRICS_VALIDATION.md
- [ ] Verify critic loss < 100 and decreasing
- [ ] Verify episode rewards improving
- [ ] Run 50K validation
- [ ] Verify long-term stability

### Production
- [ ] All validation criteria passed
- [ ] Document results in TensorBoard
- [ ] Update paper with normalization details
- [ ] Proceed to 1M production run

---

## Expected Timeline

```
Day 1:
  09:00 - 09:30  Read analysis documents
  09:30 - 10:00  Implement LayerNorm in CNN
  10:00 - 10:10  Test implementation
  10:10 - 10:20  Smoke test (100 steps)
  10:20 - 11:30  5K validation
  11:30 - 12:00  Analyze results

Day 1-2:
  12:00 - 24:00  50K validation (8-12 hours)

Day 2:
  08:00 - 09:00  Analyze 50K results
  09:00 - 09:30  Document findings
  09:30 - 10:00  Prepare for 1M run

Day 2-5:
  10:00 - ...    1M production run (24-72 hours)
```

**Total Time**: 1-2 days to production-ready

---

## Success Indicators

### Immediate (After Implementation)
‚úÖ Code compiles without errors  
‚úÖ Standalone test shows L2 norm < 100  
‚úÖ Smoke test completes without crashes

### Short-term (After 5K Validation)
‚úÖ CNN features stable (L2 norm 10-100)  
‚úÖ Critic loss decreasing  
‚úÖ Episode rewards improving  
‚úÖ No gradient explosions

### Medium-term (After 50K Validation)
‚úÖ Training dynamics healthy  
‚úÖ Agent learning (eval success > 50%)  
‚úÖ No instability or divergence

### Long-term (After 1M Production)
‚úÖ Agent achieves paper objectives  
‚úÖ Performance superior to DDPG baseline  
‚úÖ Safe and stable policies

---

## References

- **Analysis**: `CNN_IMPLEMENTATION_ANALYSIS.md`
- **Critical Fixes**: `CRITICAL_FIXES_REQUIRED.md`
- **Metrics Baseline**: `SYSTEMATIC_METRICS_VALIDATION.md`
- **PyTorch LayerNorm**: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html

---

**Status**: READY FOR IMPLEMENTATION  
**Estimated Time**: 30 minutes + validation  
**Critical**: Must complete before 1M production run  
**Next Action**: Modify `src/networks/cnn_extractor.py`
