# ROOT CAUSE DIAGNOSIS: Right-Turn Bias During Learning Phase

**Date**: 2025-01-21
**Run**: run5 (5K timesteps, TD3 training)
**Issue**: Agent turns hard right (+0.88 steering) immediately when entering learning phase

---

## üî¥ CRITICAL FINDING: MASSIVE REWARD IMBALANCE

### Empirical Evidence (TensorBoard Metrics)

**Action Statistics During Learning Phase (steps 1100-5000)**:
```
Average Steering:  +0.8765  ‚ö†Ô∏è CRITICAL: Should be ~0.0 for balanced behavior
Average Throttle:  +0.8795  ‚úÖ Expected: >0.0 for forward motion

Steering Evolution:
  Step 1100: +0.8331
  Step 1200: +0.8698
  Step 1300: +0.8994  ‚Üê INCREASING bias (agent learning wrong policy!)
  Step 1400: +0.8807
  Step 1500: +0.8926
  Step 1600: +0.8801
  Step 1700: +0.8802
```

### Configuration Analysis

**Reward Weights** (carla_config.yaml):
```yaml
weights:
  efficiency: 3.0      # 3√ó weight
  lane_keeping: 1.0    # 1√ó weight (REDUCED from 2.0)
  comfort: 0.5
  safety: 1.0          # ‚úÖ Correct (fixed from -100.0)
```

**Progress Parameters** (training_config.yaml):
```yaml
progress:
  waypoint_bonus: 10.0
  distance_scale: 50.0  # üî• CRITICAL: 50√ó signal amplification!
```

### Mathematical Analysis

**Progress Reward Calculation**:
```python
# From reward_functions.py line 968
distance_reward = distance_delta * self.distance_scale

# With distance_scale = 50.0:
# Moving 0.1 meters = +5.0 reward (before weight)
# After efficiency weight (3.0): +15.0 total

# For comparison:
# Lane keeping penalty: -1.0 (lateral error)
# After weight (1.0): -1.0 total
```

**Reward Ratio** (Progress vs Lane Keeping):
```
Progress signal: +15.0 (0.1m progress with weight 3.0 √ó scale 50.0)
Lane keeping:    -1.0  (lateral error with weight 1.0)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RATIO: 15:1 progress dominance ‚Üê EXPLAINS RIGHT-TURN BIAS!
```

### Why Right-Turning Maximizes Reward

**Agent's Learned Policy**:
1. **Hard right + full throttle** = maximize forward progress
2. Lane keeping penalty (-1.0) << Progress reward (+15.0)
3. Net reward: +14.0 even while invading lanes!

**Perverse Incentive**:
```
Behavior               | Progress | Lane | Safety | TOTAL
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Straight (lane center) | +15.0    | 0.0  | 0.0    | +15.0
Right turn (invading)  | +15.0    | -1.0 | 0.0    | +14.0  ‚Üê STILL PROFITABLE!
```

**Result**: Agent learns that lane invasion is **acceptable collateral damage** for progress maximization!

---

## üìä Comparison with Official TD3 Examples

### OpenAI Spinning Up TD3
- **Reward Design**: Balanced components (~20-30% each)
- **Progress Weight**: Implicit (part of task reward)
- **Scale**: Normalized to [-1, 1] range

### Stable-Baselines3 TD3 (MuJoCo)
- **Reward**: Sparse, goal-oriented
- **Component Weights**: Equal importance
- **Scale**: Environment-specific normalization

### Our Implementation (WRONG!)
- **Progress**: **50√ó scale** + **3√ó weight** = **150√ó amplification** üî•
- **Lane Keeping**: **1√ó weight** (no scale)
- **Ratio**: **150:1** (progress dominates)

---

## üîç Timeline of Bug Introduction

1. **Initial Design** (`distance_scale: 0.1`):
   - Too weak signal ‚Üí agent stationary
   - Fixed in commit: "PBRS reward shaping"

2. **First Fix** (`distance_scale: 1.0`):
   - Moderate signal strength
   - Likely still weak relative to penalties

3. **Over-Correction** (`distance_scale: 50.0`):
   - **CURRENT STATE** ‚Üê Bug introduced here!
   - Intention: "Enable agent to offset collision penalties"
   - Reality: Created extreme imbalance

**Comment from training_config.yaml**:
```yaml
# This 50x increase enables agent to offset collision penalties through good driving:
# - Collision: -5.0 penalty
# - Good driving (0.1m progress): +5.0 reward
# - Break-even: 0.1 meters of progress = reasonable exploration cost
```

**Critical Error**: Logic assumes collision penalty = -5.0, but actual penalty = -200.0!

---

## ‚úÖ ACTION SELECTION IMPLEMENTATION (VERIFIED CORRECT)

### TD3 Specification vs Implementation

**OpenAI Spinning Up**:
```python
# Training: a = clip(Œº_Œ∏(s) + Œµ, -1, 1) where Œµ ~ N(0, œÉ)
# Evaluation: a = Œº_Œ∏(s) (deterministic)
```

**Our Implementation** (td3_agent.py lines 310-380):
```python
def select_action(state, noise=None, deterministic=False):
    with torch.no_grad():
        action = self.actor(state_tensor).cpu().numpy().flatten()

    if not deterministic and noise is not None and noise > 0:
        noise_sample = np.random.normal(0, noise, size=self.action_dim)
        action = action + noise_sample
        action = np.clip(action, -self.max_action, self.max_action)

    return action
```

**Verdict**: ‚úÖ **MATCHES SPECIFICATION EXACTLY**

### Curriculum Learning (train_td3.py)

```python
# Exponential noise decay: 0.3 ‚Üí 0.1 over 20K steps
if t < start_timesteps:
    action = env.action_space.sample()  # Random exploration
else:
    noise_min = 0.1
    noise_max = 0.3
    decay_steps = 20000
    decay_rate = 5.0 / decay_steps
    current_noise = noise_min + (noise_max - noise_min) * exp(-decay_rate * t)
    action = agent.select_action(state, noise=current_noise)
```

**Verdict**: ‚úÖ **CORRECT** - Follows TD3 best practices

---

## üéØ THE REAL FIX (Route-Following Reward)

### Problem Summary
**Current**: Progress reward = Euclidean distance to FINAL goal
**Result**: Agent learns diagonal shortcuts (off-road optimal!)
**Solution**: Progress reward = Distance along ROUTE waypoints

### Option 1: Route Distance (RECOMMENDED - Matches Paper Intent)

**Philosophy**: Reward following the PLANNED ROUTE, not shortcuts

```python
# reward_functions.py - NEW implementation
def calculate_progress(self, vehicle_location, waypoint_reached, goal_reached):
    """
    Calculate progress reward based on ROUTE DISTANCE, not Euclidean distance.

    This prevents the agent from learning shortcuts that leave the road.
    Reference: "Interpretable End-to-end Urban Autonomous Driving" (Chen et al.)
    """
    # Calculate distance along remaining waypoints (route distance)
    route_distance = self.waypoint_manager.get_route_distance_to_goal(vehicle_location)

    if self.prev_route_distance is not None:
        # Reward = reduction in route distance
        distance_delta = self.prev_route_distance - route_distance
        distance_reward = distance_delta * self.distance_scale  # Can keep scale=50.0!
        progress += distance_reward

    self.prev_route_distance = route_distance

    # ... rest of implementation (waypoint bonus, etc.)
```

**Key Change**: Use **route distance** (following waypoints) instead of **Euclidean distance** (straight line)

**Expected Behavior**:
- Driving FORWARD along road: +reward (reduces route distance)
- Turning RIGHT off-road: 0 or negative (increases route distance!)
- Following planned turn at intersection: +reward (correct navigation)

### Option 2: Waypoint-Based Progress (SIMPLER)

**Philosophy**: Reward reaching waypoints in sequence, ignore distance

```python
def calculate_progress(self, waypoint_index, waypoint_reached, goal_reached):
    """Sparse reward based on waypoint milestones only."""
    progress = 0.0

    # Only reward waypoint milestones (no dense distance signal)
    if waypoint_reached:
        progress += self.waypoint_bonus  # e.g., +10.0

    if goal_reached:
        progress += self.goal_reached_bonus  # e.g., +100.0

    return progress
```

**Pros**: Simple, no distance calculations, hard to exploit
**Cons**: Sparse signal, slower learning

### Option 3: Reduce distance_scale (TEMPORARY WORKAROUND)

**If you can't implement route distance quickly**, reduce scale to make shortcuts unprofitable:

```yaml
# config/training_config.yaml
progress:
  distance_scale: 1.0  # Down from 50.0
```

**Expected Impact**:
```
Off-road shortcut (0.3m Euclidean reduction):
  Progress: 0.3 √ó 1.0 √ó 3.0 = +0.9
  Lane invasion: -10.0
  NET: -9.1 ‚Üê UNPROFITABLE! ‚úÖ
```

**Cons**: Weak progress signal, agent may still prefer staying still

---

## üî¨ VALIDATION PLAN

### Step 1: Fix Configuration
- [ ] Set `distance_scale: 1.0` (down from 50.0)
- [ ] Set `efficiency weight: 1.0` (down from 3.0)
- [ ] Verify config loaded correctly in logs

### Step 2: Short Test Run (1K steps)
- [ ] Monitor action_steering_mean (expect ~0.0 ¬± 0.1)
- [ ] Monitor reward percentages (expect ~25% each)
- [ ] Verify no extreme right bias

### Step 3: Full Training Run (20K steps)
- [ ] Collect TensorBoard metrics
- [ ] Compare action distributions
- [ ] Evaluate learned policy

### Expected Results (After Fix)

**Action Statistics**:
```
Steering Mean:  ~0.0 ¬± 0.2  (balanced left/right)
Throttle Mean:  ~0.5 ¬± 0.3  (forward motion)
```

**Reward Distribution**:
```
Progress:      25-30%  (balanced)
Lane Keeping:  25-30%  (balanced)
Efficiency:    20-25%
Safety:        15-20%
Comfort:       5-10%
```

---

## üìö References

1. **Official TD3 Paper**: "Addressing Function Approximation Error in Actor-Critic Methods" (Fujimoto et al. 2018)
   - Section 4: Exploration vs Exploitation
   - Action Selection: `a = clip(Œº_Œ∏(s) + Œµ, -1, 1)`

2. **OpenAI Spinning Up**:
   - https://spinningup.openai.com/en/latest/algorithms/td3.html
   - Gaussian noise exploration (not OU process)

3. **Stable-Baselines3 TD3**:
   - https://stable-baselines3.readthedocs.io/en/master/modules/td3.html
   - Default exploration_noise = 0.1

4. **Reward Shaping Theory**:
   - Ng et al. (1999): "Policy Invariance Under Reward Transformations"
   - PBRS theorem: F(s,s') = Œ≥Œ¶(s') - Œ¶(s) preserves optimality

---

## üî¥ ACTUAL ROOT CAUSE (CORRECTED ANALYSIS)

**The agent IS learning correctly - but from SPARSE, MISLEADING data!**

### What's REALLY Happening

**Observation from User**:
> "During exploration (random actions): Vehicle drives forward, reaches waypoints
> During learning (actor policy): Vehicle IMMEDIATELY turns right OFF-ROAD onto sidewalk"

**Route Geometry** (from waypoints.txt):
```
Start: (317, 129) ‚Üí Go STRAIGHT west 100+ waypoints ‚Üí Turn RIGHT at intersection ‚Üí Goal: (92, 86)
```

**The Problem**: After only **1000 random exploration steps**, the Q-networks learn:
- Q1/Q2 values: **+14 to +16** (critics think right-turning is great!)
- Actual rewards: **+12 to +14** (confirms this policy gets rewards)
- **WHY?** Progress reward uses **straight-line distance to goal** (92, 86)

### Mathematical Proof of the Bug

**Euclidean Distance Reward** (current implementation):
```python
# reward_functions.py line 968
distance_reward = (prev_distance - current_distance) * distance_scale

# With distance_scale = 50.0:
# Moving toward goal (92, 86) from (317, 129):
#   ŒîX: -225m (west)
#   ŒîY: -43m (south)
# Angle to goal: -169¬∞ (southwest)
```

**The Perverse Incentive**:
```
Vehicle at start (317, 129), heading WEST:

Option 1: Drive STRAIGHT west (follow road)
  - Reduces X distance to goal: -225m ‚Üí progress reward
  - Y distance unchanged: 0 reward for southward component
  - TOTAL PROGRESS: Moderate

Option 2: Turn RIGHT immediately (diagonal shortcut)
  - Reduces BOTH X and Y distance (Pythagorean)
  - Goes OFF-ROAD but MAXIMIZES Euclidean distance reduction!
  - TOTAL PROGRESS: HIGHER! ‚Üê Q-networks learn this!
```

**Result**: Q-networks correctly learn that "diagonal right turn = max progress" even though it's OFF-ROAD!

### Why `distance_scale=50.0` Matters (You Were Right!)

The `distance_scale=50.0` **amplifies the wrong signal**:

```
Scenario: Vehicle turns right 0.5m (off-road):
  Distance reduction: ~0.3m (Pythagorean)
  Progress reward: 0.3 √ó 50.0 √ó 3.0 (efficiency weight) = +45.0

  Lane invasion penalty: -10.0 (single invasion)

  NET REWARD: +35.0 ‚Üê HIGHLY PROFITABLE!
```

The agent **rationally exploits** the Euclidean distance metric!

---

**Next Steps**:
1. Apply Option 1 fix (balanced reward scaling)
2. Run validation test (1K steps)
3. Monitor TensorBoard metrics
4. Full training run if validation passes
