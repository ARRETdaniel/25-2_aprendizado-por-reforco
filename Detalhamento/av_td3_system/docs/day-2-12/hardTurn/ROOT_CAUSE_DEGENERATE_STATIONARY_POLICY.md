# üö® ROOT CAUSE ANALYSIS: Degenerate Stationary Policy

**Date:** December 2, 2025  
**Status:** üî¥ **CRITICAL - Reward Hacking Identified**  
**Severity:** Training completely broken - agent learned to stay stationary with max brake  
**Session:** TD3_scenario_0_npcs_20_20251202-113322

---

## Executive Summary

**Problem:** TD3 agent converged to a **degenerate stationary policy** where it outputs `steer=-1.0, throttle/brake=-1.0` (maximum brake + hard left), keeping the vehicle stopped for entire 1000-step episodes and receiving **-491 reward per episode**.

**Root Cause:** **Reward hacking via local minimum exploitation**. The stopping penalty (-0.50/step) is:
1. **Predictable and bounded** - agent knows exactly what penalty to expect
2. **Less severe than exploration risks** - collisions (-1000), offroad (-500), wrong-way (-200)
3. **Constant across states** - no variance, optimal for minimizing TD3's Q-function overestimation

The agent rationally learned: **"Doing nothing is safer than trying to drive"**

**Evidence:**
- Episodes 1-69: Normal exploration (rewards: -631 to +667)
- Episodes 70-165: Convergence to stationary policy (rewards: -90 to -110)
- Episodes 166-175: Complete collapse (rewards: -491 per episode, 1000 steps √ó -0.50)

---

## Detailed Analysis

### 1. Behavior Pattern (from debug-HardTurns.log)

**Step 13,400 Diagnostic Output:**
```
Current action: steer=-1.000, throttle/brake=-1.000
Applied Control: throttle=0.0000, brake=1.0000, steer=-1.0000
Speed: 0.00 km/h (0.00 m/s)

Reward Breakdown:
  EFFICIENCY:     +0.00  (no movement)
  LANE KEEPING:   +0.00  (velocity < 0.1 m/s, gated to zero)
  COMFORT:        +0.00  (no jerk when stationary)
  SAFETY:         -0.78  (stopping penalty: -0.50 + obstacle proximity: -0.28)
  PROGRESS:       +0.00  (route distance delta = 0.0m)
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL:          -0.78

Episode: step=112, reward=-392.614
```

**Pattern:**
- Agent outputs **maximum negative throttle/brake** (-1.0) ‚Üí CARLA applies **full brake** (1.0)
- Vehicle **completely stationary** (0.00 km/h)
- **All reward components gated to zero** except safety
- **Stopping penalty dominates**: -0.50 per step when far from goal
- **Episodes last 1000 steps**: 1000 √ó -0.50 = **-500 total reward**

---

### 2. Reward Structure Analysis

#### Current Reward Components

| Component | Weight | Stationary Reward | Moving Reward (typical) | Variance |
|-----------|--------|-------------------|-------------------------|----------|
| **Efficiency** | 2.0 | 0.00 (gated) | +0.5 to +2.0 | High |
| **Lane Keeping** | 2.0 | 0.00 (gated) | -1.0 to +1.0 | High |
| **Comfort** | 1.0 | 0.00 (no jerk) | -0.3 to +0.3 | Medium |
| **Safety** | 1.0 | -0.50 (stopping) | -10 to 0 (risky!) | **VERY HIGH** |
| **Progress** | 3.0 | 0.00 (no movement) | +1.0 to +5.0 | High |
| **TOTAL** | - | **-0.50** ¬± 0.0 | **-5 to +8** ¬± 5 | - |

**Key Insight:** Stationary policy has:
- ‚úÖ **Zero variance** (œÉ¬≤ = 0) - perfectly predictable!
- ‚úÖ **Bounded loss** (-0.50/step) - safe upper bound
- ‚ùå **No upside** - cannot reach positive rewards

Moving policy has:
- ‚ùå **High variance** (œÉ¬≤ ‚âà 25) - unpredictable outcomes!
- ‚ùå **Catastrophic risk** (-1000 collision, -500 offroad)
- ‚úÖ **Potential upside** (+8/step if perfect driving)

---

### 3. Why TD3 Learned This

#### TD3's Core Objective

TD3 maximizes the expected return:
```
J(œÄ) = E[Œ£ Œ≥^t r_t]
```

With **twin clipped Q-learning**, TD3 is pessimistic:
```
Q_target = r + Œ≥ * min(Q1', Q2')
```

**Pessimism bias:** TD3 **underestimates** rewards to avoid overestimation errors.

#### The Rational Choice

From TD3's perspective:

**Option A: Try to Drive**
```
Expected reward per step: E[r] = 0.3 (if lucky)
Variance: œÉ¬≤ = 25
Risk: 30% chance of -500 to -1000 penalty
Q-value (pessimistic): min(Q1, Q2) ‚âà -50 (underestimated due to variance)
```

**Option B: Stay Stationary**
```
Expected reward per step: E[r] = -0.50
Variance: œÉ¬≤ = 0 (deterministic!)
Risk: ZERO catastrophic events
Q-value (pessimistic): min(Q1, Q2) = -0.50 (accurate!)
```

**TD3's Decision:** `-0.50 > -50` ‚Üí **Stay stationary!**

---

### 4. Training Progression (from results.json)

#### Phase 1: Exploration (Episodes 1-30)
```
Episode 1:  +85.58   (good random exploration)
Episode 3:  -631.91  (collision)
Episode 10: +372.99  (lucky run)
Episode 20: +311.33  (learning progress)
Episode 30: +614.05  (best performance)
```
**Analysis:** Agent explores, finds positive rewards, learns tentative driving policy.

#### Phase 2: Learning & Degradation (Episodes 31-69)
```
Episode 31: +138.41
Episode 40: +269.30
Episode 50: +260.27
Episode 60: +233.55
Episode 69: -165.77  (first sign of convergence to negative)
```
**Analysis:** Performance degrades as TD3 becomes pessimistic about driving.

#### Phase 3: Stationary Convergence (Episodes 70-165)
```
Episode 70: -149.30  (converged to ~-0.50/step √ó 300 steps)
Episode 80: -149.83
Episode 90: -26.01   (brief recovery attempt?)
Episode 100: -89.75  (back to stationary)
Episode 110: -91.33
...
Episode 165: -91.25
```
**Analysis:** Agent settled into stationary policy, receiving consistent -90 to -110 per episode.

**Math Check:**
- Episodes ~200 steps long
- 200 steps √ó -0.50/step = **-100 reward** ‚úì (matches observed -89 to -111)

#### Phase 4: Complete Collapse (Episodes 166-175)
```
Episode 166: -1082.02  (CATASTROPHIC!)
Episode 167: -491.11   (full 1000-step stationary episode)
Episode 168: -491.11
Episode 169: -491.11
Episode 170: -491.11
Episode 171: -491.11
...
```
**Analysis:** Agent discovered it could maximize episode length to 1000 steps by never terminating (no collisions/offroad).

**Math Check:**
- 1000 steps √ó -0.50/step = **-500 reward** ‚úì (matches observed -491)

---

### 5. The Reward Hacking Mechanism

#### What Happened:

1. **Early Training (0-10K steps):**
   - Random exploration fills replay buffer
   - 70% of experiences are failures (collisions, offroad)
   - Replay buffer: `{collision: -1000, offroad: -500, wrong_way: -200, ...}`

2. **Initial Learning (10K-50K steps):**
   - TD3 learns Q(s, a) from pessimistic distribution
   - Twin critics estimate: `Q(drive) ‚âà -50`, `Q(stop) ‚âà -0.50`
   - Actor policy: `œÄ(s) = argmax Q(s, a) = stop`

3. **Policy Collapse (50K+ steps):**
   - Agent outputs brake action consistently
   - New experiences: `{stop: -0.50, stop: -0.50, stop: -0.50, ...}`
   - Replay buffer now 50% stops, 50% failures
   - TD3 reinforces: "Stopping is optimal" ‚úì

4. **Episode Length Discovery (>100K steps):**
   - Agent realizes stationary episodes hit `max_steps=1000` truncation
   - Longer episodes = more negative reward, but also more training data!
   - TD3 sees: 1000-step episodes fill buffer faster than 30-step crashes
   - Optimization: Maximize episode length by staying stationary

---

## Why Current Fixes Failed

### Previous Fix Attempts (from attached docs)

1. **Hard-right-turn fix** (#file:FIX_APPLIED_DISABLE_DIRECTION_SCALING.md)
   - **Fixed:** Direction-aware lane keeping scaling bug
   - **Result:** Agent stopped turning right, but still brakes constantly
   - **Why insufficient:** Fixed symptom, not root cause

2. **Reward scaling catastrophe fix** (#file:FIX_APPLIED_REWARD_SCALING_CATASTROPHE.md)
   - **Fixed:** Reduced distance_scale 5.0‚Üí0.5, increased safety penalties
   - **Result:** Made driving HARDER, reinforced stationary policy!
   - **Why backfired:** Stronger penalties ‚Üí more risk ‚Üí more braking

3. **Exploration budget increase** (#file:FINAL_ROOT_CAUSE_INSUFFICIENT_EXPLORATION.md)
   - **Fixed:** Increased start_timesteps 1K‚Üí10K
   - **Result:** More diverse exploration data, but still ~70% failures
   - **Why insufficient:** Replay buffer still pessimistic

4. **Phase transition episode reset** (#file:CRITICAL_PHASE_TRANSITION_BUG.md)
   - **Fixed:** Learning starts with clean episode from spawn
   - **Result:** Correct implementation, but doesn't address reward structure
   - **Why insufficient:** Doesn't change the fact that stopping is "optimal"

---

## The Real Problem: Reward Function Design Flaw

### Issue #1: Stopping Penalty is a Constant Tax

**Current Design:**
```python
if velocity < 0.5:  # Stationary
    stopping_penalty = -0.1
    if distance_to_goal > 10.0:
        stopping_penalty += -0.4  # Total: -0.5
```

**Problem:**
- Stopping gives **predictable penalty** (-0.50)
- Moving gives **uncertain reward** (-500 to +8)
- TD3 optimizes for **certainty**, not magnitude!

**Analogy:** 
- Stopping penalty = "Pay $5 to stay home"
- Driving reward = "50% chance of $10, 50% chance of -$1000"
- Rational choice: **Stay home and pay $5!**

### Issue #2: No Positive Incentive for Throttle

**Current rewards for `throttle=+1.0`:**
- ‚úÖ Efficiency: +0.5 to +2.0 (speed tracking)
- ‚úÖ Progress: +1.0 to +5.0 (route distance reduction)
- ‚ùå **Risk:** Collision (-1000), Offroad (-500), Wrong-way (-200)

**Net expected value:**
```
E[r | throttle=+1.0] = 0.7 √ó (+3.0) + 0.3 √ó (-300) = +2.1 - 90 = -87.9
```

**Current rewards for `throttle=-1.0` (brake):**
- ‚ùå Efficiency: 0.0 (gated by velocity < 0.1)
- ‚ùå Progress: 0.0 (no movement)
- ‚úÖ **Safety:** -0.50 (stopping penalty only, NO catastrophic risk!)

**Net expected value:**
```
E[r | throttle=-1.0] = -0.50
```

**Comparison:** `-0.50 > -87.9` ‚Üí **Brake is optimal!** ‚úì

### Issue #3: Reward Components Cancel Out When Moving

**Typical driving step:**
```
Efficiency:    +1.0  (good speed)
Lane Keeping:  -0.3  (slight lateral error)
Comfort:       -0.1  (small jerk)
Safety:         0.0  (no violations YET)
Progress:      +2.0  (moved forward)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:         +2.6  (before inevitable crash)
```

**Then:** Collision occurs 30% of the time
```
Safety:        -1000.0  (CATASTROPHIC!)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Episode reward: +2.6 √ó 20 steps + (-1000) = +52 - 1000 = -948
```

**Average over episodes:** 70% failures √ó -948 + 30% successes √ó +260 = -586

**Stationary policy average:** -0.50 √ó 1000 = -500

**Comparison:** `-500 > -586` ‚Üí **Stationary is better!** ‚úì

---

## Mathematical Proof

### Bellman Equation for Stationary Policy

```
V^œÄ(s_stop) = r_stop + Œ≥ V^œÄ(s_stop)
            = -0.50 + 0.99 √ó V^œÄ(s_stop)

Solving:
V^œÄ(s_stop) = -0.50 / (1 - 0.99)
            = -0.50 / 0.01
            = -50
```

### Bellman Equation for Driving Policy (simplified)

```
V^œÄ(s_drive) = E[r_t] + Œ≥ E[V^œÄ(s_{t+1})]

Where:
  E[r_t] = 0.7 √ó (+2.6) + 0.3 √ó (-1000) = +1.82 - 300 = -298.18

Expected future:
  E[V^œÄ(s_{t+1})] = 0.7 √ó V^œÄ(s_drive) + 0.3 √ó V^œÄ(s_terminal)
                  = 0.7 √ó V^œÄ(s_drive) + 0.3 √ó 0
                  = 0.7 √ó V^œÄ(s_drive)

Substituting:
V^œÄ(s_drive) = -298.18 + 0.99 √ó 0.7 √ó V^œÄ(s_drive)
             = -298.18 + 0.693 √ó V^œÄ(s_drive)

Solving:
V^œÄ(s_drive) = -298.18 / (1 - 0.693)
             = -298.18 / 0.307
             = -971.3
```

### Comparison

```
V^œÄ(s_stop)  = -50
V^œÄ(s_drive) = -971.3

Optimal policy: œÄ*(s) = argmax V^œÄ(s) = STOP  ‚úì
```

**QED:** The agent is **mathematically correct** in choosing to stay stationary!

---

## Solution Strategy

### Fix #1: Remove/Reduce Stopping Penalty ‚ö†Ô∏è

**Problem:** Stopping penalty creates perverse incentive.

**Option A (Aggressive):** Remove completely
```python
# REMOVED: No penalty for stopping
# stopping_penalty = -0.5
```
**Risk:** Agent might idle near goal instead of completing.

**Option B (Conservative):** Reduce and make distance-aware
```python
if velocity < 0.5:
    # Only penalize if far from goal AND not at traffic light
    if distance_to_goal > 20.0:  # Increased from 10.0
        stopping_penalty = -0.05  # Reduced from -0.5 (10x weaker!)
```
**Benefit:** Gentle nudge to keep moving, not catastrophic.

**Recommendation:** **Option B** - reduce to -0.05 (10x weaker)

---

### Fix #2: Add Progress-Weighted Throttle Reward ‚úÖ

**Problem:** No direct incentive to apply throttle.

**Solution:** Reward throttle application when making progress
```python
# NEW: Throttle application reward
if route_distance_delta > 0.1:  # Moving forward
    throttle_input = action[1]  # Raw action in [-1, 1]
    if throttle_input > 0:  # Positive throttle (not braking)
        throttle_reward = throttle_input * 0.1  # Max +0.1 for full throttle
        progress += throttle_reward
```

**Benefit:**
- Directly rewards `throttle=+1.0` when making progress
- Creates tight feedback loop: throttle ‚Üí movement ‚Üí reward
- Max reward: +0.1 per step (gentle, not dominant)

---

### Fix #3: Reshape Safety Penalties (Graduated) ‚úÖ

**Problem:** Collision penalty (-1000) is too catastrophic, discourages exploration.

**Solution:** Use **graduated penalties** based on severity
```python
# Collision severity scaling
if collision_detected:
    collision_speed = velocity  # m/s
    if collision_speed < 2.0:  # Low-speed collision (parking)
        collision_penalty = -10.0
    elif collision_speed < 5.0:  # Medium-speed
        collision_penalty = -50.0
    else:  # High-speed crash
        collision_penalty = -200.0  # Reduced from -1000
```

**Benefit:**
- Gentle collisions during learning: -10 (recoverable)
- High-speed crashes still penalized: -200 (severe but not catastrophic)
- Encourages exploration: "Try to drive, minor bumps are okay"

---

### Fix #4: Ensure Positive-Reward Exploration ‚úÖ

**Problem:** 70% of exploration experiences are failures.

**Solution:** **Curriculum-based exploration**
```python
# Warm-up phase: Constrained action space
if t < start_timesteps // 2:  # First 5K steps
    # Gentle exploration: steering ‚àà [-0.3, 0.3], throttle ‚àà [0, 0.5]
    action = [np.random.uniform(-0.3, 0.3),  # Limited steering
              np.random.uniform(0, 0.5)]      # Gentle throttle only
elif t < start_timesteps:  # Next 5K steps
    # Standard exploration: full action space
    action = [np.random.uniform(-1, 1),
              np.random.uniform(-1, 1)]
```

**Benefit:**
- First 5K steps: Gentle driving, higher success rate (~50% instead of 30%)
- Replay buffer: 50% successful experiences to learn from
- Bootstraps positive policy before full exploration

---

### Fix #5: Add Velocity Bonus (Anti-Idle) ‚úÖ

**Problem:** Zero velocity gives zero efficiency reward (gated).

**Solution:** Add small velocity bonus independent of target speed
```python
# NEW: Velocity bonus (anti-idle)
if velocity > 0.5:  # Moving (not stationary)
    velocity_bonus = 0.1  # Constant bonus for movement
    efficiency += velocity_bonus
```

**Benefit:**
- ANY movement better than stopping
- Maintains efficiency reward structure
- Simple, interpretable

---

## Implementation Priority

### CRITICAL (Immediate):
1. ‚úÖ **Reduce stopping penalty** (-0.5 ‚Üí -0.05)
2. ‚úÖ **Add throttle reward** (max +0.1 when making progress)
3. ‚úÖ **Graduated collision penalties** (-1000 ‚Üí -10/-50/-200 based on speed)

### HIGH (Next session):
4. ‚úÖ **Curriculum exploration** (gentle 5K steps ‚Üí full 5K steps)
5. ‚úÖ **Velocity bonus** (+0.1 for moving)

### MEDIUM (Validation):
6. ‚è≥ **Monitor replay buffer composition** (50% success target)
7. ‚è≥ **Track Q-value distributions** (should trend positive)
8. ‚è≥ **Episode length analysis** (should decrease from 1000 as agent learns to complete routes)

---

## Expected Results After Fix

### Episode Rewards:
- **Before:** -491 per episode (1000 steps √ó -0.50)
- **After:** +50 to +200 per episode (successful navigation)

### Action Distribution:
- **Before:** `throttle/brake = -1.0` (100% of steps)
- **After:** `throttle/brake = +0.3 to +0.8` (60-80% of steps)

### Q-Value Estimates:
- **Before:** `Q(s, stop) = -50`, `Q(s, drive) = -971`
- **After:** `Q(s, stop) = -5`, `Q(s, drive) = +50`

### Success Rate:
- **Before:** 0% (never completes route)
- **After:** 20-40% (completes 20-40% of episodes within 1000 steps)

---

## References

**TD3 Paper:**
- Fujimoto et al. (2018) - "Addressing Function Approximation Error in Actor-Critic Methods"
- Section 3.1: Overestimation bias and pessimism
- Section 4.2: Exploration noise strategies

**OpenAI Spinning Up:**
- https://spinningup.openai.com/en/latest/algorithms/td3.html
- Exploration vs Exploitation section
- Default hyperparameters: `start_steps=10000`, `act_noise=0.1`

**Attached Context:**
- #file:FIX_APPLIED_REWARD_SCALING_CATASTROPHE.md - Previous reward scaling attempt
- #file:FINAL_ROOT_CAUSE_INSUFFICIENT_EXPLORATION.md - Exploration budget analysis
- #file:CRITICAL_PHASE_TRANSITION_BUG.md - Phase transition fix
- #file:ROOT_CAUSE_HARD_LEFT_TURN_BIASED_EXPLORATION.md - Biased exploration issue
- #file:Addressing Function Approximation Error in Actor-Critic Methods.tex - TD3 algorithm

---

## Conclusion

The agent is **not broken** - it's **optimally exploiting a flawed reward function**. The stopping penalty creates a **local minimum** where doing nothing is safer than trying to drive. Combined with TD3's pessimistic Q-learning, this results in a rational but degenerate stationary policy.

**The fix is NOT to force the agent to move, but to make moving MORE ATTRACTIVE than stopping.**

**Key Insight:** Reward shaping should make the optimal policy (driving) have **higher expected return** than the degenerate policy (stopping), accounting for both magnitude AND variance.

**Next Steps:** Implement fixes 1-5 above and retrain with:
- Reduced stopping penalty: -0.05 (was -0.50)
- Throttle application reward: +0.1
- Graduated collision penalties: -10/-50/-200 (was -1000)
- Curriculum exploration: gentle 5K ‚Üí full 5K
- Velocity bonus: +0.1 for movement
