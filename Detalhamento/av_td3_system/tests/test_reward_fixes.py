#!/usr/bin/env python3
"""
Unit Tests for Fixed Reward Function

Validates that all 6 fixes are correctly implemented and working as expected.
Run this before starting training to ensure reward function behaves correctly.

Usage:
    python test_reward_fixes.py
"""

import sys
import os
import numpy as np

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from src.environment.reward_functions import RewardCalculator


def test_fix_1_efficiency_forward_velocity():
    """
    Test Critical Fix #1: Simplified Forward Velocity Reward (Priority 1)
    - v=0 should give 0 (not -1.0) ‚úÖ
    - v>0 should give positive reward ‚úÖ
    - Should be continuous and differentiable EVERYWHERE (no discontinuities) ‚úÖ
    - Linear scaling sufficient (no complex tracking) ‚úÖ
    - Based on documentation-backed analysis (Assessment: 8.5/10)
    """
    print("\nüî¥ Testing CRITICAL FIX #1: Simplified Forward Velocity Reward")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)

    # Test 1: v=0 should give 0 (neutral, prevents local optimum)
    efficiency_0 = reward_calc._calculate_efficiency_reward(velocity=0.0, heading_error=0.0)
    assert efficiency_0 == 0.0, f"‚ùå FAIL: v=0 should give 0, got {efficiency_0}"
    print(f"‚úÖ PASS: v=0.0 m/s ‚Üí efficiency = {efficiency_0:.3f} (neutral, not punishing)")

    # Test 2: v=1 m/s should give positive reward
    efficiency_1 = reward_calc._calculate_efficiency_reward(velocity=1.0, heading_error=0.0)
    assert efficiency_1 > 0, f"‚ùå FAIL: v=1 should give positive, got {efficiency_1}"
    print(f"‚úÖ PASS: v=1.0 m/s ‚Üí efficiency = {efficiency_1:.3f} (positive feedback!)")

    # Test 3: v=8.33 m/s (target) should give ~1.0
    efficiency_target = reward_calc._calculate_efficiency_reward(velocity=8.33, heading_error=0.0)
    assert 0.9 < efficiency_target <= 1.0, f"‚ùå FAIL: v=target should give ~1.0, got {efficiency_target}"
    print(f"‚úÖ PASS: v=8.33 m/s ‚Üí efficiency = {efficiency_target:.3f} (optimal)")

    # Test 4: Continuous gradient (0.5 m/s should be between 0 and 1)
    efficiency_05 = reward_calc._calculate_efficiency_reward(velocity=0.5, heading_error=0.0)
    assert 0 < efficiency_05 < efficiency_1, f"‚ùå FAIL: Continuity broken"
    print(f"‚úÖ PASS: v=0.5 m/s ‚Üí efficiency = {efficiency_05:.3f} (continuous gradient)")

    # Test 5: CONTINUITY AT v_forward=0 (no discontinuity from reverse penalty)
    # Small positive forward velocity
    efficiency_pos = reward_calc._calculate_efficiency_reward(velocity=0.01, heading_error=0.0)
    # Small negative forward velocity (going backward)
    efficiency_neg = reward_calc._calculate_efficiency_reward(velocity=0.01, heading_error=np.pi)

    # Should be symmetric (no 2x penalty): |eff_pos| ‚âà |eff_neg|
    ratio = abs(efficiency_neg) / abs(efficiency_pos) if efficiency_pos != 0 else 1.0
    assert 0.9 < ratio < 1.1, f"‚ùå FAIL: Discontinuity at v=0 (ratio={ratio:.2f}, should be ~1.0)"
    print(f"‚úÖ PASS: Continuity at v=0: forward(+{efficiency_pos:.4f}) ‚âà backward({efficiency_neg:.4f})")
    print(f"         Ratio = {ratio:.3f} (should be ~1.0, not 2.0)")

    # Test 6: Linear scaling is sufficient (no bonus/penalty tracking)
    # At 50% speed: should get exactly 0.5 * efficiency (no complex logic)
    efficiency_half = reward_calc._calculate_efficiency_reward(velocity=4.165, heading_error=0.0)
    expected_half = 4.165 / 8.33
    assert abs(efficiency_half - expected_half) < 0.01, f"‚ùå FAIL: Linear scaling broken"
    print(f"‚úÖ PASS: v=4.165 m/s (50%) ‚Üí efficiency = {efficiency_half:.3f} (pure linear)")

    # Test 7: Backward motion naturally penalized by cos(heading_error)
    efficiency_backward = reward_calc._calculate_efficiency_reward(velocity=1.0, heading_error=np.pi)
    assert efficiency_backward < 0, f"‚ùå FAIL: Backward should be negative"
    print(f"‚úÖ PASS: v=1.0 m/s, œÜ=180¬∞ ‚Üí efficiency = {efficiency_backward:.3f} (natural penalty)")

    print("\n‚úÖ CRITICAL FIX #1: VALIDATED (Simplified, KISS principle)")
    print("   Improvements: Continuous everywhere, no complex branches, TD3-compatible")


def test_fix_2_reduced_velocity_gating():
    """
    Test Critical Fix #2: Reduced velocity gating (1.0 ‚Üí 0.1 m/s)
    - v=0.5 should give some lane_keeping reward (not 0)
    - v=1.0 should give more reward than v=0.5
    - Should provide learning gradient during acceleration
    """
    print("\nüî¥ Testing CRITICAL FIX #2: Reduced Velocity Gating")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)

    # Test 1: v=0.05 should still be gated (truly stationary)
    lane_005 = reward_calc._calculate_lane_keeping_reward(0.0, 0.0, 0.05)
    assert lane_005 == 0.0, f"‚ùå FAIL: v=0.05 should be gated, got {lane_005}"
    print(f"‚úÖ PASS: v=0.05 m/s ‚Üí lane_keeping = {lane_005:.3f} (gated, truly stationary)")

    # Test 2: v=0.5 should give SOME reward (not gated!)
    lane_05 = reward_calc._calculate_lane_keeping_reward(0.0, 0.0, 0.5)
    assert lane_05 > 0, f"‚ùå FAIL: v=0.5 should give positive, got {lane_05}"
    print(f"‚úÖ PASS: v=0.5 m/s ‚Üí lane_keeping = {lane_05:.3f} (partial signal, not gated!)")

    # Test 3: v=1.0 should give MORE reward than v=0.5
    lane_10 = reward_calc._calculate_lane_keeping_reward(0.0, 0.0, 1.0)
    assert lane_10 > lane_05, f"‚ùå FAIL: v=1.0 should be > v=0.5"
    print(f"‚úÖ PASS: v=1.0 m/s ‚Üí lane_keeping = {lane_10:.3f} (velocity scaling works)")

    # Test 4: v=3.0 should give full signal
    lane_30 = reward_calc._calculate_lane_keeping_reward(0.0, 0.0, 3.0)
    assert lane_30 > lane_10, f"‚ùå FAIL: v=3.0 should be > v=1.0"
    print(f"‚úÖ PASS: v=3.0 m/s ‚Üí lane_keeping = {lane_30:.3f} (full signal)")

    # Test 5: Same for comfort reward
    reward_calc.prev_acceleration = 0.0
    reward_calc.prev_acceleration_lateral = 0.0
    comfort_05 = reward_calc._calculate_comfort_reward(1.0, 0.0, 0.5)
    assert comfort_05 != 0.0, f"‚ùå FAIL: v=0.5 comfort should not be gated"
    print(f"‚úÖ PASS: v=0.5 m/s ‚Üí comfort = {comfort_05:.3f} (not gated)")

    print("\n‚úÖ CRITICAL FIX #2: VALIDATED")


def test_fix_3_increased_progress_scale():
    """
    Test High Priority Fix #3: Increased progress scale (0.1 ‚Üí 1.0)
    - Moving 1m should give +1.0 progress (not +0.1)
    - Should be strong enough to offset efficiency penalty
    """
    print("\nüü° Testing HIGH PRIORITY FIX #3: Increased Progress Scale")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},  # Should be 1.0
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)

    # Check that distance_scale is 1.0 (not 0.1)
    assert reward_calc.distance_scale == 1.0, f"‚ùå FAIL: distance_scale should be 1.0, got {reward_calc.distance_scale}"
    print(f"‚úÖ PASS: distance_scale = {reward_calc.distance_scale} (10x increase from 0.1)")

    # Simulate moving 1m forward
    reward_calc.prev_distance_to_goal = 50.0
    progress = reward_calc._calculate_progress_reward(
        distance_to_goal=49.0,
        waypoint_reached=False,
        goal_reached=False
    )

    # Should give +1.0 from distance (not +0.1)
    # Plus PBRS component (~+1.49)
    # Total should be around +1.5
    assert progress > 1.0, f"‚ùå FAIL: Moving 1m should give >1.0, got {progress}"
    print(f"‚úÖ PASS: Moving 1m ‚Üí progress = {progress:.3f} (strong signal!)")

    # Weighted progress should be +5.0 (can offset efficiency penalty)
    weighted_progress = config["weights"]["progress"] * progress
    print(f"‚úÖ PASS: Weighted progress = {weighted_progress:.3f} (can offset penalties)")

    print("\n‚úÖ HIGH PRIORITY FIX #3: VALIDATED")


def test_fix_4_reduced_collision_penalty():
    """
    Test High Priority Fix #4: Reduced collision penalty (-1000 ‚Üí -100)
    - collision_penalty should be -100 (not -1000)
    """
    print("\nüü° Testing HIGH PRIORITY FIX #4: Reduced Collision Penalty")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},  # Should be -100
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)

    # Check that collision_penalty is -100 (not -1000)
    assert reward_calc.collision_penalty == -100.0, f"‚ùå FAIL: collision_penalty should be -100, got {reward_calc.collision_penalty}"
    print(f"‚úÖ PASS: collision_penalty = {reward_calc.collision_penalty} (reduced from -1000)")

    # Calculate safety reward with collision
    safety = reward_calc._calculate_safety_reward(
        collision_detected=True,
        offroad_detected=False,
        wrong_way=False,
        velocity=5.0,
        distance_to_goal=50.0
    )

    assert safety == -100.0, f"‚ùå FAIL: Collision should give -100, got {safety}"
    print(f"‚úÖ PASS: Collision penalty = {safety:.1f} (recoverable, not catastrophic)")

    print("\n‚úÖ HIGH PRIORITY FIX #4: VALIDATED")


def test_fix_5_removed_distance_threshold():
    """
    Test Medium Priority Fix #5: Removed distance threshold
    - Stopping penalty should apply even when distance_to_goal < 5m
    """
    print("\nüü¢ Testing MEDIUM PRIORITY FIX #5: Removed Distance Threshold")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)

    # Test 1: Stopping at distance=4m should still be penalized
    safety_4m = reward_calc._calculate_safety_reward(
        collision_detected=False,
        offroad_detected=False,
        wrong_way=False,
        velocity=0.3,  # Essentially stopped
        distance_to_goal=4.0  # Within 5m
    )

    assert safety_4m < 0, f"‚ùå FAIL: Should penalize stopping at 4m, got {safety_4m}"
    print(f"‚úÖ PASS: Stopping at 4m distance ‚Üí safety = {safety_4m:.2f} (penalized!)")

    # Test 2: Progressive penalty (farther = more penalty)
    safety_6m = reward_calc._calculate_safety_reward(
        collision_detected=False,
        offroad_detected=False,
        wrong_way=False,
        velocity=0.3,
        distance_to_goal=6.0
    )

    safety_11m = reward_calc._calculate_safety_reward(
        collision_detected=False,
        offroad_detected=False,
        wrong_way=False,
        velocity=0.3,
        distance_to_goal=11.0
    )

    assert safety_11m < safety_6m < safety_4m < 0, f"‚ùå FAIL: Progressive penalty broken"
    print(f"‚úÖ PASS: Progressive penalty: 11m({safety_11m:.2f}) < 6m({safety_6m:.2f}) < 4m({safety_4m:.2f})")

    print("\n‚úÖ MEDIUM PRIORITY FIX #5: VALIDATED")


def test_fix_6_pbrs_added():
    """
    Test Medium Priority Fix #6: PBRS (Potential-Based Reward Shaping)
    - Moving toward goal should include PBRS component
    - gamma parameter should be added
    """
    print("\nüü¢ Testing MEDIUM PRIORITY FIX #6: PBRS Added")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99  # Should be present
    }

    reward_calc = RewardCalculator(config)

    # Check that gamma is added
    assert hasattr(reward_calc, 'gamma'), "‚ùå FAIL: gamma parameter not found"
    assert reward_calc.gamma == 0.99, f"‚ùå FAIL: gamma should be 0.99, got {reward_calc.gamma}"
    print(f"‚úÖ PASS: gamma = {reward_calc.gamma} (PBRS parameter added)")

    # Calculate progress with PBRS
    reward_calc.prev_distance_to_goal = 50.0
    progress = reward_calc._calculate_progress_reward(
        distance_to_goal=49.0,
        waypoint_reached=False,
        goal_reached=False
    )

    # With PBRS, reward should be higher than just distance_scale
    # distance reward: 1.0
    # PBRS: Œ≥*(-49) - (-50) = -48.51 + 50 = +1.49
    # PBRS weighted: 1.49 * 0.5 = +0.745
    # Total: ~1.745
    assert progress > 1.0, f"‚ùå FAIL: PBRS should boost progress, got {progress}"
    print(f"‚úÖ PASS: Progress with PBRS = {progress:.3f} (includes potential shaping)")

    # Expected PBRS component
    pbrs_expected = 0.99 * (-49.0) - (-50.0)  # Œ≥*Œ¶(s') - Œ¶(s)
    pbrs_weighted = pbrs_expected * 0.5
    print(f"‚úÖ PASS: PBRS component = {pbrs_weighted:.3f} (theoretical guarantee)")

    print("\n‚úÖ MEDIUM PRIORITY FIX #6: VALIDATED")


def test_integrated_scenario():
    """
    Test integrated scenario: Initial acceleration from rest
    Should show positive total reward (OLD would be negative)
    """
    print("\nüéØ Testing INTEGRATED SCENARIO: Initial Acceleration")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)
    reward_calc.prev_distance_to_goal = 50.0

    # Scenario: Agent just started moving (v=0.5 m/s, centered, moved 0.5m)
    reward_dict = reward_calc.calculate(
        velocity=0.5,
        lateral_deviation=0.0,  # Centered
        heading_error=0.0,  # Correct heading
        acceleration=2.0,  # Accelerating
        acceleration_lateral=0.0,
        collision_detected=False,
        offroad_detected=False,
        wrong_way=False,
        distance_to_goal=49.5,
        waypoint_reached=False,
        goal_reached=False
    )

    print(f"\nReward Components:")
    print(f"  Efficiency:    {reward_dict['efficiency']:+.3f} (weighted: {reward_dict['breakdown']['efficiency'][2]:+.3f})")
    print(f"  Lane Keeping:  {reward_dict['lane_keeping']:+.3f} (weighted: {reward_dict['breakdown']['lane_keeping'][2]:+.3f})")
    print(f"  Comfort:       {reward_dict['comfort']:+.3f} (weighted: {reward_dict['breakdown']['comfort'][2]:+.3f})")
    print(f"  Safety:        {reward_dict['safety']:+.3f} (weighted: {reward_dict['breakdown']['safety'][2]:+.3f})")
    print(f"  Progress:      {reward_dict['progress']:+.3f} (weighted: {reward_dict['breakdown']['progress'][2]:+.3f})")
    print(f"  TOTAL:         {reward_dict['total']:+.3f}")

    # OLD implementation would give ~ -0.95
    # NEW implementation should give positive reward
    assert reward_dict['total'] > 0, f"‚ùå FAIL: Total should be positive, got {reward_dict['total']}"
    print(f"\n‚úÖ PASS: Total reward = {reward_dict['total']:.3f} (POSITIVE!)")
    print(f"‚úÖ PASS: OLD would give ~-0.95 (negative), NEW gives positive!")
    print(f"‚úÖ PASS: Agent will learn to MOVE instead of staying at 0 km/h!")

    print("\n‚úÖ INTEGRATED SCENARIO: VALIDATED")


def test_carla_lane_width_normalization():
    """
    Test CARLA lane width normalization enhancement.

    ENHANCEMENT (Priority 3): Use CARLA waypoint.lane_width for dynamic
    normalization instead of fixed config tolerance.

    Expected behavior:
    - Urban roads (2.5m lane ‚Üí 1.25m half): More permissive than config (0.5m)
    - Highway roads (3.5m lane ‚Üí 1.75m half): Even more permissive
    - Reduces false positive lane invasions at 0.5m < |d| < 1.25m
    - Enables multi-map generalization without retraining
    """
    print("\nüéØ Testing CARLA LANE WIDTH NORMALIZATION")
    print("="*60)

    config = {
        "weights": {"efficiency": 1.0, "lane_keeping": 2.0, "comfort": 0.5, "safety": 1.0, "progress": 5.0},
        "efficiency": {"target_speed": 8.33, "speed_tolerance": 1.39},
        "lane_keeping": {"lateral_tolerance": 0.5, "heading_tolerance": 0.1},  # Config fallback
        "comfort": {"jerk_threshold": 3.0},
        "safety": {"collision_penalty": -100.0, "offroad_penalty": -500.0},
        "progress": {"waypoint_bonus": 10.0, "distance_scale": 1.0},
        "gamma": 0.99
    }

    reward_calc = RewardCalculator(config)

    # Test Case 1: Urban road (2.5m lane ‚Üí 1.25m half-width)
    # Vehicle at 0.6m deviation (within lane but would saturate with 0.5m tolerance)
    print("\nüìç Test Case 1: Urban Road (Town01)")
    print(f"   Lane width: 2.5m ‚Üí half_width: 1.25m")
    print(f"   Lateral deviation: 0.6m")
    print(f"   OLD (config 0.5m): deviation saturated (120% of tolerance)")
    print(f"   NEW (CARLA 1.25m): deviation at 48% of tolerance")

    lane_reward_urban = reward_calc._calculate_lane_keeping_reward(
        lateral_deviation=0.6,
        heading_error=0.0,
        velocity=3.0,
        lane_half_width=1.25  # Urban lane from CARLA
    )

    lane_reward_config = reward_calc._calculate_lane_keeping_reward(
        lateral_deviation=0.6,
        heading_error=0.0,
        velocity=3.0,
        lane_half_width=None  # Use config fallback (0.5m)
    )

    print(f"   Reward (CARLA 1.25m): {lane_reward_urban:+.3f}")
    print(f"   Reward (config 0.5m): {lane_reward_config:+.3f}")
    print(f"   Difference: {lane_reward_urban - lane_reward_config:+.3f}")

    # CARLA normalization should give LESS penalty (higher reward)
    assert lane_reward_urban > lane_reward_config, (
        f"‚ùå FAIL: CARLA normalization should be more permissive! "
        f"urban={lane_reward_urban:.3f}, config={lane_reward_config:.3f}"
    )
    print(f"   ‚úÖ PASS: CARLA normalization is more permissive (higher reward)")

    # Test Case 2: Highway road (3.5m lane ‚Üí 1.75m half-width)
    print("\nüìç Test Case 2: Highway Road")
    print(f"   Lane width: 3.5m ‚Üí half_width: 1.75m")
    print(f"   Lateral deviation: 0.6m")
    print(f"   Deviation at only 34% of tolerance")

    lane_reward_highway = reward_calc._calculate_lane_keeping_reward(
        lateral_deviation=0.6,
        heading_error=0.0,
        velocity=3.0,
        lane_half_width=1.75  # Highway lane from CARLA
    )

    print(f"   Reward (CARLA 1.75m): {lane_reward_highway:+.3f}")
    print(f"   Reward (urban 1.25m): {lane_reward_urban:+.3f}")
    print(f"   Reward (config 0.5m): {lane_reward_config:+.3f}")

    # Highway should be most permissive
    assert lane_reward_highway > lane_reward_urban > lane_reward_config, (
        f"‚ùå FAIL: Expected highway > urban > config! "
        f"highway={lane_reward_highway:.3f}, urban={lane_reward_urban:.3f}, config={lane_reward_config:.3f}"
    )
    print(f"   ‚úÖ PASS: Highway most permissive, then urban, then config")

    # Test Case 3: Backward compatibility (None parameter uses config)
    print("\nüìç Test Case 3: Backward Compatibility")
    print(f"   Testing with lane_half_width=None (should use config 0.5m)")

    lane_reward_backward = reward_calc._calculate_lane_keeping_reward(
        lateral_deviation=0.6,
        heading_error=0.0,
        velocity=3.0
        # No lane_half_width parameter ‚Üí should use config
    )

    print(f"   Reward (no parameter): {lane_reward_backward:+.3f}")
    print(f"   Reward (config 0.5m):  {lane_reward_config:+.3f}")

    # Should be identical (backward compatibility)
    assert abs(lane_reward_backward - lane_reward_config) < 1e-6, (
        f"‚ùå FAIL: Backward compatibility broken! "
        f"no_param={lane_reward_backward:.3f}, config={lane_reward_config:.3f}"
    )
    print(f"   ‚úÖ PASS: Backward compatible (uses config when lane_half_width=None)")

    # Test Case 4: False positive mitigation
    print("\nüìç Test Case 4: False Positive Mitigation")
    print(f"   Testing 'false positive' zone: 0.5m < |d| < 1.25m")
    print(f"   OLD: Would consider this near/at lane boundary")
    print(f"   NEW: Recognizes vehicle is still well within lane")

    # Test at 0.8m deviation
    lane_reward_false_positive_old = reward_calc._calculate_lane_keeping_reward(
        lateral_deviation=0.8,
        heading_error=0.0,
        velocity=3.0,
        lane_half_width=None  # Config: 0.5m (would saturate at 160%)
    )

    lane_reward_false_positive_new = reward_calc._calculate_lane_keeping_reward(
        lateral_deviation=0.8,
        heading_error=0.0,
        velocity=3.0,
        lane_half_width=1.25  # CARLA urban: 64% of tolerance
    )

    print(f"   At 0.8m deviation:")
    print(f"   OLD (0.5m tol): {lane_reward_false_positive_old:+.3f} (saturated penalty)")
    print(f"   NEW (1.25m tol): {lane_reward_false_positive_new:+.3f} (moderate penalty)")
    print(f"   Improvement: {lane_reward_false_positive_new - lane_reward_false_positive_old:+.3f}")

    # New should be significantly more permissive
    improvement = lane_reward_false_positive_new - lane_reward_false_positive_old
    assert improvement > 0.10, (
        f"‚ùå FAIL: Expected significant improvement in false positive zone! "
        f"improvement={improvement:.3f}"
    )
    print(f"   ‚úÖ PASS: False positive penalty reduced by {improvement:.3f}")

    # Summary
    print("\nüìä ENHANCEMENT VALIDATION SUMMARY:")
    print(f"   ‚úì Urban lane (1.25m) is {(lane_reward_urban/lane_reward_config - 1)*100:.1f}% more permissive than config")
    print(f"   ‚úì Highway lane (1.75m) is {(lane_reward_highway/lane_reward_config - 1)*100:.1f}% more permissive than config")
    print(f"   ‚úì False positive mitigation improves reward by {improvement:.3f}")
    print(f"   ‚úì Backward compatibility maintained (config fallback works)")
    print(f"   ‚úì Multi-map generalization enabled (no retraining needed)")

    print("\n‚úÖ CARLA LANE WIDTH NORMALIZATION: VALIDATED")


def main():
    """Run all validation tests"""
    print("\n" + "="*60)
    print("REWARD FUNCTION FIX VALIDATION TEST SUITE")
    print("="*60)
    print("\nValidating all 6 fixes + 1 enhancement...")
    print("This will ensure the reward function is ready for training.")

    try:
        test_fix_1_efficiency_forward_velocity()
        test_fix_2_reduced_velocity_gating()
        test_fix_3_increased_progress_scale()
        test_fix_4_reduced_collision_penalty()
        test_fix_5_removed_distance_threshold()
        test_fix_6_pbrs_added()
        test_integrated_scenario()
        test_carla_lane_width_normalization()  # NEW TEST

        print("\n" + "="*60)
        print("üéâ ALL TESTS PASSED!")
        print("="*60)
        print("\n‚úÖ Reward function is correctly fixed and ready for training.")
        print("‚úÖ CARLA lane width normalization reduces false positives.")
        print("‚úÖ Expected outcome: Agent will learn to move (>5 km/h within 5,000 steps)")
        print("‚úÖ Next step: Run short training (1,000 steps) to verify behavior")
        print("\n" + "="*60)

        return 0

    except AssertionError as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        print("\n‚ö†Ô∏è  Please check the implementation and fix the issue.")
        return 1
    except Exception as e:
        print(f"\n‚ùå UNEXPECTED ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
