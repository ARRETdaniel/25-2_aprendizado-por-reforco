# Evaluation Environment Fixes - Complete Analysis

**Date**: October 26, 2025  
**Issue**: Training crashed at step 6,001 during environment reset after evaluation  
**Root Cause**: Two critical bugs in episode timeout logic

---

## üîç Problem Summary

### Observed Behavior
```
[EXPLORATION] Step 6000/30,000 | Episode 0 | Ep Step 6000 | Reward= -53.00 | Speed=  0.0 km/h
[EVAL] Evaluation at timestep 6,000...
[EVAL] Mean Reward: -50138.86 | Success Rate: 0.0% | Avg Collisions: 0.00 | Avg Length: 4
[TRAIN] Episode 0 | Timestep 6001 | Reward -315633.18 | Collisions 0
ERROR: failed to destroy actor 197/198: unable to destroy actor: not found
RuntimeError: close: Bad file descriptor [system:9]
```

### Key Issues
1. **Ultra-long episode**: Episode 0 lasted 6,001+ steps (should timeout at 1,000)
2. **Vehicle stationary**: Speed = 0.0 km/h entire episode (Reward = -53.00 standing still penalty)
3. **Crash during cleanup**: Sensor destruction failed after ultra-long episode
4. **Evaluation variance**: Eval episode lengths: 88, 4, 4 steps (very inconsistent)

---

## üêõ Root Cause Analysis

### Bug #1: Step Counter Incremented AFTER Timeout Check

**Location**: `src/environment/carla_env.py` lines 549-558

**Original Code**:
```python
# Check termination conditions
done, termination_reason = self._check_termination(vehicle_state)

# Gymnasium API: split done into terminated and truncated
truncated = (self.current_step >= self.max_episode_steps) and not done
terminated = done and not truncated

# Update step counter ‚Üê BUG: Counter incremented AFTER check!
self.current_step += 1
```

**The Problem**:
- At step 6000: `current_step = 5999` (not yet incremented)
- Timeout check: `5999 >= 6000` ‚Üí **FALSE**, episode continues
- Counter increments: `current_step = 6000`
- Next iteration (step 6001): check `6000 >= 6000` ‚Üí TRUE, but **one step too late!**

**Result**: Episodes run **1 step beyond** the configured limit, causing:
- Sensor desynchronization (actors become stale)
- Ultra-long episodes (6,001+ steps)
- CARLA crashes during cleanup

---

### Bug #2: Wrong Config Key for Max Steps

**Location**: `src/environment/carla_env.py` lines 198-203

**Original Code**:
```python
self.max_episode_steps = (
    self.carla_config.get("episode", {}).get("max_duration_seconds", 300) * 20
)  # Convert to steps @ 20 Hz
```

**The Problem**:
- Code reads `episode.max_duration_seconds` (doesn't exist in config)
- Defaults to 300 seconds * 20 Hz = **6,000 steps** (way too long!)
- Config actually has `episode.max_time_steps: 5000` (but code ignores it)

**Config File** (`config/carla_config.yaml`):
```yaml
episode:
  max_time_steps: 5000  # ‚Üê Code doesn't read this!
```

**Result**: Episodes allowed to run 6,000 steps instead of intended 1,000 steps

---

## ‚úÖ Applied Fixes

### Fix #1: Increment Step Counter BEFORE Termination Check

**File**: `src/environment/carla_env.py`

```python
# üîß FIX: Increment step counter BEFORE checking termination
# This ensures timeout check uses correct step count
self.current_step += 1

# Check termination conditions
done, termination_reason = self._check_termination(vehicle_state)

# Gymnasium API: split done into terminated and truncated
# terminated: episode ended naturally (collision, goal, off-road)
# truncated: episode ended due to time/step limit
truncated = (self.current_step >= self.max_episode_steps) and not done
terminated = done and not truncated
```

**Impact**:
- ‚úÖ Timeout check now uses correct step count
- ‚úÖ Episodes terminate exactly at `max_episode_steps`
- ‚úÖ No more off-by-one errors

---

### Fix #2: Read Correct Config Key

**File**: `src/environment/carla_env.py`

```python
# Episode state
self.current_step = 0
self.episode_start_time = None
# üîß FIX: Read max_time_steps from config (not max_duration_seconds)
# Config has episode.max_time_steps (5000) directly in steps
self.max_episode_steps = self.carla_config.get("episode", {}).get("max_time_steps", 1000)
```

**Impact**:
- ‚úÖ Now reads `episode.max_time_steps` from config
- ‚úÖ Default fallback to 1,000 steps (safe value)
- ‚úÖ Respects config file settings

---

### Fix #3: Reduce Max Episode Steps for Training

**File**: `config/carla_config.yaml`

```yaml
# ============================================================================
# EPISODE SETTINGS
# ============================================================================
episode:
  # üîß FIX: Reduced from 5000 to 1000 steps for faster learning cycles
  # At 20 Hz: 1000 steps = 50 seconds per episode (reasonable for training)
  # Original 5000 steps = 250 seconds was causing ultra-long episodes
  max_time_steps: 1000  # Maximum steps per episode (~50 seconds at 20 Hz)
  max_distance: 1000.0  # Maximum distance (meters)
```

**Rationale**:
- TD3 paper uses episodes of ~100-200 steps typically
- 1,000 steps (50 seconds) is reasonable for autonomous driving
- Faster episode turnover ‚Üí more diverse training data
- Prevents ultra-long stationary episodes

**Impact**:
- ‚úÖ Episodes now timeout at 1,000 steps (50 seconds)
- ‚úÖ Faster learning cycles (10 episodes in 500 seconds vs 5,000 seconds)
- ‚úÖ Prevents sensor desynchronization

---

## üìä Expected Results After Fixes

### Before Fixes
```
[EXPLORATION] Step 6000 | Episode 0 | Ep Step 6000 | Reward= -53.00 | Speed= 0.0 km/h
[TRAIN] Episode 0 | Timestep 6001 | Reward -315633.18 | Collisions 0
ERROR: failed to destroy actor 197/198
```

### After Fixes
```
[EXPLORATION] Step 100 | Episode 0 | Ep Step 100 | Reward= -53.00 | Speed= 0.0 km/h
...
[EXPLORATION] Step 1000 | Episode 0 | Ep Step 1000 | Reward= -53000.00
[TRAIN] Episode 0 | Timestep 1000 | Reward -53000.00 | Collisions 0
[EXPLORATION] Step 1001 | Episode 1 | Ep Step 1 | ...  ‚Üê NEW EPISODE!
```

**Expected Behavior**:
- ‚úÖ Episodes terminate at 1,000 steps
- ‚úÖ No crashes during sensor cleanup
- ‚úÖ Multiple episodes complete successfully
- ‚úÖ Evaluation runs without issues

---

## üéØ Evaluation Function - Keep or Remove?

### Answer: **DEFINITELY KEEP!**

Based on analysis of TD3 paper and CARLA implementations:

#### Evidence from TD3 Original Paper (Fujimoto et al., 2018)

> "Each task is run for 1 million time steps with **evaluations every 5000 time steps**, where each evaluation reports the **average reward over 10 episodes with no exploration noise**."

**Key principles**:
1. Evaluate **every 5,000 steps** (we do 2,000 - even better for debugging)
2. Use **separate environment** with different seed (**we do this correctly**)
3. Run **deterministic policy** (noise=0.0) (**we do this correctly**)
4. Average over 5-10 episodes (**we do 5 - correct**)

#### Evidence from Official TD3 Implementation

```python
# From TD3/main.py
parser.add_argument("--eval_freq", default=5e3, type=int)  # Every 5k steps

# Evaluation function
def eval_policy(policy, env_name, seed, eval_episodes=10):
    eval_env = gym.make(env_name)  # ‚Üê SEPARATE ENVIRONMENT!
    eval_env.seed(seed + 100)       # ‚Üê DIFFERENT SEED!
    
    avg_reward = 0.
    for _ in range(eval_episodes):
        state, done = eval_env.reset(), False
        while not done:
            action = policy.select_action(np.array(state))  # ‚Üê NO NOISE!
            state, reward, done, _ = eval_env.step(action)
            avg_reward += reward
    
    return avg_reward / eval_episodes
```

#### Why Evaluation is Critical

1. **Training vs. Evaluation Performance Gap**
   - Training uses exploration noise ‚Üí actions randomized ‚Üí artificially worse performance
   - Evaluation uses deterministic policy ‚Üí true policy capability

2. **Monitoring Convergence**
   - Can't tell if policy improving without eval
   - Can't detect overfitting or divergence
   - Can't compare to baselines

3. **Scientific Reproducibility**
   - Evaluation metrics are the ONLY fair comparison method
   - Required for publishing results

4. **Early Detection of Issues**
   - You discovered evaluation problems at 2k, 4k, 6k steps
   - Without eval, would have wasted entire 30k training run!

---

## üìù Remaining Work

### ‚úÖ Fixed Issues
- [x] Episode timeout now enforced correctly
- [x] Step counter incremented at correct time
- [x] Config key mismatch resolved
- [x] Max steps reduced to reasonable value (1,000)

### ‚è≥ Still Need Investigation
- [ ] **Waypoint progression**: Only 5 positive rewards in 7,010 steps
- [ ] **Vehicle standing still**: Reward = -53.00 entire episode
- [ ] **Evaluation episode variance**: Lengths of 88, 4, 4 steps (why?)

### üîç Spawn Point Consistency (VERIFIED CORRECT)

The environment **ALWAYS uses the same spawn point**:
```python
# From carla_env.py reset() method
route_start = self.waypoint_manager.waypoints[0]  # Always first waypoint!
spawn_point = carla.Transform(
    carla.Location(x=route_start[0], y=route_start[1], z=spawn_z),
    carla.Rotation(pitch=0.0, yaw=initial_yaw, roll=0.0)
)
```

**Verification**:
- ‚úÖ Both training and evaluation use same config files
- ‚úÖ Both use `waypoint_manager.waypoints[0]` as spawn
- ‚úÖ No randomization in spawn point selection
- ‚úÖ Route is deterministic (from `waypoints.txt`)

**The evaluation episode variance (88, 4, 4) is likely due to**:
- Vehicle not moving (standing still)
- Random actions during exploration causing quick terminations
- NOT due to different spawn points (spawn is consistent)

---

## üöÄ Next Steps

### 1. Restart Training (Immediate)
```bash
cd /media/danielterra/Windows-SSD/Users/danie/Documents/Documents/MESTRADO/25-2_aprendizado-por-reforco/Detalhamento/av_td3_system
bash scripts/run_validation_training.sh
```

**Expected Results**:
- Episodes should terminate at 1,000 steps
- No crashes during sensor cleanup
- Multiple episodes complete successfully
- Evaluation runs every 2,000 steps without issues

### 2. Monitor First Hour (Critical)
Watch for:
- [ ] Episodes terminate at ~1,000 steps (not 6,000+)
- [ ] Multiple episodes complete (Episode 1, 2, 3... not stuck on Episode 0)
- [ ] Vehicle moves (Speed > 0 km/h, even if small)
- [ ] Evaluations complete without crashes (at 2k, 4k, 6k, 8k, 10k steps)
- [ ] Checkpoint saves successfully at 5,000 steps

### 3. Morning Analysis (After Overnight Run)
If training completes 30k steps successfully:
```bash
# Check training completion
tail -100 data/logs/validation_training_30k_*.log

# Count positive rewards
grep "Rew= +" data/logs/validation_training_30k_*.log | wc -l

# Analyze reward distribution
python3 scripts/analyze_validation_run.py \
  --log-file data/logs/validation_training_30k_*.log \
  --output-dir data/validation_analysis
```

### 4. Address Waypoint Issue (If Still Present)
If vehicle still standing still after fixes:
- Run waypoint diagnostic: `python3 scripts/test_waypoint_system.py`
- Check reward function weights
- Verify action selection during exploration
- Consider increasing efficiency reward weight

---

## üìñ References

1. **TD3 Original Paper**: Fujimoto et al., 2018 - "Addressing Function Approximation Error in Actor-Critic Methods"
2. **TD3 Official Implementation**: https://github.com/sfujim/TD3
3. **CARLA Documentation**: https://carla.readthedocs.io/en/latest/
4. **TD3 CARLA Paper**: 2023 - "Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation"
5. **RECPO Highway Paper**: 2024 - "Towards Robust Decision-Making for Autonomous Highway Driving Based on Safe Reinforcement Learning"

---

## ‚ú® Summary

**Two critical bugs fixed**:
1. ‚úÖ Step counter incremented at wrong time ‚Üí Episodes ran 1 step too long
2. ‚úÖ Wrong config key ‚Üí Timeout set to 6,000 steps instead of 1,000

**Evaluation function is CORRECT and ESSENTIAL** - keep it!

**Expected outcome**: Training should now complete 30k steps without crashes, with proper episode timeouts and stable evaluation.

**User can now safely run overnight training** üåô
