# Training Configuration
# Global training settings for TD3/DDPG agents in CARLA
# IEEE Paper: "End-to-End Visual Autonomous Navigation with Twin Delayed DDPG"

# Training scenarios (paper requirement: 3 traffic densities)
scenarios:
  # Town01 with different NPC vehicle densities
  - name: 'town01_light_traffic'
    map: 'Town01'
    num_vehicles: 20  # Light traffic
    num_pedestrians: 0
    weather: 'ClearNoon'
    max_episodes: 2000  # Paper requirement
    
  - name: 'town01_medium_traffic'
    map: 'Town01'
    num_vehicles: 50  # Medium traffic
    num_pedestrians: 0
    weather: 'ClearNoon'
    max_episodes: 2000  # Paper requirement
    
  - name: 'town01_heavy_traffic'
    map: 'Town01'
    num_vehicles: 100  # Heavy traffic
    num_pedestrians: 0
    weather: 'ClearNoon'
    max_episodes: 2000  # Paper requirement

# Episode settings
episode:
  max_steps: 1000  # Maximum timesteps per episode
  max_duration: 300.0  # Maximum duration in seconds (5 minutes)
  max_distance: 1000.0  # Maximum distance in meters
  
  # Success criteria
  success:
    reach_goal: true  # Must reach destination
    no_collision: true  # Must not collide
    stay_on_road: true  # Must stay on drivable area
    min_distance_ratio: 0.8  # Must cover at least 80% of route
    
  # Termination conditions
  termination:
    on_collision: true  # End episode on collision
    on_offroad: true  # End episode when off-road for too long
    offroad_tolerance_time: 3.0  # Seconds allowed off-road
    wrong_way_tolerance_time: 5.0  # Seconds allowed in wrong direction

# Training schedule
training:
  # Total training budget per scenario
  total_timesteps: 2000000  # ~2000 episodes × 1000 steps
  
  # Curriculum learning (optional, start with easier scenarios)
  curriculum:
    enabled: false  # Disable for now (train on target scenario directly)
    start_scenario: 'town01_light_traffic'
    progression_criteria:
      success_rate_threshold: 0.7  # Move to next scenario at 70% success
      min_episodes: 500  # Minimum episodes before progression
      
  # Training phases
  phases:
    # Phase 1: Warm-up with random exploration
    - name: 'random_exploration'
      timesteps: 25000  # Match algorithm learning_starts
      exploration: 'uniform_random'
      update_networks: false
      
    # Phase 2: Active learning with exploration noise
    - name: 'active_learning'
      timesteps: 1975000  # Remaining timesteps
      exploration: 'gaussian_noise'
      update_networks: true
      
  # Checkpoint management
  checkpointing:
    save_freq: 10000  # Save every 10k timesteps
    keep_best: 5  # Keep top 5 checkpoints (by evaluation reward)
    keep_freq: 50000  # Always keep checkpoints every 50k timesteps
    save_final: true  # Save final checkpoint
    save_replay_buffer: true  # Save replay buffer (large files!)
    checkpoint_dir: './data/checkpoints/'
    
  # Evaluation during training
  evaluation:
    enabled: true
    eval_freq: 5000  # Evaluate every 5k timesteps
    n_eval_episodes: 10  # Average over 10 episodes
    deterministic: true  # No exploration noise during evaluation
    record_video: false  # Don't record during training eval (saves disk)
    log_metrics: true  # Log evaluation metrics to TensorBoard/WandB
    
  # Early stopping (optional)
  early_stopping:
    enabled: false  # Disable for paper experiments (full training)
    patience: 10  # Stop if no improvement for N evaluations
    min_delta: 0.01  # Minimum improvement threshold
    metric: 'success_rate'  # Metric to monitor
    
# Logging configuration
logging:
  # Console output
  console:
    verbose: 1  # 0: no output, 1: info, 2: debug
    log_freq: 100  # Print training stats every N episodes
    
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: './data/logs/tensorboard/'
    log_graph: false  # Log network architecture (slow)
    log_interval: 100  # Log every N timesteps
    
  # Weights & Biases
  wandb:
    enabled: true
    project: 'td3-av-carla'
    entity: null  # Set via environment variable WANDB_ENTITY
    group: null  # Group related runs (e.g., 'td3_experiments')
    tags: []  # Additional tags (added to agent-specific tags)
    notes: ''  # Additional notes
    save_code: true  # Upload code to WandB
    monitor_gym: true  # Log videos and metrics from gym environment
    log_interval: 100  # Log every N timesteps
    
  # File logging
  file:
    enabled: true
    log_dir: './data/logs/training/'
    log_level: 'INFO'  # DEBUG, INFO, WARNING, ERROR
    
  # Metrics to track
  metrics:
    training:
      - 'episode_reward'  # Total reward per episode
      - 'episode_length'  # Timesteps per episode
      - 'actor_loss'  # Actor network loss
      - 'critic_loss'  # Critic network loss
      - 'q_value_mean'  # Mean Q-value
      - 'collision_rate'  # Collisions per episode
      - 'success_rate'  # Success rate (rolling average)
      
    evaluation:
      - 'mean_reward'  # Mean evaluation reward
      - 'std_reward'  # Std dev of evaluation reward
      - 'success_rate'  # Percentage of successful episodes
      - 'mean_episode_length'  # Mean episode duration
      - 'collision_count'  # Total collisions
      - 'offroad_count'  # Total off-road events

# Evaluation configuration (post-training testing)
evaluation:
  # Test scenarios (same as training + edge cases)
  scenarios:
    # Standard scenarios (paper requirement)
    - name: 'test_light_traffic'
      map: 'Town01'
      num_vehicles: 20
      num_pedestrians: 0
      weather: 'ClearNoon'
      n_episodes: 20  # Paper requirement (minimum)
      
    - name: 'test_medium_traffic'
      map: 'Town01'
      num_vehicles: 50
      num_pedestrians: 0
      weather: 'ClearNoon'
      n_episodes: 20
      
    - name: 'test_heavy_traffic'
      map: 'Town01'
      num_vehicles: 100
      num_pedestrians: 0
      weather: 'ClearNoon'
      n_episodes: 20
      
    # Robustness tests (optional, not in paper)
    - name: 'test_rain'
      map: 'Town01'
      num_vehicles: 50
      num_pedestrians: 0
      weather: 'WetCloudyNoon'
      n_episodes: 10
      
  # Evaluation settings
  deterministic: true  # Always use deterministic policy
  render: false  # Headless evaluation (faster)
  record_video: true  # Record episodes for analysis
  video_fps: 20  # Frames per second for video
  video_dir: './data/videos/evaluation/'
  
  # Metrics to compute (paper requirements)
  metrics:
    # Safety metrics (PRIMARY)
    safety:
      - 'success_rate'  # Percentage of collision-free episodes (%)
      - 'collision_count'  # Total collisions per episode
      - 'collisions_per_km'  # Collisions per kilometer driven
      - 'offroad_count'  # Number of off-road events
      - 'min_ttc'  # Minimum time-to-collision (seconds)
      - 'ttc_violations'  # Number of times TTC < 2.0 seconds
      
    # Efficiency metrics
    efficiency:
      - 'average_speed'  # Mean speed during episode (km/h)
      - 'completion_time'  # Time to complete route (seconds)
      - 'route_completion_ratio'  # Percentage of route completed
      
    # Comfort metrics
    comfort:
      - 'avg_longitudinal_jerk'  # Mean abs longitudinal jerk (m/s³)
      - 'max_longitudinal_jerk'  # Max abs longitudinal jerk (m/s³)
      - 'avg_lateral_acceleration'  # Mean abs lateral acceleration (m/s²)
      - 'max_lateral_acceleration'  # Max abs lateral acceleration (m/s²)
      
    # Navigation metrics
    navigation:
      - 'avg_lateral_error'  # Mean distance from lane center (m)
      - 'avg_heading_error'  # Mean heading error (degrees)
      - 'lane_departure_count'  # Number of lane departures
      
  # Statistical analysis
  statistics:
    compute_confidence_intervals: true
    confidence_level: 0.95  # 95% confidence intervals
    min_episodes: 20  # Minimum episodes for statistical validity
    
  # Results export
  export:
    csv: true  # Export results to CSV
    json: true  # Export results to JSON
    latex_table: true  # Generate LaTeX table for paper
    output_dir: './data/results/'

# Multi-agent comparison setup
comparison:
  agents:
    - name: 'TD3'
      config: './config/td3_config.yaml'
      checkpoint: null  # Path to trained checkpoint (set at runtime)
      
    - name: 'DDPG'
      config: './config/ddpg_config.yaml'
      checkpoint: null  # Path to trained checkpoint (set at runtime)
      
    # Classical baseline (optional, implement separately)
    # - name: 'IDM_MOBIL'
    #   type: 'classical'
    #   config: './config/idm_mobil_config.yaml'
      
  # Statistical comparison
  statistical_tests:
    - 'mann_whitney_u'  # Non-parametric test (does not assume normality)
    - 'wilcoxon_signed_rank'  # Paired test for matched scenarios
    - 'anova'  # Analysis of variance (if multiple agents)
    
  # Visualization
  visualization:
    generate_plots: true
    plot_types:
      - 'learning_curves'  # Training reward over time
      - 'success_rate_comparison'  # Bar chart of success rates
      - 'metric_distributions'  # Box plots of metrics
      - 'radar_charts'  # Multi-metric comparison
    plot_dir: './data/plots/'
    save_formats: ['png', 'pdf']  # Save in multiple formats

# Resource management
resources:
  # GPU settings
  gpu:
    enabled: true
    device_id: 0  # CUDA device ID
    memory_fraction: 0.8  # Fraction of GPU memory to use
    allow_growth: true  # Allow dynamic memory growth
    
  # CPU settings
  cpu:
    n_workers: 4  # Number of parallel environments (if vectorized)
    n_threads: 8  # Number of threads for PyTorch
    
  # Memory management
  memory:
    replay_buffer_on_gpu: false  # Keep replay buffer on CPU (saves GPU RAM)
    pin_memory: true  # Pin CPU memory for faster GPU transfer

# Reproducibility
reproducibility:
  seed: 42  # Global random seed
  deterministic_pytorch: true  # Deterministic PyTorch operations
  deterministic_carla: true  # Deterministic CARLA simulation
  set_all_seeds: true  # Set seeds for all libraries (numpy, random, etc.)

# Experiment tracking
experiment:
  name: 'td3_vs_ddpg_visual_navigation'
  description: |
    End-to-end visual autonomous navigation with TD3 and DDPG baseline.
    Training on CARLA Town01 with 3 traffic density levels (20, 50, 100 NPCs).
    Paper: "End-to-End Visual Autonomous Navigation with Twin Delayed DDPG"
  version: '1.0.0'
  author: 'Daniel Terra Gomes'
  institution: 'Federal University of Minas Gerais (UFMG)'
  advisor: 'Luiz Chaimowicz'
  date: '2025-10-20'
  
  # Experiment organization
  tags:
    - 'deep-reinforcement-learning'
    - 'autonomous-vehicles'
    - 'td3'
    - 'ddpg'
    - 'carla'
    - 'visual-navigation'
    - 'end-to-end'
    - 'ieee-paper'
    
  # Output organization
  output_structure:
    base_dir: './data/'
    subdirs:
      checkpoints: 'checkpoints/{agent}/{scenario}/'
      logs: 'logs/{agent}/{scenario}/'
      videos: 'videos/{agent}/{scenario}/'
      results: 'results/{agent}/{scenario}/'
      plots: 'plots/'
