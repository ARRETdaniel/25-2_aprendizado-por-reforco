# TD3 Algorithm Configuration
# Based on Fujimoto et al. 2018 (https://arxiv.org/abs/1802.09477)
# Implementation follows official TD3 repository: https://github.com/sfujim/TD3
# Stable-Baselines3 documentation: https://stable-baselines3.readthedocs.io/en/master/modules/td3.html

algorithm:
  name: 'TD3'

  # Core TD3 hyperparameters (from official implementation)
  learning_rate: 0.0003  # 3e-4, Adam optimizer for both actor and critics
  discount: 0.99  # Gamma (γ), discount factor for future rewards
  tau: 0.005  # Polyak averaging coefficient (ρ) for soft target network updates

  # TD3-specific mechanisms
  policy_noise: 0.2  # Sigma (σ) for target policy smoothing (scaled by max_action)
  noise_clip: 0.5  # Clip value (c) for target policy noise (scaled by max_action)
  policy_freq: 2  # Delayed policy updates - update actor every 2 critic updates

  # Training parameters
  batch_size: 256  # Mini-batch size sampled from replay buffer
  buffer_size: 97000  # Replay buffer capacity (~22GB RAM, 1M for full training)
  # Memory calculation: ~226KB per transition (state+action+next_state+reward+done)
  # 22GB / 226KB ≈ 97,345 transitions. Using 97,000 for ~22GB usage.

  # MODIFIED FOR 10K DEBUG RUN: Reduced from 25,000 to 5,000
  # Rationale: TD3 exploration phase (OpenAI Spinning Up, Fujimoto et al. 2018)
  #   - Steps 1-5,000: Random actions to fill replay buffer (exploration)
  #   - Steps 5,001-10,000: Policy-based actions with TD3 training (learning)
  # This allows us to observe BOTH phases in a 10K-step debug run:
  #   - Exploration phase behavior (no CNN/policy calls, random actions)
  #   - Learning phase behavior (CNN forward pass, TD3 training, full logs)
  # Original: 25,000 (OpenAI Spinning Up default: 10,000)
  learning_starts: 5000  # REDUCED: Start learning after 5K steps for 10K debug window

  train_freq: 1  # Update networks every step
  gradient_steps: -1  # -1 means as many gradient steps as environment steps

  # Exploration noise (added to actor output during training)
  # INCREASED from 0.1 to 0.2 based on literature recommendations:
  # - Fujimoto et al. (2018): Original TD3 uses 0.1 for MuJoCo
  # - Elallid et al. (2023): TD3-CARLA intersection, complex scenarios benefit from higher exploration
  # - Chen et al., Mnih et al.: Early training needs more exploration to escape local minima
  # Rationale: After learning regression at step 11,600+, agent got stuck in "don't move" local minimum.
  # Higher exploration (0.2) encourages continued movement exploration even after initial success.
  exploration_noise: 0.1  # Std of Gaussian noise (scaled by max_action) - INCREASED for stability

# Network architecture (matching official TD3 implementation)
networks:
  # CNN Feature Extractor (visual input processing)
  cnn:
    learning_rate: 0.0001  # Conservative 1e-4 for CNN (lower than actor/critic 3e-4)
    # Rationale: Visual features require more stable learning than policy/value
    # Lower LR prevents catastrophic forgetting of learned visual representations

  # Actor network (deterministic policy μ_φ(s))
  actor:
    hidden_layers: [256, 256]  # Two hidden layers with 256 neurons each
    activation: 'relu'  # ReLU activation for hidden layers
    output_activation: 'tanh'  # Tanh for bounded action output [-1, 1]
    learning_rate: 0.0003  # Can override algorithm learning_rate if needed

  # Twin Critic networks (Q_θ1(s,a) and Q_θ2(s,a))
  critic:
    hidden_layers: [256, 256]  # Two hidden layers with 256 neurons each
    activation: 'relu'  # ReLU activation
    n_critics: 2  # Twin critics for clipped double Q-learning
    learning_rate: 0.0003  # Can override algorithm learning_rate if needed

  # CNN Feature Extractor (for visual input processing)
  cnn_extractor:
    # Architecture type: 'nature' (NatureCNN), 'resnet18', 'mobilenet_v2'
    architecture: 'nature'  # NatureCNN from DQN paper (default in SB3)

    # Input preprocessing
    input_channels: 4  # Stack of 4 consecutive grayscale frames
    input_height: 84  # Resized height (from 256→84 or 144→84)
    input_width: 84  # Resized width (from 256→84 or 144→84)

    # Transfer learning options
    use_pretrained: false  # Use ImageNet pre-trained weights
    freeze_features: false  # Freeze feature extractor during training

    # Feature dimension output (after CNN, before concatenation with vector obs)
    features_dim: 512  # Output dimension from CNN (NatureCNN outputs 512)

# State space configuration
state:
  # Visual input (front camera)
  image:
    enabled: true
    stack_frames: 4  # Number of consecutive frames to stack (temporal information)
    grayscale: true  # Convert RGB to grayscale
    normalize: true  # Normalize pixel values to [0, 1]
    resize: [84, 84]  # Resize to 84x84 (smaller for faster training)

  # Kinematic features (vehicle state)
  kinematic:
    enabled: true
    features:
      - 'velocity'  # Current vehicle velocity (m/s)
      - 'lateral_deviation'  # Distance from lane center (m)
      - 'heading_error'  # Heading error w.r.t. lane direction (radians)
    normalize: true  # Normalize kinematic features

  # Goal-oriented features (waypoints)
  waypoints:
    enabled: true
    num_waypoints: 10  # Number of future waypoints to include in state
    lookahead_distance: 50.0  # Maximum distance to look ahead (meters)
    relative_coords: true  # Use vehicle-local coordinate frame
    normalize: true  # Normalize waypoint coordinates

# Action space configuration
action:
  type: 'continuous'  # Continuous action space
  dim: 2  # [steering, throttle/brake]

  # Action bounds
  steering:
    min: -1.0  # Full left
    max: 1.0   # Full right

  throttle_brake:
    min: -1.0  # Full brake (negative values)
    max: 1.0   # Full throttle (positive values)

  # Action scaling (if needed for CARLA compatibility)
  scale_steering: 1.0  # Multiply steering output by this factor
  scale_throttle_brake: 1.0  # Multiply throttle/brake output by this factor

# Reward function configuration
reward:
  # Component weights (tuned for safety-first autonomous driving)
  # REWARD REBALANCING (Based on previous 30K failure analysis):
  # Previous issue: Progress component dominated 88-99% of total reward magnitude
  # Root cause: progress weight (5.0) was too high relative to other components
  # Solution: Reduced progress from 5.0 to 1.0 for balanced contributions
  # Target distribution: Each component contributes 10-30% of total reward
  # References:
  #   - TD3 paper (Fujimoto et al. 2018): Balanced reward components critical
  #   - Contextual papers (Elallid et al., Chen et al.): Multi-component rewards
  weights:
    efficiency: 1.0  # Reward for maintaining target speed
    lane_keeping: 2.0  # Reward for staying centered in lane (only when moving!)
    comfort: 0.5  # Penalty for high jerk / Reward for smooth driving (only when moving!)
    safety: 1.0  # FIXED: Changed from -100.0. Penalties are already negative values.
    progress: 1.0  # REDUCED from 5.0: Reward for forward progress (prevent domination)

  # Efficiency reward parameters
  efficiency:
    target_speed: 10.0  # Target speed in m/s (~36 km/h, urban speed)
    speed_tolerance: 2.0  # Tolerance around target speed (m/s)
    overspeed_penalty_scale: 2.0  # Amplify penalty for exceeding speed limit

  # Lane keeping reward parameters
  lane_keeping:
    lateral_tolerance: 0.5  # Acceptable lateral deviation (meters)
    heading_tolerance: 0.1  # Acceptable heading error (radians, ~5.7 degrees)

  # Comfort penalty parameters
  comfort:
    jerk_threshold: 3.0  # Jerk magnitude threshold for penalty (m/s³)

  # Safety penalty parameters
  safety:
    collision_penalty: -1000.0  # Terminal penalty for collision
    offroad_penalty: -500.0  # Terminal penalty for going off-road
    wrong_way_penalty: -200.0  # Penalty for driving in wrong direction

  # Progress reward parameters (NEW: Goal-directed navigation)
  progress:
    waypoint_bonus: 10.0  # Bonus reward for reaching each waypoint milestone
    distance_scale: 0.1  # Scale for distance reduction reward (reward = delta_distance * scale)
    goal_reached_bonus: 100.0  # Large bonus for completing the entire route

# Training configuration
training:
  # Episode settings
  max_timesteps: 2000000  # Total training timesteps (~2000 episodes × 1000 steps)
  max_episode_steps: 1000  # Maximum steps per episode (CARLA default)

  # Checkpointing
  save_freq: 10000  # Save model every N timesteps
  checkpoint_dir: './data/checkpoints/td3/'
  save_replay_buffer: true  # Save replay buffer with checkpoint

  # Evaluation during training
  eval_freq: 5000  # Evaluate every N timesteps
  n_eval_episodes: 10  # Number of episodes for evaluation
  eval_deterministic: true  # Use deterministic policy (no exploration noise)

  # Logging
  log_dir: './data/logs/td3/'
  tensorboard: true  # Enable TensorBoard logging
  wandb:
    enabled: true  # Enable Weights & Biases logging
    project: 'td3-av-carla'
    entity: null  # WandB username/team (set via env var or here)
    tags: ['TD3', 'CARLA', 'Town01', 'visual-navigation']
    notes: 'TD3 agent for end-to-end visual autonomous navigation'

  # Verbosity
  verbose: 1  # 0: no output, 1: info, 2: debug

# Evaluation configuration (post-training)
evaluation:
  n_episodes: 20  # Number of episodes per scenario
  deterministic: true  # Always use deterministic policy
  render: false  # Render simulation (slow, use for visualization only)
  record_video: true  # Record video of evaluation episodes
  video_dir: './data/videos/td3/'

  # Metrics to compute
  metrics:
    - 'success_rate'  # Percentage of collision-free episodes
    - 'average_speed'  # Average speed during episode (km/h)
    - 'completion_time'  # Time to complete route (seconds)
    - 'collision_count'  # Number of collisions
    - 'offroad_count'  # Number of off-road events
    - 'avg_lateral_error'  # Average distance from lane center (m)
    - 'avg_heading_error'  # Average heading error (degrees)
    - 'avg_jerk'  # Average longitudinal jerk (m/s³)
    - 'avg_lateral_acceleration'  # Average lateral acceleration (m/s²)
    - 'min_ttc'  # Minimum time-to-collision (seconds)

# Device configuration
device:
  type: 'auto'  # 'auto', 'cuda', 'cpu'
  cuda_device_id: 0  # GPU device ID if using CUDA

# Reproducibility
seed: 42  # Random seed for reproducibility (null for random seed)
deterministic_pytorch: true  # Use deterministic PyTorch operations (slower but reproducible)

# Experiment metadata
experiment:
  name: 'td3_visual_navigation_baseline'
  description: 'TD3 agent for end-to-end visual autonomous navigation in CARLA Town01'
  version: '1.0.0'
  author: 'Daniel Terra Gomes'
  paper: 'End-to-End Visual Autonomous Navigation with Twin Delayed DDPG'
