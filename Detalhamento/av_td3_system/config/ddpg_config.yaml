# DDPG Baseline Configuration
# Based on Lillicrap et al. 2015 (https://arxiv.org/abs/1509.02971)
# This is the "OurDDPG" baseline from Fujimoto et al. TD3 paper
# Architecture MATCHES TD3 exactly for fair comparison, but WITHOUT:
#   1. Twin critics (single Q-network)
#   2. Delayed policy updates (update every step)
#   3. Target policy smoothing (no noise on target actions)

algorithm:
  name: 'DDPG'

  # Core DDPG hyperparameters (matching TD3 for fair comparison)
  learning_rate: 0.0003  # 3e-4, Adam optimizer for both actor and critic
  discount: 0.99  # Gamma (γ), discount factor for future rewards
  tau: 0.005  # Polyak averaging coefficient (ρ) for soft target network updates

  # Training parameters (identical to TD3)
  batch_size: 256  # Mini-batch size sampled from replay buffer
  buffer_size: 1000000  # Replay buffer capacity (1M transitions)
  learning_starts: 25000  # Warm-up timesteps with uniform random exploration
  train_freq: 1  # Update networks every step
  gradient_steps: -1  # -1 means as many gradient steps as environment steps

  # Exploration noise (added to actor output during training)
  exploration_noise: 0.1  # Std of Gaussian noise (scaled by max_action)

# Network architecture (IDENTICAL to TD3 for fair comparison)
networks:
  # Actor network (deterministic policy μ_φ(s))
  actor:
    hidden_layers: [256, 256]  # Two hidden layers with 256 neurons each
    activation: 'relu'  # ReLU activation for hidden layers
    output_activation: 'tanh'  # Tanh for bounded action output [-1, 1]
    learning_rate: 0.0003  # Can override algorithm learning_rate if needed

  # Single Critic network (Q_θ(s,a)) - KEY DIFFERENCE FROM TD3
  critic:
    hidden_layers: [256, 256]  # Two hidden layers with 256 neurons each
    activation: 'relu'  # ReLU activation
    n_critics: 1  # SINGLE critic (vs twin critics in TD3)
    learning_rate: 0.0003  # Can override algorithm learning_rate if needed

  # CNN Feature Extractor (IDENTICAL to TD3)
  cnn_extractor:
    # Architecture type: 'nature' (NatureCNN), 'resnet18', 'mobilenet_v2'
    architecture: 'nature'  # NatureCNN from DQN paper (default in SB3)

    # Input preprocessing
    input_channels: 4  # Stack of 4 consecutive grayscale frames
    input_height: 84  # Resized height (from 256→84 or 144→84)
    input_width: 84  # Resized width (from 256→84 or 144→84)

    # Transfer learning options
    use_pretrained: false  # Use ImageNet pre-trained weights
    freeze_features: false  # Freeze feature extractor during training

    # Feature dimension output (after CNN, before concatenation with vector obs)
    features_dim: 512  # Output dimension from CNN (NatureCNN outputs 512)

# State space configuration (IDENTICAL to TD3)
state:
  # Visual input (front camera)
  image:
    enabled: true
    stack_frames: 4  # Number of consecutive frames to stack (temporal information)
    grayscale: true  # Convert RGB to grayscale
    normalize: true  # Normalize pixel values to [0, 1]
    resize: [84, 84]  # Resize to 84x84 (smaller for faster training)

  # Kinematic features (vehicle state)
  kinematic:
    enabled: true
    features:
      - 'velocity'  # Current vehicle velocity (m/s)
      - 'lateral_deviation'  # Distance from lane center (m)
      - 'heading_error'  # Heading error w.r.t. lane direction (radians)
    normalize: true  # Normalize kinematic features

  # Goal-oriented features (waypoints)
  waypoints:
    enabled: true
    num_waypoints: 10  # Number of future waypoints to include in state
    lookahead_distance: 50.0  # Maximum distance to look ahead (meters)
    relative_coords: true  # Use vehicle-local coordinate frame
    normalize: true  # Normalize waypoint coordinates

# Action space configuration (IDENTICAL to TD3)
action:
  type: 'continuous'  # Continuous action space
  dim: 2  # [steering, throttle/brake]

  # Action bounds
  steering:
    min: -1.0  # Full left
    max: 1.0   # Full right

  throttle_brake:
    min: -1.0  # Full brake (negative values)
    max: 1.0   # Full throttle (positive values)

  # Action scaling (if needed for CARLA compatibility)
  scale_steering: 1.0  # Multiply steering output by this factor
  scale_throttle_brake: 1.0  # Multiply throttle/brake output by this factor

# Reward function configuration (IDENTICAL to TD3)
reward:
  # Component weights (tuned for safety-first autonomous driving)
  weights:
    efficiency: 1.0  # Reward for maintaining target speed
    lane_keeping: 2.0  # Reward for staying centered in lane
    comfort: 0.5  # Penalty for high jerk (smooth driving)
    safety: -100.0  # Large penalty for collisions/violations

  # Efficiency reward parameters
  efficiency:
    target_speed: 10.0  # Target speed in m/s (~36 km/h, urban speed)
    speed_tolerance: 2.0  # Tolerance around target speed (m/s)
    overspeed_penalty_scale: 2.0  # Amplify penalty for exceeding speed limit

  # Lane keeping reward parameters
  lane_keeping:
    lateral_tolerance: 0.5  # Acceptable lateral deviation (meters)
    heading_tolerance: 0.1  # Acceptable heading error (radians, ~5.7 degrees)

  # Comfort penalty parameters
  comfort:
    jerk_threshold: 3.0  # Jerk magnitude threshold for penalty (m/s³)

  # Safety penalty parameters
  safety:
    collision_penalty: -1000.0  # Terminal penalty for collision
    offroad_penalty: -500.0  # Terminal penalty for going off-road
    wrong_way_penalty: -200.0  # Penalty for driving in wrong direction

# Training configuration (IDENTICAL to TD3)
training:
  # Episode settings
  max_timesteps: 2000000  # Total training timesteps (~2000 episodes × 1000 steps)
  max_episode_steps: 1000  # Maximum steps per episode (CARLA default)

  # Checkpointing
  save_freq: 10000  # Save model every N timesteps
  checkpoint_dir: './data/checkpoints/ddpg/'
  save_replay_buffer: true  # Save replay buffer with checkpoint

  # Evaluation during training
  eval_freq: 5000  # Evaluate every N timesteps
  n_eval_episodes: 10  # Number of episodes for evaluation
  eval_deterministic: true  # Use deterministic policy (no exploration noise)

  # Logging
  log_dir: './data/logs/ddpg/'
  tensorboard: true  # Enable TensorBoard logging
  wandb:
    enabled: true  # Enable Weights & Biases logging
    project: 'td3-av-carla'
    entity: null  # WandB username/team (set via env var or here)
    tags: ['DDPG', 'CARLA', 'Town01', 'visual-navigation', 'baseline']
    notes: 'DDPG baseline for comparison with TD3 (matching architecture)'

  # Verbosity
  verbose: 1  # 0: no output, 1: info, 2: debug

# Evaluation configuration (post-training) (IDENTICAL to TD3)
evaluation:
  n_episodes: 20  # Number of episodes per scenario
  deterministic: true  # Always use deterministic policy
  render: false  # Render simulation (slow, use for visualization only)
  record_video: true  # Record video of evaluation episodes
  video_dir: './data/videos/ddpg/'

  # Metrics to compute
  metrics:
    - 'success_rate'  # Percentage of collision-free episodes
    - 'average_speed'  # Average speed during episode (km/h)
    - 'completion_time'  # Time to complete route (seconds)
    - 'collision_count'  # Number of collisions
    - 'offroad_count'  # Number of off-road events
    - 'avg_lateral_error'  # Average distance from lane center (m)
    - 'avg_heading_error'  # Average heading error (degrees)
    - 'avg_jerk'  # Average longitudinal jerk (m/s³)
    - 'avg_lateral_acceleration'  # Average lateral acceleration (m/s²)
    - 'min_ttc'  # Minimum time-to-collision (seconds)

# Device configuration
device:
  type: 'auto'  # 'auto', 'cuda', 'cpu'
  cuda_device_id: 0  # GPU device ID if using CUDA

# Reproducibility
seed: 42  # Random seed for reproducibility (null for random seed)
deterministic_pytorch: true  # Use deterministic PyTorch operations (slower but reproducible)

# Experiment metadata
experiment:
  name: 'ddpg_visual_navigation_baseline'
  description: 'DDPG baseline for comparison with TD3 (matching architecture, no TD3 improvements)'
  version: '1.0.0'
  author: 'Daniel Terra Gomes'
  paper: 'End-to-End Visual Autonomous Navigation with Twin Delayed DDPG'
  notes: |
    This DDPG baseline matches TD3 architecture exactly but lacks:
    1. Twin critics (single Q-network only)
    2. Delayed policy updates (updates every step)
    3. Target policy smoothing (no noise on target actions)
    This ensures fair comparison to isolate TD3's algorithmic improvements.
