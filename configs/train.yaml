# PPO Training Configuration
algorithm:
  name: "PPO"
  policy: "CnnPolicy"  # CNN for image processing

hyperparameters:
  learning_rate: 3.0e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null

network_architecture:
  features_extractor:
    cnn_layers:
      - [32, 8, 4]  # [filters, kernel_size, stride]
      - [64, 4, 2]
      - [64, 3, 1]
    mlp_layers: [512, 512]
    activation: "relu"

  policy_head:
    layers: [256]
    activation: "tanh"

  value_head:
    layers: [256]
    activation: "relu"

training:
  total_timesteps: 1000000
  eval_freq: 10000
  n_eval_episodes: 5
  eval_deterministic: true
  save_freq: 50000

environment:
  reward_function:
    # Reward weights (must sum to reasonable scale)
    speed_reward_weight: 1.0
    lane_keeping_weight: 2.0
    collision_penalty: -100.0
    lane_invasion_penalty: -10.0
    progress_reward_weight: 5.0
    comfort_weight: 0.5

  observation_space:
    image_size: [84, 84, 3]  # RGB camera
    normalize_images: true
    include_velocity: true
    include_angular_velocity: true
    include_position: false  # Use relative features only

  action_space:
    continuous: true
    action_bounds:
      throttle: [0.0, 1.0]
      steering: [-1.0, 1.0]

  episode:
    max_steps: 2000
    timeout_seconds: 100.0
    success_distance_threshold: 5.0  # meters to goal

logging:
  tensorboard_log: "./logs/tensorboard"
  checkpoint_dir: "./models/checkpoints"
  log_interval: 1
  verbose: 1
  save_replay_buffer: false

# Reproducibility
random_seed: 42
torch_deterministic: true
cuda_deterministic: false  # Set true for full determinism (slower)
